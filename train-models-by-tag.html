<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>7 train Models By Tag | The caret Package</title>
  <meta name="description" content="Documentation for the caret package.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="7 train Models By Tag | The caret Package" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Documentation for the caret package." />
  <meta name="github-repo" content="topepo/caret" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7 train Models By Tag | The caret Package" />
  
  <meta name="twitter:description" content="Documentation for the caret package." />
  

<meta name="author" content="Max Kuhn">


<meta name="date" content="2019-03-27">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="available-models.html">
<link rel="next" href="models-clustered-by-tag-similarity.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.3/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.5/datatables.js"></script>
<link href="libs/dt-core-bootstrap-1.10.16/css/dataTables.bootstrap.min.css" rel="stylesheet" />
<link href="libs/dt-core-bootstrap-1.10.16/css/dataTables.bootstrap.extra.css" rel="stylesheet" />
<script src="libs/dt-core-bootstrap-1.10.16/js/jquery.dataTables.min.js"></script>
<script src="libs/dt-core-bootstrap-1.10.16/js/dataTables.bootstrap.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<script src="libs/d3-4.5.0/d3.min.js"></script>
<script src="libs/forceNetwork-binding-0.4/forceNetwork.js"></script>
<script src="libs/plotly-binding-4.8.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.39.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.39.2/plotly-latest.min.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="visualizations.html"><a href="visualizations.html"><i class="fa fa-check"></i><b>2</b> Visualizations</a></li>
<li class="chapter" data-level="3" data-path="pre-processing.html"><a href="pre-processing.html"><i class="fa fa-check"></i><b>3</b> Pre-Processing</a><ul>
<li class="chapter" data-level="3.1" data-path="pre-processing.html"><a href="pre-processing.html#creating-dummy-variables"><i class="fa fa-check"></i><b>3.1</b> Creating Dummy Variables</a></li>
<li class="chapter" data-level="3.2" data-path="pre-processing.html"><a href="pre-processing.html#zero--and-near-zero-variance-predictors"><i class="fa fa-check"></i><b>3.2</b> Zero- and Near Zero-Variance Predictors</a></li>
<li class="chapter" data-level="3.3" data-path="pre-processing.html"><a href="pre-processing.html#identifying-correlated-predictors"><i class="fa fa-check"></i><b>3.3</b> Identifying Correlated Predictors</a></li>
<li class="chapter" data-level="3.4" data-path="pre-processing.html"><a href="pre-processing.html#linear-dependencies"><i class="fa fa-check"></i><b>3.4</b> Linear Dependencies</a></li>
<li class="chapter" data-level="3.5" data-path="pre-processing.html"><a href="pre-processing.html#the-preprocess-function"><i class="fa fa-check"></i><b>3.5</b> The <code>preProcess</code> Function</a></li>
<li class="chapter" data-level="3.6" data-path="pre-processing.html"><a href="pre-processing.html#centering-and-scaling"><i class="fa fa-check"></i><b>3.6</b> Centering and Scaling</a></li>
<li class="chapter" data-level="3.7" data-path="pre-processing.html"><a href="pre-processing.html#imputation"><i class="fa fa-check"></i><b>3.7</b> Imputation</a></li>
<li class="chapter" data-level="3.8" data-path="pre-processing.html"><a href="pre-processing.html#transforming-predictors"><i class="fa fa-check"></i><b>3.8</b> Transforming Predictors</a></li>
<li class="chapter" data-level="3.9" data-path="pre-processing.html"><a href="pre-processing.html#putting-it-all-together"><i class="fa fa-check"></i><b>3.9</b> Putting It All Together</a></li>
<li class="chapter" data-level="3.10" data-path="pre-processing.html"><a href="pre-processing.html#class-distance-calculations"><i class="fa fa-check"></i><b>3.10</b> Class Distance Calculations</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="data-splitting.html"><a href="data-splitting.html"><i class="fa fa-check"></i><b>4</b> Data Splitting</a><ul>
<li class="chapter" data-level="4.1" data-path="data-splitting.html"><a href="data-splitting.html#simple-splitting-based-on-the-outcome"><i class="fa fa-check"></i><b>4.1</b> Simple Splitting Based on the Outcome</a></li>
<li class="chapter" data-level="4.2" data-path="data-splitting.html"><a href="data-splitting.html#splitting-based-on-the-predictors"><i class="fa fa-check"></i><b>4.2</b> Splitting Based on the Predictors</a></li>
<li class="chapter" data-level="4.3" data-path="data-splitting.html"><a href="data-splitting.html#data-splitting-for-time-series"><i class="fa fa-check"></i><b>4.3</b> Data Splitting for Time Series</a></li>
<li class="chapter" data-level="4.4" data-path="data-splitting.html"><a href="data-splitting.html#simple-splitting-with-important-groups"><i class="fa fa-check"></i><b>4.4</b> Simple Splitting with Important Groups</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html"><i class="fa fa-check"></i><b>5</b> Model Training and Tuning</a><ul>
<li class="chapter" data-level="5.1" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#model-training-and-parameter-tuning"><i class="fa fa-check"></i><b>5.1</b> Model Training and Parameter Tuning</a></li>
<li class="chapter" data-level="5.2" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#an-example"><i class="fa fa-check"></i><b>5.2</b> An Example</a></li>
<li class="chapter" data-level="5.3" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#basic-parameter-tuning"><i class="fa fa-check"></i><b>5.3</b> Basic Parameter Tuning</a></li>
<li class="chapter" data-level="5.4" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#notes-on-reproducibility"><i class="fa fa-check"></i><b>5.4</b> Notes on Reproducibility</a></li>
<li class="chapter" data-level="5.5" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#customizing-the-tuning-process"><i class="fa fa-check"></i><b>5.5</b> Customizing the Tuning Process</a><ul>
<li class="chapter" data-level="5.5.1" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#pre-processing-options"><i class="fa fa-check"></i><b>5.5.1</b> Pre-Processing Options</a></li>
<li class="chapter" data-level="5.5.2" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#alternate-tuning-grids"><i class="fa fa-check"></i><b>5.5.2</b> Alternate Tuning Grids</a></li>
<li class="chapter" data-level="5.5.3" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#plotting-the-resampling-profile"><i class="fa fa-check"></i><b>5.5.3</b> Plotting the Resampling Profile</a></li>
<li class="chapter" data-level="5.5.4" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#the-traincontrol-function"><i class="fa fa-check"></i><b>5.5.4</b> The <code>trainControl</code> Function</a></li>
<li class="chapter" data-level="5.5.5" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#alternate-performance-metrics"><i class="fa fa-check"></i><b>5.5.5</b> Alternate Performance Metrics</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#choosing-the-final-model"><i class="fa fa-check"></i><b>5.6</b> Choosing the Final Model</a></li>
<li class="chapter" data-level="5.7" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#extracting-predictions-and-class-probabilities"><i class="fa fa-check"></i><b>5.7</b> Extracting Predictions and Class Probabilities</a></li>
<li class="chapter" data-level="5.8" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#exploring-and-comparing-resampling-distributions"><i class="fa fa-check"></i><b>5.8</b> Exploring and Comparing Resampling Distributions</a><ul>
<li class="chapter" data-level="5.8.1" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#within-model"><i class="fa fa-check"></i><b>5.8.1</b> Within-Model</a></li>
<li class="chapter" data-level="5.8.2" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#between-models"><i class="fa fa-check"></i><b>5.8.2</b> Between-Models</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="model-training-and-tuning.html"><a href="model-training-and-tuning.html#fitting-models-without-parameter-tuning"><i class="fa fa-check"></i><b>5.9</b> Fitting Models Without Parameter Tuning</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="available-models.html"><a href="available-models.html"><i class="fa fa-check"></i><b>6</b> Available Models</a></li>
<li class="chapter" data-level="7" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html"><i class="fa fa-check"></i><b>7</b> <code>train</code> Models By Tag</a><ul>
<li class="chapter" data-level="7.0.1" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#accepts-case-weights"><i class="fa fa-check"></i><b>7.0.1</b> Accepts Case Weights</a></li>
<li class="chapter" data-level="7.0.2" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#bagging"><i class="fa fa-check"></i><b>7.0.2</b> Bagging</a></li>
<li class="chapter" data-level="7.0.3" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#bayesian-model"><i class="fa fa-check"></i><b>7.0.3</b> Bayesian Model</a></li>
<li class="chapter" data-level="7.0.4" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#binary-predictors-only"><i class="fa fa-check"></i><b>7.0.4</b> Binary Predictors Only</a></li>
<li class="chapter" data-level="7.0.5" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#boosting"><i class="fa fa-check"></i><b>7.0.5</b> Boosting</a></li>
<li class="chapter" data-level="7.0.6" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#categorical-predictors-only"><i class="fa fa-check"></i><b>7.0.6</b> Categorical Predictors Only</a></li>
<li class="chapter" data-level="7.0.7" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#cost-sensitive-learning"><i class="fa fa-check"></i><b>7.0.7</b> Cost Sensitive Learning</a></li>
<li class="chapter" data-level="7.0.8" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#discriminant-analysis"><i class="fa fa-check"></i><b>7.0.8</b> Discriminant Analysis</a></li>
<li class="chapter" data-level="7.0.9" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#distance-weighted-discrimination"><i class="fa fa-check"></i><b>7.0.9</b> Distance Weighted Discrimination</a></li>
<li class="chapter" data-level="7.0.10" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#ensemble-model"><i class="fa fa-check"></i><b>7.0.10</b> Ensemble Model</a></li>
<li class="chapter" data-level="7.0.11" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#feature-extraction"><i class="fa fa-check"></i><b>7.0.11</b> Feature Extraction</a></li>
<li class="chapter" data-level="7.0.12" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#feature-selection-wrapper"><i class="fa fa-check"></i><b>7.0.12</b> Feature Selection Wrapper</a></li>
<li class="chapter" data-level="7.0.13" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#gaussian-process"><i class="fa fa-check"></i><b>7.0.13</b> Gaussian Process</a></li>
<li class="chapter" data-level="7.0.14" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#generalized-additive-model"><i class="fa fa-check"></i><b>7.0.14</b> Generalized Additive Model</a></li>
<li class="chapter" data-level="7.0.15" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#generalized-linear-model"><i class="fa fa-check"></i><b>7.0.15</b> Generalized Linear Model</a></li>
<li class="chapter" data-level="7.0.16" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#handle-missing-predictor-data"><i class="fa fa-check"></i><b>7.0.16</b> Handle Missing Predictor Data</a></li>
<li class="chapter" data-level="7.0.17" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#implicit-feature-selection"><i class="fa fa-check"></i><b>7.0.17</b> Implicit Feature Selection</a></li>
<li class="chapter" data-level="7.0.18" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#kernel-method"><i class="fa fa-check"></i><b>7.0.18</b> Kernel Method</a></li>
<li class="chapter" data-level="7.0.19" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#l1-regularization"><i class="fa fa-check"></i><b>7.0.19</b> L1 Regularization</a></li>
<li class="chapter" data-level="7.0.20" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#l2-regularization"><i class="fa fa-check"></i><b>7.0.20</b> L2 Regularization</a></li>
<li class="chapter" data-level="7.0.21" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#linear-classifier"><i class="fa fa-check"></i><b>7.0.21</b> Linear Classifier</a></li>
<li class="chapter" data-level="7.0.22" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#linear-regression"><i class="fa fa-check"></i><b>7.0.22</b> Linear Regression</a></li>
<li class="chapter" data-level="7.0.23" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#logic-regression"><i class="fa fa-check"></i><b>7.0.23</b> Logic Regression</a></li>
<li class="chapter" data-level="7.0.24" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#logistic-regression"><i class="fa fa-check"></i><b>7.0.24</b> Logistic Regression</a></li>
<li class="chapter" data-level="7.0.25" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#mixture-model"><i class="fa fa-check"></i><b>7.0.25</b> Mixture Model</a></li>
<li class="chapter" data-level="7.0.26" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#model-tree"><i class="fa fa-check"></i><b>7.0.26</b> Model Tree</a></li>
<li class="chapter" data-level="7.0.27" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#multivariate-adaptive-regression-splines"><i class="fa fa-check"></i><b>7.0.27</b> Multivariate Adaptive Regression Splines</a></li>
<li class="chapter" data-level="7.0.28" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#neural-network"><i class="fa fa-check"></i><b>7.0.28</b> Neural Network</a></li>
<li class="chapter" data-level="7.0.29" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#oblique-tree"><i class="fa fa-check"></i><b>7.0.29</b> Oblique Tree</a></li>
<li class="chapter" data-level="7.0.30" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#ordinal-outcomes"><i class="fa fa-check"></i><b>7.0.30</b> Ordinal Outcomes</a></li>
<li class="chapter" data-level="7.0.31" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#partial-least-squares"><i class="fa fa-check"></i><b>7.0.31</b> Partial Least Squares</a></li>
<li class="chapter" data-level="7.0.32" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#patient-rule-induction-method"><i class="fa fa-check"></i><b>7.0.32</b> Patient Rule Induction Method</a></li>
<li class="chapter" data-level="7.0.33" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#polynomial-model"><i class="fa fa-check"></i><b>7.0.33</b> Polynomial Model</a></li>
<li class="chapter" data-level="7.0.34" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#prototype-models"><i class="fa fa-check"></i><b>7.0.34</b> Prototype Models</a></li>
<li class="chapter" data-level="7.0.35" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#quantile-regression"><i class="fa fa-check"></i><b>7.0.35</b> Quantile Regression</a></li>
<li class="chapter" data-level="7.0.36" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#radial-basis-function"><i class="fa fa-check"></i><b>7.0.36</b> Radial Basis Function</a></li>
<li class="chapter" data-level="7.0.37" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#random-forest"><i class="fa fa-check"></i><b>7.0.37</b> Random Forest</a></li>
<li class="chapter" data-level="7.0.38" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#regularization"><i class="fa fa-check"></i><b>7.0.38</b> Regularization</a></li>
<li class="chapter" data-level="7.0.39" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#relevance-vector-machines"><i class="fa fa-check"></i><b>7.0.39</b> Relevance Vector Machines</a></li>
<li class="chapter" data-level="7.0.40" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#ridge-regression"><i class="fa fa-check"></i><b>7.0.40</b> Ridge Regression</a></li>
<li class="chapter" data-level="7.0.41" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#robust-methods"><i class="fa fa-check"></i><b>7.0.41</b> Robust Methods</a></li>
<li class="chapter" data-level="7.0.42" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#robust-model"><i class="fa fa-check"></i><b>7.0.42</b> Robust Model</a></li>
<li class="chapter" data-level="7.0.43" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#roc-curves"><i class="fa fa-check"></i><b>7.0.43</b> ROC Curves</a></li>
<li class="chapter" data-level="7.0.44" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#rule-based-model"><i class="fa fa-check"></i><b>7.0.44</b> Rule-Based Model</a></li>
<li class="chapter" data-level="7.0.45" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#self-organising-maps"><i class="fa fa-check"></i><b>7.0.45</b> Self-Organising Maps</a></li>
<li class="chapter" data-level="7.0.46" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#string-kernel"><i class="fa fa-check"></i><b>7.0.46</b> String Kernel</a></li>
<li class="chapter" data-level="7.0.47" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#support-vector-machines"><i class="fa fa-check"></i><b>7.0.47</b> Support Vector Machines</a></li>
<li class="chapter" data-level="7.0.48" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#supports-class-probabilities"><i class="fa fa-check"></i><b>7.0.48</b> Supports Class Probabilities</a></li>
<li class="chapter" data-level="7.0.49" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#text-mining"><i class="fa fa-check"></i><b>7.0.49</b> Text Mining</a></li>
<li class="chapter" data-level="7.0.50" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#tree-based-model"><i class="fa fa-check"></i><b>7.0.50</b> Tree-Based Model</a></li>
<li class="chapter" data-level="7.0.51" data-path="train-models-by-tag.html"><a href="train-models-by-tag.html#two-class-only"><i class="fa fa-check"></i><b>7.0.51</b> Two Class Only</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="models-clustered-by-tag-similarity.html"><a href="models-clustered-by-tag-similarity.html"><i class="fa fa-check"></i><b>8</b> Models Clustered by Tag Similarity</a></li>
<li class="chapter" data-level="9" data-path="parallel-processing.html"><a href="parallel-processing.html"><i class="fa fa-check"></i><b>9</b> Parallel Processing</a></li>
<li class="chapter" data-level="10" data-path="random-hyperparameter-search.html"><a href="random-hyperparameter-search.html"><i class="fa fa-check"></i><b>10</b> Random Hyperparameter Search</a></li>
<li class="chapter" data-level="11" data-path="subsampling-for-class-imbalances.html"><a href="subsampling-for-class-imbalances.html"><i class="fa fa-check"></i><b>11</b> Subsampling For Class Imbalances</a><ul>
<li class="chapter" data-level="11.1" data-path="subsampling-for-class-imbalances.html"><a href="subsampling-for-class-imbalances.html#subsampling-techniques"><i class="fa fa-check"></i><b>11.1</b> Subsampling Techniques</a></li>
<li class="chapter" data-level="11.2" data-path="subsampling-for-class-imbalances.html"><a href="subsampling-for-class-imbalances.html#subsampling-during-resampling"><i class="fa fa-check"></i><b>11.2</b> Subsampling During Resampling</a></li>
<li class="chapter" data-level="11.3" data-path="subsampling-for-class-imbalances.html"><a href="subsampling-for-class-imbalances.html#complications"><i class="fa fa-check"></i><b>11.3</b> Complications</a></li>
<li class="chapter" data-level="11.4" data-path="subsampling-for-class-imbalances.html"><a href="subsampling-for-class-imbalances.html#using-custom-subsampling-techniques"><i class="fa fa-check"></i><b>11.4</b> Using Custom Subsampling Techniques</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="using-recipes-with-train.html"><a href="using-recipes-with-train.html"><i class="fa fa-check"></i><b>12</b> Using Recipes with train</a><ul>
<li class="chapter" data-level="12.1" data-path="using-recipes-with-train.html"><a href="using-recipes-with-train.html#why-should-you-learn-this"><i class="fa fa-check"></i><b>12.1</b> Why Should you learn this?</a><ul>
<li class="chapter" data-level="12.1.1" data-path="using-recipes-with-train.html"><a href="using-recipes-with-train.html#more-versatile-tools-for-preprocessing-data"><i class="fa fa-check"></i><b>12.1.1</b> More versatile tools for preprocessing data</a></li>
<li class="chapter" data-level="12.1.2" data-path="using-recipes-with-train.html"><a href="using-recipes-with-train.html#using-additional-data-to-measure-performance"><i class="fa fa-check"></i><b>12.1.2</b> Using additional data to measure performance</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="using-recipes-with-train.html"><a href="using-recipes-with-train.html#an-example-1"><i class="fa fa-check"></i><b>12.2</b> An Example</a></li>
<li class="chapter" data-level="12.3" data-path="using-recipes-with-train.html"><a href="using-recipes-with-train.html#case-weights"><i class="fa fa-check"></i><b>12.3</b> Case Weights</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html"><i class="fa fa-check"></i><b>13</b> Using Your Own Model in <code>train</code></a><ul>
<li class="chapter" data-level="13.1" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#introduction-1"><i class="fa fa-check"></i><b>13.1</b> Introduction</a></li>
<li class="chapter" data-level="13.2" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#illustrative-example-1-svms-with-laplacian-kernels"><i class="fa fa-check"></i><b>13.2</b> Illustrative Example 1: SVMs with Laplacian Kernels</a></li>
<li class="chapter" data-level="13.3" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#model-components"><i class="fa fa-check"></i><b>13.3</b> Model Components</a><ul>
<li class="chapter" data-level="13.3.1" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#the-parameters-element"><i class="fa fa-check"></i><b>13.3.1</b> The parameters Element</a></li>
<li class="chapter" data-level="13.3.2" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#the-grid-element"><i class="fa fa-check"></i><b>13.3.2</b> The <code>grid</code> Element</a></li>
<li class="chapter" data-level="13.3.3" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#the-fit-element"><i class="fa fa-check"></i><b>13.3.3</b> The <code>fit</code> Element</a></li>
<li class="chapter" data-level="13.3.4" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#the-predict-element"><i class="fa fa-check"></i><b>13.3.4</b> The <code>predict</code> Element</a></li>
<li class="chapter" data-level="13.3.5" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#the-prob-element"><i class="fa fa-check"></i><b>13.3.5</b> The <code>prob</code> Element</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#the-sort-element"><i class="fa fa-check"></i><b>13.4</b> The sort Element</a><ul>
<li class="chapter" data-level="13.4.1" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#the-levels-element"><i class="fa fa-check"></i><b>13.4.1</b> The <code>levels</code> Element</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#illustrative-example-2-something-more-complicated---logitboost"><i class="fa fa-check"></i><b>13.5</b> Illustrative Example 2: Something More Complicated - <code>LogitBoost</code></a></li>
<li class="chapter" data-level="13.6" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#illustrative-example-3-nonstandard-formulas"><i class="fa fa-check"></i><b>13.6</b> Illustrative Example 3: Nonstandard Formulas</a></li>
<li class="chapter" data-level="13.7" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#illustrative-example-4-pls-feature-extraction-pre-processing"><i class="fa fa-check"></i><b>13.7</b> Illustrative Example 4: PLS Feature Extraction Pre-Processing</a></li>
<li class="chapter" data-level="13.8" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#illustrative-example-5-optimizing-probability-thresholds-for-class-imbalances"><i class="fa fa-check"></i><b>13.8</b> Illustrative Example 5: Optimizing probability thresholds for class imbalances</a></li>
<li class="chapter" data-level="13.9" data-path="using-your-own-model-in-train.html"><a href="using-your-own-model-in-train.html#illustrative-example-6-offsets-in-generalized-linear-models"><i class="fa fa-check"></i><b>13.9</b> Illustrative Example 6: Offsets in Generalized Linear Models</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="adaptive-resampling.html"><a href="adaptive-resampling.html"><i class="fa fa-check"></i><b>14</b> Adaptive Resampling</a></li>
<li class="chapter" data-level="15" data-path="variable-importance.html"><a href="variable-importance.html"><i class="fa fa-check"></i><b>15</b> Variable Importance</a><ul>
<li class="chapter" data-level="15.1" data-path="variable-importance.html"><a href="variable-importance.html#model-specific-metrics"><i class="fa fa-check"></i><b>15.1</b> Model Specific Metrics</a></li>
<li class="chapter" data-level="15.2" data-path="variable-importance.html"><a href="variable-importance.html#model-independent-metrics"><i class="fa fa-check"></i><b>15.2</b> Model Independent Metrics</a></li>
<li class="chapter" data-level="15.3" data-path="variable-importance.html"><a href="variable-importance.html#an-example-2"><i class="fa fa-check"></i><b>15.3</b> An Example</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="miscellaneous-model-functions.html"><a href="miscellaneous-model-functions.html"><i class="fa fa-check"></i><b>16</b> Miscellaneous Model Functions</a><ul>
<li class="chapter" data-level="16.1" data-path="miscellaneous-model-functions.html"><a href="miscellaneous-model-functions.html#yet-another-k-nearest-neighbor-function"><i class="fa fa-check"></i><b>16.1</b> Yet Another <em>k</em>-Nearest Neighbor Function</a></li>
<li class="chapter" data-level="16.2" data-path="miscellaneous-model-functions.html"><a href="miscellaneous-model-functions.html#partial-least-squares-discriminant-analysis"><i class="fa fa-check"></i><b>16.2</b> Partial Least Squares Discriminant Analysis</a></li>
<li class="chapter" data-level="16.3" data-path="miscellaneous-model-functions.html"><a href="miscellaneous-model-functions.html#bagged-mars-and-fda"><i class="fa fa-check"></i><b>16.3</b> Bagged MARS and FDA</a></li>
<li class="chapter" data-level="16.4" data-path="miscellaneous-model-functions.html"><a href="miscellaneous-model-functions.html#bagging-1"><i class="fa fa-check"></i><b>16.4</b> Bagging</a><ul>
<li class="chapter" data-level="16.4.1" data-path="miscellaneous-model-functions.html"><a href="miscellaneous-model-functions.html#the-fit-function"><i class="fa fa-check"></i><b>16.4.1</b> The <code>fit</code> Function</a></li>
<li class="chapter" data-level="16.4.2" data-path="miscellaneous-model-functions.html"><a href="miscellaneous-model-functions.html#the-pred-function"><i class="fa fa-check"></i><b>16.4.2</b> The <code>pred</code> Function</a></li>
<li class="chapter" data-level="16.4.3" data-path="miscellaneous-model-functions.html"><a href="miscellaneous-model-functions.html#the-aggregate-function"><i class="fa fa-check"></i><b>16.4.3</b> The <code>aggregate</code> Function</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="miscellaneous-model-functions.html"><a href="miscellaneous-model-functions.html#model-averaged-neural-networks"><i class="fa fa-check"></i><b>16.5</b> Model Averaged Neural Networks</a></li>
<li class="chapter" data-level="16.6" data-path="miscellaneous-model-functions.html"><a href="miscellaneous-model-functions.html#neural-networks-with-a-principal-component-step"><i class="fa fa-check"></i><b>16.6</b> Neural Networks with a Principal Component Step</a></li>
<li class="chapter" data-level="16.7" data-path="miscellaneous-model-functions.html"><a href="miscellaneous-model-functions.html#independent-component-regression"><i class="fa fa-check"></i><b>16.7</b> Independent Component Regression</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="measuring-performance.html"><a href="measuring-performance.html"><i class="fa fa-check"></i><b>17</b> Measuring Performance</a><ul>
<li class="chapter" data-level="17.1" data-path="measuring-performance.html"><a href="measuring-performance.html#measures-for-regression"><i class="fa fa-check"></i><b>17.1</b> Measures for Regression</a></li>
<li class="chapter" data-level="17.2" data-path="measuring-performance.html"><a href="measuring-performance.html#measures-for-predicted-classes"><i class="fa fa-check"></i><b>17.2</b> Measures for Predicted Classes</a></li>
<li class="chapter" data-level="17.3" data-path="measuring-performance.html"><a href="measuring-performance.html#measures-for-class-probabilities"><i class="fa fa-check"></i><b>17.3</b> Measures for Class Probabilities</a></li>
<li class="chapter" data-level="17.4" data-path="measuring-performance.html"><a href="measuring-performance.html#lift-curves"><i class="fa fa-check"></i><b>17.4</b> Lift Curves</a></li>
<li class="chapter" data-level="17.5" data-path="measuring-performance.html"><a href="measuring-performance.html#calibration-curves"><i class="fa fa-check"></i><b>17.5</b> Calibration Curves</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="feature-selection-overview.html"><a href="feature-selection-overview.html"><i class="fa fa-check"></i><b>18</b> Feature Selection Overview</a><ul>
<li class="chapter" data-level="18.1" data-path="feature-selection-overview.html"><a href="feature-selection-overview.html#models-with-built-in-feature-selection"><i class="fa fa-check"></i><b>18.1</b> Models with Built-In Feature Selection</a></li>
<li class="chapter" data-level="18.2" data-path="feature-selection-overview.html"><a href="feature-selection-overview.html#feature-selection-methods"><i class="fa fa-check"></i><b>18.2</b> Feature Selection Methods</a></li>
<li class="chapter" data-level="18.3" data-path="feature-selection-overview.html"><a href="feature-selection-overview.html#external-validation"><i class="fa fa-check"></i><b>18.3</b> External Validation</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="feature-selection-using-univariate-filters.html"><a href="feature-selection-using-univariate-filters.html"><i class="fa fa-check"></i><b>19</b> Feature Selection using Univariate Filters</a><ul>
<li class="chapter" data-level="19.1" data-path="feature-selection-using-univariate-filters.html"><a href="feature-selection-using-univariate-filters.html#univariate-filters"><i class="fa fa-check"></i><b>19.1</b> Univariate Filters</a></li>
<li class="chapter" data-level="19.2" data-path="feature-selection-using-univariate-filters.html"><a href="feature-selection-using-univariate-filters.html#basic-syntax"><i class="fa fa-check"></i><b>19.2</b> Basic Syntax</a><ul>
<li class="chapter" data-level="19.2.1" data-path="feature-selection-using-univariate-filters.html"><a href="feature-selection-using-univariate-filters.html#the-score-function"><i class="fa fa-check"></i><b>19.2.1</b> The <code>score</code> Function</a></li>
<li class="chapter" data-level="19.2.2" data-path="feature-selection-using-univariate-filters.html"><a href="feature-selection-using-univariate-filters.html#the-filter-function"><i class="fa fa-check"></i><b>19.2.2</b> The <code>filter</code> Function</a></li>
<li class="chapter" data-level="19.2.3" data-path="feature-selection-using-univariate-filters.html"><a href="feature-selection-using-univariate-filters.html#the-fit-function-1"><i class="fa fa-check"></i><b>19.2.3</b> The <code>fit</code> Function</a></li>
<li class="chapter" data-level="19.2.4" data-path="feature-selection-using-univariate-filters.html"><a href="feature-selection-using-univariate-filters.html#the-summary-and-pred-functions"><i class="fa fa-check"></i><b>19.2.4</b> The <code>summary</code> and <code>pred</code> Functions</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="feature-selection-using-univariate-filters.html"><a href="feature-selection-using-univariate-filters.html#fexample"><i class="fa fa-check"></i><b>19.3</b> The Example</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="recursive-feature-elimination.html"><a href="recursive-feature-elimination.html"><i class="fa fa-check"></i><b>20</b> Recursive Feature Elimination</a><ul>
<li class="chapter" data-level="20.1" data-path="recursive-feature-elimination.html"><a href="recursive-feature-elimination.html#backwards-selection"><i class="fa fa-check"></i><b>20.1</b> Backwards Selection</a></li>
<li class="chapter" data-level="20.2" data-path="recursive-feature-elimination.html"><a href="recursive-feature-elimination.html#resampling-and-external-validation"><i class="fa fa-check"></i><b>20.2</b> Resampling and External Validation</a></li>
<li class="chapter" data-level="20.3" data-path="recursive-feature-elimination.html"><a href="recursive-feature-elimination.html#recursive-feature-elimination-via-caret"><i class="fa fa-check"></i><b>20.3</b> Recursive Feature Elimination via <a href="http://cran.r-project.org/web/packages/caret/index.html"><code>caret</code></a></a></li>
<li class="chapter" data-level="20.4" data-path="recursive-feature-elimination.html"><a href="recursive-feature-elimination.html#rfeexample"><i class="fa fa-check"></i><b>20.4</b> An Example</a></li>
<li class="chapter" data-level="20.5" data-path="recursive-feature-elimination.html"><a href="recursive-feature-elimination.html#rfehelpers"><i class="fa fa-check"></i><b>20.5</b> Helper Functions</a><ul>
<li class="chapter" data-level="20.5.1" data-path="recursive-feature-elimination.html"><a href="recursive-feature-elimination.html#the-summary-function"><i class="fa fa-check"></i><b>20.5.1</b> The <code>summary</code> Function</a></li>
<li class="chapter" data-level="20.5.2" data-path="recursive-feature-elimination.html"><a href="recursive-feature-elimination.html#the-fit-function-2"><i class="fa fa-check"></i><b>20.5.2</b> The <code>fit</code> Function</a></li>
<li class="chapter" data-level="20.5.3" data-path="recursive-feature-elimination.html"><a href="recursive-feature-elimination.html#the-pred-function-1"><i class="fa fa-check"></i><b>20.5.3</b> The <code>pred</code> Function</a></li>
<li class="chapter" data-level="20.5.4" data-path="recursive-feature-elimination.html"><a href="recursive-feature-elimination.html#the-rank-function"><i class="fa fa-check"></i><b>20.5.4</b> The <code>rank</code> Function</a></li>
<li class="chapter" data-level="20.5.5" data-path="recursive-feature-elimination.html"><a href="recursive-feature-elimination.html#the-selectsize-function"><i class="fa fa-check"></i><b>20.5.5</b> The <code>selectSize</code> Function</a></li>
<li class="chapter" data-level="20.5.6" data-path="recursive-feature-elimination.html"><a href="recursive-feature-elimination.html#the-selectvar-function"><i class="fa fa-check"></i><b>20.5.6</b> The <code>selectVar</code> Function</a></li>
</ul></li>
<li class="chapter" data-level="20.6" data-path="recursive-feature-elimination.html"><a href="recursive-feature-elimination.html#rfeexample2"><i class="fa fa-check"></i><b>20.6</b> The Example</a></li>
<li class="chapter" data-level="20.7" data-path="recursive-feature-elimination.html"><a href="recursive-feature-elimination.html#rferecipes"><i class="fa fa-check"></i><b>20.7</b> Using a Recipe</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html"><i class="fa fa-check"></i><b>21</b> Feature Selection using Genetic Algorithms</a><ul>
<li class="chapter" data-level="21.1" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#genetic-algorithms"><i class="fa fa-check"></i><b>21.1</b> Genetic Algorithms</a></li>
<li class="chapter" data-level="21.2" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#internal-and-external-performance-estimates"><i class="fa fa-check"></i><b>21.2</b> Internal and External Performance Estimates</a></li>
<li class="chapter" data-level="21.3" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#basic-syntax-1"><i class="fa fa-check"></i><b>21.3</b> Basic Syntax</a></li>
<li class="chapter" data-level="21.4" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#gaexample"><i class="fa fa-check"></i><b>21.4</b> Genetic Algorithm Example</a></li>
<li class="chapter" data-level="21.5" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#customizing-the-search"><i class="fa fa-check"></i><b>21.5</b> Customizing the Search</a><ul>
<li class="chapter" data-level="21.5.1" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#the-fit-function-3"><i class="fa fa-check"></i><b>21.5.1</b> The <code>fit</code> Function</a></li>
<li class="chapter" data-level="21.5.2" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#the-pred-function-2"><i class="fa fa-check"></i><b>21.5.2</b> The <code>pred</code> Function</a></li>
<li class="chapter" data-level="21.5.3" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#the-fitness_intern-function"><i class="fa fa-check"></i><b>21.5.3</b> The <code>fitness_intern</code> Function</a></li>
<li class="chapter" data-level="21.5.4" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#the-fitness_extern-function"><i class="fa fa-check"></i><b>21.5.4</b> The <code>fitness_extern</code> Function</a></li>
<li class="chapter" data-level="21.5.5" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#the-initial-function"><i class="fa fa-check"></i><b>21.5.5</b> The <code>initial</code> Function</a></li>
<li class="chapter" data-level="21.5.6" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#the-selection-function"><i class="fa fa-check"></i><b>21.5.6</b> The <code>selection</code> Function</a></li>
<li class="chapter" data-level="21.5.7" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#the-crossover-function"><i class="fa fa-check"></i><b>21.5.7</b> The <code>crossover</code> Function</a></li>
<li class="chapter" data-level="21.5.8" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#the-mutation-function"><i class="fa fa-check"></i><b>21.5.8</b> The <code>mutation</code> Function</a></li>
<li class="chapter" data-level="21.5.9" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#the-selectiter-function"><i class="fa fa-check"></i><b>21.5.9</b> The <code>selectIter</code> Function</a></li>
</ul></li>
<li class="chapter" data-level="21.6" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#the-example-revisited"><i class="fa fa-check"></i><b>21.6</b> The Example Revisited</a></li>
<li class="chapter" data-level="21.7" data-path="feature-selection-using-genetic-algorithms.html"><a href="feature-selection-using-genetic-algorithms.html#using-recipes"><i class="fa fa-check"></i><b>21.7</b> Using Recipes</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="feature-selection-using-simulated-annealing.html"><a href="feature-selection-using-simulated-annealing.html"><i class="fa fa-check"></i><b>22</b> Feature Selection using Simulated Annealing</a><ul>
<li class="chapter" data-level="22.1" data-path="feature-selection-using-simulated-annealing.html"><a href="feature-selection-using-simulated-annealing.html#simulated-annealing"><i class="fa fa-check"></i><b>22.1</b> Simulated Annealing</a></li>
<li class="chapter" data-level="22.2" data-path="feature-selection-using-simulated-annealing.html"><a href="feature-selection-using-simulated-annealing.html#internal-and-external-performance-estimates-1"><i class="fa fa-check"></i><b>22.2</b> Internal and External Performance Estimates</a></li>
<li class="chapter" data-level="22.3" data-path="feature-selection-using-simulated-annealing.html"><a href="feature-selection-using-simulated-annealing.html#basic-syntax-2"><i class="fa fa-check"></i><b>22.3</b> Basic Syntax</a></li>
<li class="chapter" data-level="22.4" data-path="feature-selection-using-simulated-annealing.html"><a href="feature-selection-using-simulated-annealing.html#saexample"><i class="fa fa-check"></i><b>22.4</b> Simulated Annealing Example</a></li>
<li class="chapter" data-level="22.5" data-path="feature-selection-using-simulated-annealing.html"><a href="feature-selection-using-simulated-annealing.html#customizing-the-search-1"><i class="fa fa-check"></i><b>22.5</b> Customizing the Search</a><ul>
<li class="chapter" data-level="22.5.1" data-path="feature-selection-using-simulated-annealing.html"><a href="feature-selection-using-simulated-annealing.html#the-fit-function-4"><i class="fa fa-check"></i><b>22.5.1</b> The <code>fit</code> Function</a></li>
<li class="chapter" data-level="22.5.2" data-path="feature-selection-using-simulated-annealing.html"><a href="feature-selection-using-simulated-annealing.html#the-pred-function-3"><i class="fa fa-check"></i><b>22.5.2</b> The <code>pred</code> Function</a></li>
<li class="chapter" data-level="22.5.3" data-path="feature-selection-using-simulated-annealing.html"><a href="feature-selection-using-simulated-annealing.html#the-fitness_intern-function-1"><i class="fa fa-check"></i><b>22.5.3</b> The <code>fitness_intern</code> Function</a></li>
<li class="chapter" data-level="22.5.4" data-path="feature-selection-using-simulated-annealing.html"><a href="feature-selection-using-simulated-annealing.html#the-fitness_extern-function-1"><i class="fa fa-check"></i><b>22.5.4</b> The <code>fitness_extern</code> Function</a></li>
<li class="chapter" data-level="22.5.5" data-path="feature-selection-using-simulated-annealing.html"><a href="feature-selection-using-simulated-annealing.html#the-initial-function-1"><i class="fa fa-check"></i><b>22.5.5</b> The <code>initial</code> Function</a></li>
<li class="chapter" data-level="22.5.6" data-path="feature-selection-using-simulated-annealing.html"><a href="feature-selection-using-simulated-annealing.html#the-perturb-function"><i class="fa fa-check"></i><b>22.5.6</b> The <code>perturb</code> Function</a></li>
<li class="chapter" data-level="22.5.7" data-path="feature-selection-using-simulated-annealing.html"><a href="feature-selection-using-simulated-annealing.html#the-prob-function"><i class="fa fa-check"></i><b>22.5.7</b> The <code>prob</code> Function</a></li>
</ul></li>
<li class="chapter" data-level="22.6" data-path="feature-selection-using-simulated-annealing.html"><a href="feature-selection-using-simulated-annealing.html#using-recipes-1"><i class="fa fa-check"></i><b>22.6</b> Using Recipes</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="data-sets.html"><a href="data-sets.html"><i class="fa fa-check"></i><b>23</b> Data Sets</a><ul>
<li class="chapter" data-level="23.1" data-path="data-sets.html"><a href="data-sets.html#blood-brain-barrier-data"><i class="fa fa-check"></i><b>23.1</b> Blood-Brain Barrier Data</a></li>
<li class="chapter" data-level="23.2" data-path="data-sets.html"><a href="data-sets.html#cox-2-activity-data"><i class="fa fa-check"></i><b>23.2</b> COX-2 Activity Data</a></li>
<li class="chapter" data-level="23.3" data-path="data-sets.html"><a href="data-sets.html#dhfr-inhibition"><i class="fa fa-check"></i><b>23.3</b> DHFR Inhibition</a></li>
<li class="chapter" data-level="23.4" data-path="data-sets.html"><a href="data-sets.html#tecator-nir-data"><i class="fa fa-check"></i><b>23.4</b> Tecator NIR Data</a></li>
<li class="chapter" data-level="23.5" data-path="data-sets.html"><a href="data-sets.html#fatty-acid-composition-data"><i class="fa fa-check"></i><b>23.5</b> Fatty Acid Composition Data</a></li>
<li class="chapter" data-level="23.6" data-path="data-sets.html"><a href="data-sets.html#german-credit-data"><i class="fa fa-check"></i><b>23.6</b> German Credit Data</a></li>
<li class="chapter" data-level="23.7" data-path="data-sets.html"><a href="data-sets.html#kelly-blue-book"><i class="fa fa-check"></i><b>23.7</b> Kelly Blue Book</a></li>
<li class="chapter" data-level="23.8" data-path="data-sets.html"><a href="data-sets.html#cell-body-segmentation-data"><i class="fa fa-check"></i><b>23.8</b> Cell Body Segmentation Data</a></li>
<li class="chapter" data-level="23.9" data-path="data-sets.html"><a href="data-sets.html#sacramento-house-price-data"><i class="fa fa-check"></i><b>23.9</b> Sacramento House Price Data</a></li>
<li class="chapter" data-level="23.10" data-path="data-sets.html"><a href="data-sets.html#animal-scat-data"><i class="fa fa-check"></i><b>23.10</b> Animal Scat Data</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="session-information.html"><a href="session-information.html"><i class="fa fa-check"></i><b>24</b> Session Information</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The <code>caret</code> Package</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="train-models-by-tag" class="section level1">
<h1><span class="header-section-number">7</span> <code>train</code> Models By Tag</h1>
<p>The following is a basic list of model types or relevant characteristics. There entires in these lists are arguable. For example: random forests theoretically use feature selection but effectively may not, support vector machines use L2 regularization etc.</p>
<div id="top">

</div>
<p>Contents</p>
<ul>
<li><a href="train-models-by-tag.html#Accepts_Case_Weights">Accepts Case Weights</a></li>
<li><a href="train-models-by-tag.html#Bagging">Bagging</a></li>
<li><a href="train-models-by-tag.html#Bayesian_Model">Bayesian Model</a></li>
<li><a href="train-models-by-tag.html#Binary_Predictors_Only">Binary Predictors Only</a></li>
<li><a href="train-models-by-tag.html#Boosting">Boosting</a></li>
<li><a href="train-models-by-tag.html#Categorical_Predictors_Only">Categorical Predictors Only</a></li>
<li><a href="train-models-by-tag.html#Cost_Sensitive_Learning">Cost Sensitive Learning</a></li>
<li><a href="train-models-by-tag.html#Discriminant_Analysis">Discriminant Analysis</a></li>
<li><a href="train-models-by-tag.html#Distance_Weighted_Discrimination">Distance Weighted Discrimination</a></li>
<li><a href="train-models-by-tag.html#Ensemble_Model">Ensemble Model</a></li>
<li><a href="train-models-by-tag.html#Feature_Extraction">Feature Extraction</a></li>
<li><a href="train-models-by-tag.html#Feature_Selection_Wrapper">Feature Selection Wrapper</a></li>
<li><a href="train-models-by-tag.html#Gaussian_Process">Gaussian Process</a></li>
<li><a href="train-models-by-tag.html#Generalized_Additive_Model">Generalized Additive Model</a></li>
<li><a href="train-models-by-tag.html#Generalized_Linear_Model">Generalized Linear Model</a></li>
<li><a href="train-models-by-tag.html#Handle_Missing_Predictor_Data">Handle Missing Predictor Data</a></li>
<li><a href="train-models-by-tag.html#Implicit_Feature_Selection">Implicit Feature Selection</a></li>
<li><a href="train-models-by-tag.html#Kernel_Method">Kernel Method</a></li>
<li><a href="train-models-by-tag.html#L1_Regularization">L1 Regularization</a></li>
<li><a href="train-models-by-tag.html#L2_Regularization">L2 Regularization</a></li>
<li><a href="train-models-by-tag.html#Linear_Classifier">Linear Classifier</a></li>
<li><a href="train-models-by-tag.html#Linear_Regression">Linear Regression</a></li>
<li><a href="train-models-by-tag.html#Logic_Regression">Logic Regression</a></li>
<li><a href="train-models-by-tag.html#Logistic_Regression">Logistic Regression</a></li>
<li><a href="train-models-by-tag.html#Mixture_Model">Mixture Model</a></li>
<li><a href="train-models-by-tag.html#Model_Tree">Model Tree</a></li>
<li><a href="train-models-by-tag.html#Multivariate_Adaptive_Regression_Splines">Multivariate Adaptive Regression Splines</a></li>
<li><a href="train-models-by-tag.html#Neural_Network">Neural Network</a></li>
<li><a href="train-models-by-tag.html#Oblique_Tree">Oblique Tree</a></li>
<li><a href="train-models-by-tag.html#Ordinal_Outcomes">Ordinal Outcomes</a></li>
<li><a href="train-models-by-tag.html#Partial_Least_Squares">Partial Least Squares</a></li>
<li><a href="train-models-by-tag.html#Patient_Rule_Induction_Method">Patient Rule Induction Method</a></li>
<li><a href="train-models-by-tag.html#Polynomial_Model">Polynomial Model</a></li>
<li><a href="train-models-by-tag.html#Prototype_Models">Prototype Models</a></li>
<li><a href="train-models-by-tag.html#Quantile_Regression">Quantile Regression</a></li>
<li><a href="train-models-by-tag.html#Radial_Basis_Function">Radial Basis Function</a></li>
<li><a href="train-models-by-tag.html#Random_Forest">Random Forest</a></li>
<li><a href="train-models-by-tag.html#Regularization">Regularization</a></li>
<li><a href="train-models-by-tag.html#Relevance_Vector_Machines">Relevance Vector Machines</a></li>
<li><a href="train-models-by-tag.html#Ridge_Regression">Ridge Regression</a></li>
<li><a href="train-models-by-tag.html#Robust_Methods">Robust Methods</a></li>
<li><a href="train-models-by-tag.html#Robust_Model">Robust Model</a></li>
<li><a href="train-models-by-tag.html#ROC_Curves">ROC Curves</a></li>
<li><a href="train-models-by-tag.html#Rule_Based_Model">Rule-Based Model</a></li>
<li><a href="train-models-by-tag.html#Self_Organising_Maps">Self-Organising Maps</a></li>
<li><a href="train-models-by-tag.html#String_Kernel">String Kernel</a></li>
<li><a href="train-models-by-tag.html#Support_Vector_Machines">Support Vector Machines</a></li>
<li><a href="train-models-by-tag.html#Supports_Class_Probabilities">Supports Class Probabilities</a></li>
<li><a href="train-models-by-tag.html#Text_Mining">Text Mining</a></li>
<li><a href="train-models-by-tag.html#Tree_Based_Model">Tree-Based Model</a></li>
<li><a href="train-models-by-tag.html#Two_Class_Only">Two Class Only</a></li>
</ul>
<div id="Accepts_Case_Weights">

</div>
<div id="accepts-case-weights" class="section level3">
<h3><span class="header-section-number">7.0.1</span> Accepts Case Weights</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Adjacent Categories Probability Model for Ordinal Data</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;vglmAdjCat&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>parallel</code> (Parallel Curves)</li>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>VGAM</code></p>
<p><strong>Bagged CART</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;treebag&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>ipred</code>, <code>plyr</code>, <code>e1071</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Bagged Flexible Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bagFDA&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Product Degree)</li>
<li><code>nprune</code> (#Terms)</li>
</ul>
<p>Required packages: <code>earth</code>, <code>mda</code></p>
<p>A model-specific variable importance metric is available. Notes: Unlike other packages used by <code>train</code>, the <code>earth</code> package is fully loaded when this model is used.</p>
<p><strong>Bagged MARS</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bagEarth&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nprune</code> (#Terms)</li>
<li><code>degree</code> (Product Degree)</li>
</ul>
<p>Required packages: <code>earth</code></p>
<p>A model-specific variable importance metric is available. Notes: Unlike other packages used by <code>train</code>, the <code>earth</code> package is fully loaded when this model is used.</p>
<p><strong>Bagged MARS using gCV Pruning</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bagEarthGCV&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Product Degree)</li>
</ul>
<p>Required packages: <code>earth</code></p>
<p>A model-specific variable importance metric is available. Notes: Unlike other packages used by <code>train</code>, the <code>earth</code> package is fully loaded when this model is used.</p>
<p><strong>Bayesian Generalized Linear Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bayesglm&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>arm</code></p>
<p><strong>Boosted Generalized Additive Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gamboost&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>prune</code> (AIC Prune?)</li>
</ul>
<p>Required packages: <code>mboost</code>, <code>plyr</code>, <code>import</code></p>
<p>Notes: The <code>prune</code> option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in <code>?mboost::mstop</code>. If pruning is not used, the ensemble makes predictions using the exact value of the <code>mstop</code> tuning parameter value.</p>
<p><strong>Boosted Generalized Linear Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmboost&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>prune</code> (AIC Prune?)</li>
</ul>
<p>Required packages: <code>plyr</code>, <code>mboost</code></p>
<p>A model-specific variable importance metric is available. Notes: The <code>prune</code> option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in <code>?mboost::mstop</code>. If pruning is not used, the ensemble makes predictions using the exact value of the <code>mstop</code> tuning parameter value.</p>
<p><strong>Boosted Tree</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;blackboost&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
</ul>
<p>Required packages: <code>party</code>, <code>mboost</code>, <code>plyr</code>, <code>partykit</code></p>
<p><strong>C5.0</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>trials</code> (# Boosting Iterations)</li>
<li><code>model</code> (Model Type)</li>
<li><code>winnow</code> (Winnow)</li>
</ul>
<p>Required packages: <code>C50</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>CART</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpart&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cp</code> (Complexity Parameter)</li>
</ul>
<p>Required packages: <code>rpart</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>CART</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpart1SE&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>rpart</code></p>
<p>A model-specific variable importance metric is available. Notes: This CART model replicates the same process used by the <code>rpart</code> function where the model complexity is determined using the one-standard error method. This procedure is replicated inside of the resampling done by <code>train</code> so that an external resampling estimate can be obtained.</p>
<p><strong>CART</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpart2&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxdepth</code> (Max Tree Depth)</li>
</ul>
<p>Required packages: <code>rpart</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>CART or Ordinal Responses</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpartScore&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cp</code> (Complexity Parameter)</li>
<li><code>split</code> (Split Function)</li>
<li><code>prune</code> (Pruning Measure)</li>
</ul>
<p>Required packages: <code>rpartScore</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>CHi-squared Automated Interaction Detection</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;chaid&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha2</code> (Merging Threshold)</li>
<li><code>alpha3</code> (Splitting former Merged Threshold)</li>
<li><code>alpha4</code> (
Splitting former Merged Threshold)</li>
</ul>
<p>Required packages: <code>CHAID</code></p>
<p><strong>Conditional Inference Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;cforest&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>party</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Conditional Inference Tree</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ctree&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mincriterion</code> (1 - P-Value Threshold)</li>
</ul>
<p>Required packages: <code>party</code></p>
<p><strong>Conditional Inference Tree</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ctree2&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxdepth</code> (Max Tree Depth)</li>
<li><code>mincriterion</code> (1 - P-Value Threshold)</li>
</ul>
<p>Required packages: <code>party</code></p>
<p><strong>Continuation Ratio Model for Ordinal Data</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;vglmContRatio&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>parallel</code> (Parallel Curves)</li>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>VGAM</code></p>
<p><strong>Cost-Sensitive C5.0</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0Cost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>trials</code> (# Boosting Iterations)</li>
<li><code>model</code> (Model Type)</li>
<li><code>winnow</code> (Winnow)</li>
<li><code>cost</code> (Cost)</li>
</ul>
<p>Required packages: <code>C50</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Cost-Sensitive CART</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpartCost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cp</code> (Complexity Parameter)</li>
<li><code>Cost</code> (Cost)</li>
</ul>
<p>Required packages: <code>rpart</code>, <code>plyr</code></p>
<p><strong>Cumulative Probability Model for Ordinal Data</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;vglmCumulative&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>parallel</code> (Parallel Curves)</li>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>VGAM</code></p>
<p><strong>DeepBoost</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;deepboost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>num_iter</code> (# Boosting Iterations)</li>
<li><code>tree_depth</code> (Tree Depth)</li>
<li><code>beta</code> (L1 Regularization)</li>
<li><code>lambda</code> (Tree Depth Regularization)</li>
<li><code>loss_type</code> (Loss)</li>
</ul>
<p>Required packages: <code>deepboost</code></p>
<p><strong>eXtreme Gradient Boosting</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;xgbDART&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nrounds</code> (# Boosting Iterations)</li>
<li><code>max_depth</code> (Max Tree Depth)</li>
<li><code>eta</code> (Shrinkage)</li>
<li><code>gamma</code> (Minimum Loss Reduction)</li>
<li><code>subsample</code> (Subsample Percentage)</li>
<li><code>colsample_bytree</code> (Subsample Ratio of Columns)</li>
<li><code>rate_drop</code> (Fraction of Trees Dropped)</li>
<li><code>skip_drop</code> (Prob. of Skipping Drop-out)</li>
<li><code>min_child_weight</code> (Minimum Sum of Instance Weight)</li>
</ul>
<p>Required packages: <code>xgboost</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>eXtreme Gradient Boosting</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;xgbTree&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nrounds</code> (# Boosting Iterations)</li>
<li><code>max_depth</code> (Max Tree Depth)</li>
<li><code>eta</code> (Shrinkage)</li>
<li><code>gamma</code> (Minimum Loss Reduction)</li>
<li><code>colsample_bytree</code> (Subsample Ratio of Columns)</li>
<li><code>min_child_weight</code> (Minimum Sum of Instance Weight)</li>
<li><code>subsample</code> (Subsample Percentage)</li>
</ul>
<p>Required packages: <code>xgboost</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Flexible Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;fda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Product Degree)</li>
<li><code>nprune</code> (#Terms)</li>
</ul>
<p>Required packages: <code>earth</code>, <code>mda</code></p>
<p>A model-specific variable importance metric is available. Notes: Unlike other packages used by <code>train</code>, the <code>earth</code> package is fully loaded when this model is used.</p>
<p><strong>Generalized Linear Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glm&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Generalized Linear Model with Stepwise Feature Selection</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmStepAIC&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>MASS</code></p>
<p><strong>Linear Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lm&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>intercept</code> (intercept)</li>
</ul>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Linear Regression with Stepwise Selection</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lmStepAIC&#39;</span></code></pre>
<p>Type: Regression</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>MASS</code></p>
<p><strong>Model Averaged Neural Network</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;avNNet&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>decay</code> (Weight Decay)</li>
<li><code>bag</code> (Bagging)</li>
</ul>
<p>Required packages: <code>nnet</code></p>
<p><strong>Multivariate Adaptive Regression Spline</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;earth&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nprune</code> (#Terms)</li>
<li><code>degree</code> (Product Degree)</li>
</ul>
<p>Required packages: <code>earth</code></p>
<p>A model-specific variable importance metric is available. Notes: Unlike other packages used by <code>train</code>, the <code>earth</code> package is fully loaded when this model is used.</p>
<p><strong>Multivariate Adaptive Regression Splines</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gcvEarth&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Product Degree)</li>
</ul>
<p>Required packages: <code>earth</code></p>
<p>A model-specific variable importance metric is available. Notes: Unlike other packages used by <code>train</code>, the <code>earth</code> package is fully loaded when this model is used.</p>
<p><strong>Negative Binomial Generalized Linear Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glm.nb&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>MASS</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Neural Network</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;nnet&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>decay</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>nnet</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Neural Networks with Feature Extraction</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pcaNNet&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>decay</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>nnet</code></p>
<p><strong>Ordered Logistic or Probit Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;polr&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>method</code> (parameter)</li>
</ul>
<p>Required packages: <code>MASS</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Penalized Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Shrinkage Penalty Coefficient)</li>
</ul>
<p>Required packages: <code>mda</code></p>
<p><strong>Penalized Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pda2&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>df</code> (Degrees of Freedom)</li>
</ul>
<p>Required packages: <code>mda</code></p>
<p><strong>Penalized Multinomial Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;multinom&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>decay</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>nnet</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Projection Pursuit Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ppr&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nterms</code> (# Terms)</li>
</ul>
<p><strong>Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ranger&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
<li><code>splitrule</code> (Splitting Rule)</li>
<li><code>min.node.size</code> (Minimal Node Size)</li>
</ul>
<p>Required packages: <code>e1071</code>, <code>ranger</code>, <code>dplyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Robust Linear Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rlm&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>intercept</code> (intercept)</li>
<li><code>psi</code> (psi)</li>
</ul>
<p>Required packages: <code>MASS</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Single C5.0 Ruleset</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0Rules&#39;</span></code></pre>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>C50</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Single C5.0 Tree</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0Tree&#39;</span></code></pre>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>C50</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Stochastic Gradient Boosting</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gbm&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>n.trees</code> (# Boosting Iterations)</li>
<li><code>interaction.depth</code> (Max Tree Depth)</li>
<li><code>shrinkage</code> (Shrinkage)</li>
<li><code>n.minobsinnode</code> (Min. Terminal Node Size)</li>
</ul>
<p>Required packages: <code>gbm</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Tree Models from Genetic Algorithms</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;evtree&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Complexity Parameter)</li>
</ul>
<p>Required packages: <code>evtree</code></p>
<div id="Bagging">

</div>
</div>
<div id="bagging" class="section level3">
<h3><span class="header-section-number">7.0.2</span> Bagging</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Bagged AdaBoost</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;AdaBag&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mfinal</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
</ul>
<p>Required packages: <code>adabag</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Bagged CART</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;treebag&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>ipred</code>, <code>plyr</code>, <code>e1071</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Bagged Flexible Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bagFDA&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Product Degree)</li>
<li><code>nprune</code> (#Terms)</li>
</ul>
<p>Required packages: <code>earth</code>, <code>mda</code></p>
<p>A model-specific variable importance metric is available. Notes: Unlike other packages used by <code>train</code>, the <code>earth</code> package is fully loaded when this model is used.</p>
<p><strong>Bagged Logic Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;logicBag&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nleaves</code> (Maximum Number of Leaves)</li>
<li><code>ntrees</code> (Number of Trees)</li>
</ul>
<p>Required packages: <code>logicFS</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>logicFS</code> package is fully loaded when this model is used.</p>
<p><strong>Bagged MARS</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bagEarth&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nprune</code> (#Terms)</li>
<li><code>degree</code> (Product Degree)</li>
</ul>
<p>Required packages: <code>earth</code></p>
<p>A model-specific variable importance metric is available. Notes: Unlike other packages used by <code>train</code>, the <code>earth</code> package is fully loaded when this model is used.</p>
<p><strong>Bagged MARS using gCV Pruning</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bagEarthGCV&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Product Degree)</li>
</ul>
<p>Required packages: <code>earth</code></p>
<p>A model-specific variable importance metric is available. Notes: Unlike other packages used by <code>train</code>, the <code>earth</code> package is fully loaded when this model is used.</p>
<p><strong>Bagged Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bag&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>vars</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>caret</code></p>
<p><strong>Conditional Inference Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;cforest&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>party</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Ensembles of Generalized Linear Models</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;randomGLM&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxInteractionOrder</code> (Interaction Order)</li>
</ul>
<p>Required packages: <code>randomGLM</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>randomGLM</code> package is fully loaded when this model is used.</p>
<p><strong>Model Averaged Neural Network</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;avNNet&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>decay</code> (Weight Decay)</li>
<li><code>bag</code> (Bagging)</li>
</ul>
<p>Required packages: <code>nnet</code></p>
<p><strong>Parallel Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;parRF&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>e1071</code>, <code>randomForest</code>, <code>foreach</code>, <code>import</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Quantile Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;qrf&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>quantregForest</code></p>
<p><strong>Quantile Regression Neural Network</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;qrnn&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>n.hidden</code> (#Hidden Units)</li>
<li><code>penalty</code> ( Weight Decay)</li>
<li><code>bag</code> (Bagged Models?)</li>
</ul>
<p>Required packages: <code>qrnn</code></p>
<p><strong>Random Ferns</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rFerns&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>depth</code> (Fern Depth)</li>
</ul>
<p>Required packages: <code>rFerns</code></p>
<p><strong>Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ordinalRF&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nsets</code> (# score sets tried prior to the approximation)</li>
<li><code>ntreeperdiv</code> (# of trees (small RFs))</li>
<li><code>ntreefinal</code> (# of trees (final RF))</li>
</ul>
<p>Required packages: <code>e1071</code>, <code>ranger</code>, <code>dplyr</code>, <code>ordinalForest</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ranger&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
<li><code>splitrule</code> (Splitting Rule)</li>
<li><code>min.node.size</code> (Minimal Node Size)</li>
</ul>
<p>Required packages: <code>e1071</code>, <code>ranger</code>, <code>dplyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;Rborist&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>predFixed</code> (#Randomly Selected Predictors)</li>
<li><code>minNode</code> (Minimal Node Size)</li>
</ul>
<p>Required packages: <code>Rborist</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rf&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>randomForest</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Random Forest by Randomization</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;extraTrees&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (# Randomly Selected Predictors)</li>
<li><code>numRandomCuts</code> (# Random Cuts)</li>
</ul>
<p>Required packages: <code>extraTrees</code></p>
<p><strong>Random Forest Rule-Based Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rfRules&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
<li><code>maxdepth</code> (Maximum Rule Depth)</li>
</ul>
<p>Required packages: <code>randomForest</code>, <code>inTrees</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Regularized Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;RRF&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
<li><code>coefReg</code> (Regularization Value)</li>
<li><code>coefImp</code> (Importance Coefficient)</li>
</ul>
<p>Required packages: <code>randomForest</code>, <code>RRF</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Regularized Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;RRFglobal&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
<li><code>coefReg</code> (Regularization Value)</li>
</ul>
<p>Required packages: <code>RRF</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Weighted Subspace Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;wsrf&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>wsrf</code></p>
<div id="Bayesian_Model">

</div>
</div>
<div id="bayesian-model" class="section level3">
<h3><span class="header-section-number">7.0.3</span> Bayesian Model</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Bayesian Additive Regression Trees</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bartMachine&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>num_trees</code> (#Trees)</li>
<li><code>k</code> (Prior Boundary)</li>
<li><code>alpha</code> (Base Terminal Node Hyperparameter)</li>
<li><code>beta</code> (Power Terminal Node Hyperparameter)</li>
<li><code>nu</code> (Degrees of Freedom)</li>
</ul>
<p>Required packages: <code>bartMachine</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Bayesian Generalized Linear Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bayesglm&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>arm</code></p>
<p><strong>Bayesian Regularized Neural Networks</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;brnn&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>neurons</code> (# Neurons)</li>
</ul>
<p>Required packages: <code>brnn</code></p>
<p><strong>Bayesian Ridge Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bridge&#39;</span></code></pre>
<p>Type: Regression</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>monomvn</code></p>
<p><strong>Bayesian Ridge Regression (Model Averaged)</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;blassoAveraged&#39;</span></code></pre>
<p>Type: Regression</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>monomvn</code></p>
<p>Notes: This model makes predictions by averaging the predictions based on the posterior estimates of the regression coefficients. While it is possible that some of these posterior estimates are zero for non-informative predictors, the final predicted value may be a function of many (or even all) predictors.</p>
<p><strong>Model Averaged Naive Bayes Classifier</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;manb&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>smooth</code> (Smoothing Parameter)</li>
<li><code>prior</code> (Prior Probability)</li>
</ul>
<p>Required packages: <code>bnclassify</code></p>
<p><strong>Naive Bayes</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;naive_bayes&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>laplace</code> (Laplace Correction)</li>
<li><code>usekernel</code> (Distribution Type)</li>
<li><code>adjust</code> (Bandwidth Adjustment)</li>
</ul>
<p>Required packages: <code>naivebayes</code></p>
<p><strong>Naive Bayes</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;nb&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>fL</code> (Laplace Correction)</li>
<li><code>usekernel</code> (Distribution Type)</li>
<li><code>adjust</code> (Bandwidth Adjustment)</li>
</ul>
<p>Required packages: <code>klaR</code></p>
<p><strong>Naive Bayes Classifier</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;nbDiscrete&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>smooth</code> (Smoothing Parameter)</li>
</ul>
<p>Required packages: <code>bnclassify</code></p>
<p><strong>Naive Bayes Classifier with Attribute Weighting</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;awnb&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>smooth</code> (Smoothing Parameter)</li>
</ul>
<p>Required packages: <code>bnclassify</code></p>
<p><strong>Semi-Naive Structure Learner Wrapper</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;nbSearch&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>k</code> (#Folds)</li>
<li><code>epsilon</code> (Minimum Absolute Improvement)</li>
<li><code>smooth</code> (Smoothing Parameter)</li>
<li><code>final_smooth</code> (Final Smoothing Parameter)</li>
<li><code>direction</code> (Search Direction)</li>
</ul>
<p>Required packages: <code>bnclassify</code></p>
<p><strong>Spike and Slab Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;spikeslab&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>vars</code> (Variables Retained)</li>
</ul>
<p>Required packages: <code>spikeslab</code>, <code>plyr</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>spikeslab</code> package is fully loaded when this model is used.</p>
<p><strong>The Bayesian lasso</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;blasso&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sparsity</code> (Sparsity Threshold)</li>
</ul>
<p>Required packages: <code>monomvn</code></p>
<p>Notes: This model creates predictions using the mean of the posterior distributions but sets some parameters specifically to zero based on the tuning parameter <code>sparsity</code>. For example, when <code>sparsity = .5</code>, only coefficients where at least half the posterior estimates are nonzero are used.</p>
<p><strong>Tree Augmented Naive Bayes Classifier</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;tan&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>score</code> (Score Function)</li>
<li><code>smooth</code> (Smoothing Parameter)</li>
</ul>
<p>Required packages: <code>bnclassify</code></p>
<p><strong>Tree Augmented Naive Bayes Classifier Structure Learner Wrapper</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;tanSearch&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>k</code> (#Folds)</li>
<li><code>epsilon</code> (Minimum Absolute Improvement)</li>
<li><code>smooth</code> (Smoothing Parameter)</li>
<li><code>final_smooth</code> (Final Smoothing Parameter)</li>
<li><code>sp</code> (Super-Parent)</li>
</ul>
<p>Required packages: <code>bnclassify</code></p>
<p><strong>Tree Augmented Naive Bayes Classifier with Attribute Weighting</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;awtan&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>score</code> (Score Function)</li>
<li><code>smooth</code> (Smoothing Parameter)</li>
</ul>
<p>Required packages: <code>bnclassify</code></p>
<p><strong>Variational Bayesian Multinomial Probit Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;vbmpRadial&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>estimateTheta</code> (Theta Estimated)</li>
</ul>
<p>Required packages: <code>vbmp</code></p>
<div id="Binary_Predictors_Only">

</div>
</div>
<div id="binary-predictors-only" class="section level3">
<h3><span class="header-section-number">7.0.4</span> Binary Predictors Only</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Bagged Logic Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;logicBag&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nleaves</code> (Maximum Number of Leaves)</li>
<li><code>ntrees</code> (Number of Trees)</li>
</ul>
<p>Required packages: <code>logicFS</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>logicFS</code> package is fully loaded when this model is used.</p>
<p><strong>Binary Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;binda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda.freqs</code> (Shrinkage Intensity)</li>
</ul>
<p>Required packages: <code>binda</code></p>
<p><strong>Logic Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;logreg&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>treesize</code> (Maximum Number of Leaves)</li>
<li><code>ntrees</code> (Number of Trees)</li>
</ul>
<p>Required packages: <code>LogicReg</code></p>
<div id="Boosting">

</div>
</div>
<div id="boosting" class="section level3">
<h3><span class="header-section-number">7.0.5</span> Boosting</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>AdaBoost Classification Trees</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;adaboost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nIter</code> (#Trees)</li>
<li><code>method</code> (Method)</li>
</ul>
<p>Required packages: <code>fastAdaboost</code></p>
<p><strong>AdaBoost.M1</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;AdaBoost.M1&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mfinal</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
<li><code>coeflearn</code> (Coefficient Type)</li>
</ul>
<p>Required packages: <code>adabag</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Bagged AdaBoost</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;AdaBag&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mfinal</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
</ul>
<p>Required packages: <code>adabag</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Boosted Classification Trees</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ada&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>iter</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
<li><code>nu</code> (Learning Rate)</li>
</ul>
<p>Required packages: <code>ada</code>, <code>plyr</code></p>
<p><strong>Boosted Generalized Additive Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gamboost&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>prune</code> (AIC Prune?)</li>
</ul>
<p>Required packages: <code>mboost</code>, <code>plyr</code>, <code>import</code></p>
<p>Notes: The <code>prune</code> option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in <code>?mboost::mstop</code>. If pruning is not used, the ensemble makes predictions using the exact value of the <code>mstop</code> tuning parameter value.</p>
<p><strong>Boosted Generalized Linear Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmboost&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>prune</code> (AIC Prune?)</li>
</ul>
<p>Required packages: <code>plyr</code>, <code>mboost</code></p>
<p>A model-specific variable importance metric is available. Notes: The <code>prune</code> option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in <code>?mboost::mstop</code>. If pruning is not used, the ensemble makes predictions using the exact value of the <code>mstop</code> tuning parameter value.</p>
<p><strong>Boosted Linear Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;BstLm&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>nu</code> (Shrinkage)</li>
</ul>
<p>Required packages: <code>bst</code>, <code>plyr</code></p>
<p><strong>Boosted Logistic Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;LogitBoost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nIter</code> (# Boosting Iterations)</li>
</ul>
<p>Required packages: <code>caTools</code></p>
<p><strong>Boosted Smoothing Spline</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bstSm&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>nu</code> (Shrinkage)</li>
</ul>
<p>Required packages: <code>bst</code>, <code>plyr</code></p>
<p><strong>Boosted Tree</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;blackboost&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
</ul>
<p>Required packages: <code>party</code>, <code>mboost</code>, <code>plyr</code>, <code>partykit</code></p>
<p><strong>Boosted Tree</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bstTree&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
<li><code>nu</code> (Shrinkage)</li>
</ul>
<p>Required packages: <code>bst</code>, <code>plyr</code></p>
<p><strong>C5.0</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>trials</code> (# Boosting Iterations)</li>
<li><code>model</code> (Model Type)</li>
<li><code>winnow</code> (Winnow)</li>
</ul>
<p>Required packages: <code>C50</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Cost-Sensitive C5.0</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0Cost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>trials</code> (# Boosting Iterations)</li>
<li><code>model</code> (Model Type)</li>
<li><code>winnow</code> (Winnow)</li>
<li><code>cost</code> (Cost)</li>
</ul>
<p>Required packages: <code>C50</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Cubist</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;cubist&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>committees</code> (#Committees)</li>
<li><code>neighbors</code> (#Instances)</li>
</ul>
<p>Required packages: <code>Cubist</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>DeepBoost</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;deepboost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>num_iter</code> (# Boosting Iterations)</li>
<li><code>tree_depth</code> (Tree Depth)</li>
<li><code>beta</code> (L1 Regularization)</li>
<li><code>lambda</code> (Tree Depth Regularization)</li>
<li><code>loss_type</code> (Loss)</li>
</ul>
<p>Required packages: <code>deepboost</code></p>
<p><strong>eXtreme Gradient Boosting</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;xgbDART&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nrounds</code> (# Boosting Iterations)</li>
<li><code>max_depth</code> (Max Tree Depth)</li>
<li><code>eta</code> (Shrinkage)</li>
<li><code>gamma</code> (Minimum Loss Reduction)</li>
<li><code>subsample</code> (Subsample Percentage)</li>
<li><code>colsample_bytree</code> (Subsample Ratio of Columns)</li>
<li><code>rate_drop</code> (Fraction of Trees Dropped)</li>
<li><code>skip_drop</code> (Prob. of Skipping Drop-out)</li>
<li><code>min_child_weight</code> (Minimum Sum of Instance Weight)</li>
</ul>
<p>Required packages: <code>xgboost</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>eXtreme Gradient Boosting</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;xgbLinear&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nrounds</code> (# Boosting Iterations)</li>
<li><code>lambda</code> (L2 Regularization)</li>
<li><code>alpha</code> (L1 Regularization)</li>
<li><code>eta</code> (Learning Rate)</li>
</ul>
<p>Required packages: <code>xgboost</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>eXtreme Gradient Boosting</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;xgbTree&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nrounds</code> (# Boosting Iterations)</li>
<li><code>max_depth</code> (Max Tree Depth)</li>
<li><code>eta</code> (Shrinkage)</li>
<li><code>gamma</code> (Minimum Loss Reduction)</li>
<li><code>colsample_bytree</code> (Subsample Ratio of Columns)</li>
<li><code>min_child_weight</code> (Minimum Sum of Instance Weight)</li>
<li><code>subsample</code> (Subsample Percentage)</li>
</ul>
<p>Required packages: <code>xgboost</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Gradient Boosting Machines</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gbm_h2o&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ntrees</code> (# Boosting Iterations)</li>
<li><code>max_depth</code> (Max Tree Depth)</li>
<li><code>min_rows</code> (Min. Terminal Node Size)</li>
<li><code>learn_rate</code> (Shrinkage)</li>
<li><code>col_sample_rate</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>h2o</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Stochastic Gradient Boosting</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gbm&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>n.trees</code> (# Boosting Iterations)</li>
<li><code>interaction.depth</code> (Max Tree Depth)</li>
<li><code>shrinkage</code> (Shrinkage)</li>
<li><code>n.minobsinnode</code> (Min. Terminal Node Size)</li>
</ul>
<p>Required packages: <code>gbm</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<div id="Categorical_Predictors_Only">

</div>
</div>
<div id="categorical-predictors-only" class="section level3">
<h3><span class="header-section-number">7.0.6</span> Categorical Predictors Only</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Model Averaged Naive Bayes Classifier</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;manb&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>smooth</code> (Smoothing Parameter)</li>
<li><code>prior</code> (Prior Probability)</li>
</ul>
<p>Required packages: <code>bnclassify</code></p>
<p><strong>Naive Bayes Classifier</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;nbDiscrete&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>smooth</code> (Smoothing Parameter)</li>
</ul>
<p>Required packages: <code>bnclassify</code></p>
<p><strong>Naive Bayes Classifier with Attribute Weighting</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;awnb&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>smooth</code> (Smoothing Parameter)</li>
</ul>
<p>Required packages: <code>bnclassify</code></p>
<p><strong>Semi-Naive Structure Learner Wrapper</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;nbSearch&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>k</code> (#Folds)</li>
<li><code>epsilon</code> (Minimum Absolute Improvement)</li>
<li><code>smooth</code> (Smoothing Parameter)</li>
<li><code>final_smooth</code> (Final Smoothing Parameter)</li>
<li><code>direction</code> (Search Direction)</li>
</ul>
<p>Required packages: <code>bnclassify</code></p>
<p><strong>Tree Augmented Naive Bayes Classifier</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;tan&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>score</code> (Score Function)</li>
<li><code>smooth</code> (Smoothing Parameter)</li>
</ul>
<p>Required packages: <code>bnclassify</code></p>
<p><strong>Tree Augmented Naive Bayes Classifier Structure Learner Wrapper</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;tanSearch&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>k</code> (#Folds)</li>
<li><code>epsilon</code> (Minimum Absolute Improvement)</li>
<li><code>smooth</code> (Smoothing Parameter)</li>
<li><code>final_smooth</code> (Final Smoothing Parameter)</li>
<li><code>sp</code> (Super-Parent)</li>
</ul>
<p>Required packages: <code>bnclassify</code></p>
<p><strong>Tree Augmented Naive Bayes Classifier with Attribute Weighting</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;awtan&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>score</code> (Score Function)</li>
<li><code>smooth</code> (Smoothing Parameter)</li>
</ul>
<p>Required packages: <code>bnclassify</code></p>
<div id="Cost_Sensitive_Learning">

</div>
</div>
<div id="cost-sensitive-learning" class="section level3">
<h3><span class="header-section-number">7.0.7</span> Cost Sensitive Learning</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Cost-Sensitive C5.0</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0Cost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>trials</code> (# Boosting Iterations)</li>
<li><code>model</code> (Model Type)</li>
<li><code>winnow</code> (Winnow)</li>
<li><code>cost</code> (Cost)</li>
</ul>
<p>Required packages: <code>C50</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Cost-Sensitive CART</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpartCost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cp</code> (Complexity Parameter)</li>
<li><code>Cost</code> (Cost)</li>
</ul>
<p>Required packages: <code>rpart</code>, <code>plyr</code></p>
<p><strong>L2 Regularized Linear Support Vector Machines with Class Weights</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinearWeights2&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
<li><code>Loss</code> (Loss Function)</li>
<li><code>weight</code> (Class Weight)</li>
</ul>
<p>Required packages: <code>LiblineaR</code></p>
<p><strong>Linear Support Vector Machines with Class Weights</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinearWeights&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
<li><code>weight</code> (Class Weight)</li>
</ul>
<p>Required packages: <code>e1071</code></p>
<p><strong>Multilayer Perceptron Network with Dropout</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mlpKerasDropoutCost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>dropout</code> (Dropout Rate)</li>
<li><code>batch_size</code> (Batch Size)</li>
<li><code>lr</code> (Learning Rate)</li>
<li><code>rho</code> (Rho)</li>
<li><code>decay</code> (Learning Rate Decay)</li>
<li><code>cost</code> (Cost)</li>
<li><code>activation</code> (Activation Function)</li>
</ul>
<p>Required packages: <code>keras</code></p>
<p>Notes: After <code>train</code> completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use <code>keras::unsearlize_model(object$finalModel$object)</code> in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Finally, the cost parameter weights the first class in the outcome vector. Unlike other packages used by <code>train</code>, the <code>dplyr</code> package is fully loaded when this model is used.</p>
<p><strong>Multilayer Perceptron Network with Weight Decay</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mlpKerasDecayCost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>lambda</code> (L2 Regularization)</li>
<li><code>batch_size</code> (Batch Size)</li>
<li><code>lr</code> (Learning Rate)</li>
<li><code>rho</code> (Rho)</li>
<li><code>decay</code> (Learning Rate Decay)</li>
<li><code>cost</code> (Cost)</li>
<li><code>activation</code> (Activation Function)</li>
</ul>
<p>Required packages: <code>keras</code></p>
<p>Notes: After <code>train</code> completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use <code>keras::unsearlize_model(object$finalModel$object)</code> in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Finally, the cost parameter weights the first class in the outcome vector. Unlike other packages used by <code>train</code>, the <code>dplyr</code> package is fully loaded when this model is used.</p>
<p><strong>Support Vector Machines with Class Weights</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmRadialWeights&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
<li><code>C</code> (Cost)</li>
<li><code>Weight</code> (Weight)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<div id="Discriminant_Analysis">

</div>
</div>
<div id="discriminant-analysis" class="section level3">
<h3><span class="header-section-number">7.0.8</span> Discriminant Analysis</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Adaptive Mixture Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;amdai&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>model</code> (Model Type)</li>
</ul>
<p>Required packages: <code>adaptDA</code></p>
<p><strong>Binary Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;binda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda.freqs</code> (Shrinkage Intensity)</li>
</ul>
<p>Required packages: <code>binda</code></p>
<p><strong>Diagonal Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>model</code> (Model)</li>
<li><code>shrinkage</code> (Shrinkage Type)</li>
</ul>
<p>Required packages: <code>sparsediscrim</code></p>
<p><strong>Distance Weighted Discrimination with Polynomial Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dwdPoly&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>qval</code> (q)</li>
<li><code>degree</code> (Polynomial Degree)</li>
<li><code>scale</code> (Scale)</li>
</ul>
<p>Required packages: <code>kerndwd</code></p>
<p><strong>Distance Weighted Discrimination with Radial Basis Function Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dwdRadial&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>qval</code> (q)</li>
<li><code>sigma</code> (Sigma)</li>
</ul>
<p>Required packages: <code>kernlab</code>, <code>kerndwd</code></p>
<p><strong>Factor-Based Linear Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;RFlda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>q</code> (# Factors)</li>
</ul>
<p>Required packages: <code>HiDimDA</code></p>
<p><strong>Heteroscedastic Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;hda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>gamma</code> (Gamma)</li>
<li><code>lambda</code> (Lambda)</li>
<li><code>newdim</code> (Dimension of the Discriminative Subspace)</li>
</ul>
<p>Required packages: <code>hda</code></p>
<p><strong>High Dimensional Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;hdda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>threshold</code> (Threshold)</li>
<li><code>model</code> (Model Type)</li>
</ul>
<p>Required packages: <code>HDclassif</code></p>
<p><strong>High-Dimensional Regularized Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;hdrda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>gamma</code> (Gamma)</li>
<li><code>lambda</code> (Lambda)</li>
<li><code>shrinkage_type</code> (Shrinkage Type)</li>
</ul>
<p>Required packages: <code>sparsediscrim</code></p>
<p><strong>Linear Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>MASS</code></p>
<p><strong>Linear Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lda2&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>dimen</code> (#Discriminant Functions)</li>
</ul>
<p>Required packages: <code>MASS</code></p>
<p><strong>Linear Discriminant Analysis with Stepwise Feature Selection</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;stepLDA&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxvar</code> (Maximum #Variables)</li>
<li><code>direction</code> (Search Direction)</li>
</ul>
<p>Required packages: <code>klaR</code>, <code>MASS</code></p>
<p><strong>Linear Distance Weighted Discrimination</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dwdLinear&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>qval</code> (q)</li>
</ul>
<p>Required packages: <code>kerndwd</code></p>
<p><strong>Localized Linear Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;loclda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>k</code> (#Nearest Neighbors)</li>
</ul>
<p>Required packages: <code>klaR</code></p>
<p><strong>Maximum Uncertainty Linear Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;Mlda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>HiDimDA</code></p>
<p><strong>Mixture Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>subclasses</code> (#Subclasses Per Class)</li>
</ul>
<p>Required packages: <code>mda</code></p>
<p><strong>Penalized Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Shrinkage Penalty Coefficient)</li>
</ul>
<p>Required packages: <code>mda</code></p>
<p><strong>Penalized Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pda2&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>df</code> (Degrees of Freedom)</li>
</ul>
<p>Required packages: <code>mda</code></p>
<p><strong>Penalized Linear Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;PenalizedLDA&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (L1 Penalty)</li>
<li><code>K</code> (#Discriminant Functions)</li>
</ul>
<p>Required packages: <code>penalizedLDA</code>, <code>plyr</code></p>
<p><strong>Quadratic Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;qda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>MASS</code></p>
<p><strong>Quadratic Discriminant Analysis with Stepwise Feature Selection</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;stepQDA&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxvar</code> (Maximum #Variables)</li>
<li><code>direction</code> (Search Direction)</li>
</ul>
<p>Required packages: <code>klaR</code>, <code>MASS</code></p>
<p><strong>Regularized Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>gamma</code> (Gamma)</li>
<li><code>lambda</code> (Lambda)</li>
</ul>
<p>Required packages: <code>klaR</code></p>
<p><strong>Regularized Linear Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rlda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>estimator</code> (Regularization Method)</li>
</ul>
<p>Required packages: <code>sparsediscrim</code></p>
<p><strong>Robust Linear Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;Linda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>rrcov</code></p>
<p><strong>Robust Mixture Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rmda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K</code> (#Subclasses Per Class)</li>
<li><code>model</code> (Model)</li>
</ul>
<p>Required packages: <code>robustDA</code></p>
<p><strong>Robust Quadratic Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;QdaCov&#39;</span></code></pre>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>rrcov</code></p>
<p><strong>Robust Regularized Linear Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rrlda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Penalty Parameter)</li>
<li><code>hp</code> (Robustness Parameter)</li>
<li><code>penalty</code> (Penalty Type)</li>
</ul>
<p>Required packages: <code>rrlda</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>rrlda</code> package is fully loaded when this model is used.</p>
<p><strong>Shrinkage Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;sda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>diagonal</code> (Diagonalize)</li>
<li><code>lambda</code> (shrinkage)</li>
</ul>
<p>Required packages: <code>sda</code></p>
<p><strong>Sparse Linear Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;sparseLDA&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>NumVars</code> (# Predictors)</li>
<li><code>lambda</code> (Lambda)</li>
</ul>
<p>Required packages: <code>sparseLDA</code></p>
<p><strong>Sparse Mixture Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;smda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>NumVars</code> (# Predictors)</li>
<li><code>lambda</code> (Lambda)</li>
<li><code>R</code> (# Subclasses)</li>
</ul>
<p>Required packages: <code>sparseLDA</code></p>
<p><strong>Stabilized Linear Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;slda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>ipred</code></p>
<div id="Distance_Weighted_Discrimination">

</div>
</div>
<div id="distance-weighted-discrimination" class="section level3">
<h3><span class="header-section-number">7.0.9</span> Distance Weighted Discrimination</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Distance Weighted Discrimination with Polynomial Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dwdPoly&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>qval</code> (q)</li>
<li><code>degree</code> (Polynomial Degree)</li>
<li><code>scale</code> (Scale)</li>
</ul>
<p>Required packages: <code>kerndwd</code></p>
<p><strong>Distance Weighted Discrimination with Radial Basis Function Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dwdRadial&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>qval</code> (q)</li>
<li><code>sigma</code> (Sigma)</li>
</ul>
<p>Required packages: <code>kernlab</code>, <code>kerndwd</code></p>
<p><strong>Linear Distance Weighted Discrimination</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dwdLinear&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>qval</code> (q)</li>
</ul>
<p>Required packages: <code>kerndwd</code></p>
<p><strong>Sparse Distance Weighted Discrimination</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;sdwd&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (L1 Penalty)</li>
<li><code>lambda2</code> (L2 Penalty)</li>
</ul>
<p>Required packages: <code>sdwd</code></p>
<p>A model-specific variable importance metric is available.</p>
<div id="Ensemble_Model">

</div>
</div>
<div id="ensemble-model" class="section level3">
<h3><span class="header-section-number">7.0.10</span> Ensemble Model</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>AdaBoost Classification Trees</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;adaboost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nIter</code> (#Trees)</li>
<li><code>method</code> (Method)</li>
</ul>
<p>Required packages: <code>fastAdaboost</code></p>
<p><strong>AdaBoost.M1</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;AdaBoost.M1&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mfinal</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
<li><code>coeflearn</code> (Coefficient Type)</li>
</ul>
<p>Required packages: <code>adabag</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Bagged AdaBoost</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;AdaBag&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mfinal</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
</ul>
<p>Required packages: <code>adabag</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Bagged CART</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;treebag&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>ipred</code>, <code>plyr</code>, <code>e1071</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Bagged Flexible Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bagFDA&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Product Degree)</li>
<li><code>nprune</code> (#Terms)</li>
</ul>
<p>Required packages: <code>earth</code>, <code>mda</code></p>
<p>A model-specific variable importance metric is available. Notes: Unlike other packages used by <code>train</code>, the <code>earth</code> package is fully loaded when this model is used.</p>
<p><strong>Bagged Logic Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;logicBag&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nleaves</code> (Maximum Number of Leaves)</li>
<li><code>ntrees</code> (Number of Trees)</li>
</ul>
<p>Required packages: <code>logicFS</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>logicFS</code> package is fully loaded when this model is used.</p>
<p><strong>Bagged MARS</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bagEarth&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nprune</code> (#Terms)</li>
<li><code>degree</code> (Product Degree)</li>
</ul>
<p>Required packages: <code>earth</code></p>
<p>A model-specific variable importance metric is available. Notes: Unlike other packages used by <code>train</code>, the <code>earth</code> package is fully loaded when this model is used.</p>
<p><strong>Bagged MARS using gCV Pruning</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bagEarthGCV&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Product Degree)</li>
</ul>
<p>Required packages: <code>earth</code></p>
<p>A model-specific variable importance metric is available. Notes: Unlike other packages used by <code>train</code>, the <code>earth</code> package is fully loaded when this model is used.</p>
<p><strong>Bagged Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bag&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>vars</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>caret</code></p>
<p><strong>Boosted Classification Trees</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ada&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>iter</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
<li><code>nu</code> (Learning Rate)</li>
</ul>
<p>Required packages: <code>ada</code>, <code>plyr</code></p>
<p><strong>Boosted Generalized Additive Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gamboost&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>prune</code> (AIC Prune?)</li>
</ul>
<p>Required packages: <code>mboost</code>, <code>plyr</code>, <code>import</code></p>
<p>Notes: The <code>prune</code> option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in <code>?mboost::mstop</code>. If pruning is not used, the ensemble makes predictions using the exact value of the <code>mstop</code> tuning parameter value.</p>
<p><strong>Boosted Generalized Linear Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmboost&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>prune</code> (AIC Prune?)</li>
</ul>
<p>Required packages: <code>plyr</code>, <code>mboost</code></p>
<p>A model-specific variable importance metric is available. Notes: The <code>prune</code> option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in <code>?mboost::mstop</code>. If pruning is not used, the ensemble makes predictions using the exact value of the <code>mstop</code> tuning parameter value.</p>
<p><strong>Boosted Linear Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;BstLm&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>nu</code> (Shrinkage)</li>
</ul>
<p>Required packages: <code>bst</code>, <code>plyr</code></p>
<p><strong>Boosted Logistic Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;LogitBoost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nIter</code> (# Boosting Iterations)</li>
</ul>
<p>Required packages: <code>caTools</code></p>
<p><strong>Boosted Smoothing Spline</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bstSm&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>nu</code> (Shrinkage)</li>
</ul>
<p>Required packages: <code>bst</code>, <code>plyr</code></p>
<p><strong>Boosted Tree</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;blackboost&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
</ul>
<p>Required packages: <code>party</code>, <code>mboost</code>, <code>plyr</code>, <code>partykit</code></p>
<p><strong>Boosted Tree</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bstTree&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
<li><code>nu</code> (Shrinkage)</li>
</ul>
<p>Required packages: <code>bst</code>, <code>plyr</code></p>
<p><strong>C5.0</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>trials</code> (# Boosting Iterations)</li>
<li><code>model</code> (Model Type)</li>
<li><code>winnow</code> (Winnow)</li>
</ul>
<p>Required packages: <code>C50</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Conditional Inference Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;cforest&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>party</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Cost-Sensitive C5.0</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0Cost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>trials</code> (# Boosting Iterations)</li>
<li><code>model</code> (Model Type)</li>
<li><code>winnow</code> (Winnow)</li>
<li><code>cost</code> (Cost)</li>
</ul>
<p>Required packages: <code>C50</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Cubist</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;cubist&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>committees</code> (#Committees)</li>
<li><code>neighbors</code> (#Instances)</li>
</ul>
<p>Required packages: <code>Cubist</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>DeepBoost</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;deepboost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>num_iter</code> (# Boosting Iterations)</li>
<li><code>tree_depth</code> (Tree Depth)</li>
<li><code>beta</code> (L1 Regularization)</li>
<li><code>lambda</code> (Tree Depth Regularization)</li>
<li><code>loss_type</code> (Loss)</li>
</ul>
<p>Required packages: <code>deepboost</code></p>
<p><strong>Ensembles of Generalized Linear Models</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;randomGLM&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxInteractionOrder</code> (Interaction Order)</li>
</ul>
<p>Required packages: <code>randomGLM</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>randomGLM</code> package is fully loaded when this model is used.</p>
<p><strong>eXtreme Gradient Boosting</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;xgbDART&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nrounds</code> (# Boosting Iterations)</li>
<li><code>max_depth</code> (Max Tree Depth)</li>
<li><code>eta</code> (Shrinkage)</li>
<li><code>gamma</code> (Minimum Loss Reduction)</li>
<li><code>subsample</code> (Subsample Percentage)</li>
<li><code>colsample_bytree</code> (Subsample Ratio of Columns)</li>
<li><code>rate_drop</code> (Fraction of Trees Dropped)</li>
<li><code>skip_drop</code> (Prob. of Skipping Drop-out)</li>
<li><code>min_child_weight</code> (Minimum Sum of Instance Weight)</li>
</ul>
<p>Required packages: <code>xgboost</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>eXtreme Gradient Boosting</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;xgbLinear&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nrounds</code> (# Boosting Iterations)</li>
<li><code>lambda</code> (L2 Regularization)</li>
<li><code>alpha</code> (L1 Regularization)</li>
<li><code>eta</code> (Learning Rate)</li>
</ul>
<p>Required packages: <code>xgboost</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>eXtreme Gradient Boosting</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;xgbTree&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nrounds</code> (# Boosting Iterations)</li>
<li><code>max_depth</code> (Max Tree Depth)</li>
<li><code>eta</code> (Shrinkage)</li>
<li><code>gamma</code> (Minimum Loss Reduction)</li>
<li><code>colsample_bytree</code> (Subsample Ratio of Columns)</li>
<li><code>min_child_weight</code> (Minimum Sum of Instance Weight)</li>
<li><code>subsample</code> (Subsample Percentage)</li>
</ul>
<p>Required packages: <code>xgboost</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Gradient Boosting Machines</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gbm_h2o&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ntrees</code> (# Boosting Iterations)</li>
<li><code>max_depth</code> (Max Tree Depth)</li>
<li><code>min_rows</code> (Min. Terminal Node Size)</li>
<li><code>learn_rate</code> (Shrinkage)</li>
<li><code>col_sample_rate</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>h2o</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Model Averaged Neural Network</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;avNNet&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>decay</code> (Weight Decay)</li>
<li><code>bag</code> (Bagging)</li>
</ul>
<p>Required packages: <code>nnet</code></p>
<p><strong>Oblique Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFlog&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>obliqueRF</code> package is fully loaded when this model is used.</p>
<p><strong>Oblique Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFpls&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>obliqueRF</code> package is fully loaded when this model is used.</p>
<p><strong>Oblique Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFridge&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>obliqueRF</code> package is fully loaded when this model is used.</p>
<p><strong>Oblique Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFsvm&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>obliqueRF</code> package is fully loaded when this model is used.</p>
<p><strong>Parallel Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;parRF&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>e1071</code>, <code>randomForest</code>, <code>foreach</code>, <code>import</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Quantile Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;qrf&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>quantregForest</code></p>
<p><strong>Quantile Regression Neural Network</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;qrnn&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>n.hidden</code> (#Hidden Units)</li>
<li><code>penalty</code> ( Weight Decay)</li>
<li><code>bag</code> (Bagged Models?)</li>
</ul>
<p>Required packages: <code>qrnn</code></p>
<p><strong>Random Ferns</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rFerns&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>depth</code> (Fern Depth)</li>
</ul>
<p>Required packages: <code>rFerns</code></p>
<p><strong>Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ordinalRF&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nsets</code> (# score sets tried prior to the approximation)</li>
<li><code>ntreeperdiv</code> (# of trees (small RFs))</li>
<li><code>ntreefinal</code> (# of trees (final RF))</li>
</ul>
<p>Required packages: <code>e1071</code>, <code>ranger</code>, <code>dplyr</code>, <code>ordinalForest</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ranger&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
<li><code>splitrule</code> (Splitting Rule)</li>
<li><code>min.node.size</code> (Minimal Node Size)</li>
</ul>
<p>Required packages: <code>e1071</code>, <code>ranger</code>, <code>dplyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;Rborist&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>predFixed</code> (#Randomly Selected Predictors)</li>
<li><code>minNode</code> (Minimal Node Size)</li>
</ul>
<p>Required packages: <code>Rborist</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rf&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>randomForest</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Random Forest by Randomization</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;extraTrees&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (# Randomly Selected Predictors)</li>
<li><code>numRandomCuts</code> (# Random Cuts)</li>
</ul>
<p>Required packages: <code>extraTrees</code></p>
<p><strong>Random Forest Rule-Based Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rfRules&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
<li><code>maxdepth</code> (Maximum Rule Depth)</li>
</ul>
<p>Required packages: <code>randomForest</code>, <code>inTrees</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Regularized Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;RRF&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
<li><code>coefReg</code> (Regularization Value)</li>
<li><code>coefImp</code> (Importance Coefficient)</li>
</ul>
<p>Required packages: <code>randomForest</code>, <code>RRF</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Regularized Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;RRFglobal&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
<li><code>coefReg</code> (Regularization Value)</li>
</ul>
<p>Required packages: <code>RRF</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Rotation Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rotationForest&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K</code> (#Variable Subsets)</li>
<li><code>L</code> (Ensemble Size)</li>
</ul>
<p>Required packages: <code>rotationForest</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Rotation Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rotationForestCp&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K</code> (#Variable Subsets)</li>
<li><code>L</code> (Ensemble Size)</li>
<li><code>cp</code> (Complexity Parameter)</li>
</ul>
<p>Required packages: <code>rpart</code>, <code>plyr</code>, <code>rotationForest</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Stochastic Gradient Boosting</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gbm&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>n.trees</code> (# Boosting Iterations)</li>
<li><code>interaction.depth</code> (Max Tree Depth)</li>
<li><code>shrinkage</code> (Shrinkage)</li>
<li><code>n.minobsinnode</code> (Min. Terminal Node Size)</li>
</ul>
<p>Required packages: <code>gbm</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Tree-Based Ensembles</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;nodeHarvest&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxinter</code> (Maximum Interaction Depth)</li>
<li><code>mode</code> (Prediction Mode)</li>
</ul>
<p>Required packages: <code>nodeHarvest</code></p>
<p><strong>Weighted Subspace Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;wsrf&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>wsrf</code></p>
<div id="Feature_Extraction">

</div>
</div>
<div id="feature-extraction" class="section level3">
<h3><span class="header-section-number">7.0.11</span> Feature Extraction</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Independent Component Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;icr&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>n.comp</code> (#Components)</li>
</ul>
<p>Required packages: <code>fastICA</code></p>
<p><strong>Neural Networks with Feature Extraction</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pcaNNet&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>decay</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>nnet</code></p>
<p><strong>Partial Least Squares</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;kernelpls&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ncomp</code> (#Components)</li>
</ul>
<p>Required packages: <code>pls</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Partial Least Squares</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pls&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ncomp</code> (#Components)</li>
</ul>
<p>Required packages: <code>pls</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Partial Least Squares</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;simpls&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ncomp</code> (#Components)</li>
</ul>
<p>Required packages: <code>pls</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Partial Least Squares</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;widekernelpls&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ncomp</code> (#Components)</li>
</ul>
<p>Required packages: <code>pls</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Principal Component Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pcr&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ncomp</code> (#Components)</li>
</ul>
<p>Required packages: <code>pls</code></p>
<p><strong>Projection Pursuit Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ppr&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nterms</code> (# Terms)</li>
</ul>
<p><strong>Sparse Partial Least Squares</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;spls&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K</code> (#Components)</li>
<li><code>eta</code> (Threshold)</li>
<li><code>kappa</code> (Kappa)</li>
</ul>
<p>Required packages: <code>spls</code></p>
<p><strong>Supervised Principal Component Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;superpc&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>threshold</code> (Threshold)</li>
<li><code>n.components</code> (#Components)</li>
</ul>
<p>Required packages: <code>superpc</code></p>
<div id="Feature_Selection_Wrapper">

</div>
</div>
<div id="feature-selection-wrapper" class="section level3">
<h3><span class="header-section-number">7.0.12</span> Feature Selection Wrapper</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Generalized Linear Model with Stepwise Feature Selection</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmStepAIC&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>MASS</code></p>
<p><strong>Linear Discriminant Analysis with Stepwise Feature Selection</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;stepLDA&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxvar</code> (Maximum #Variables)</li>
<li><code>direction</code> (Search Direction)</li>
</ul>
<p>Required packages: <code>klaR</code>, <code>MASS</code></p>
<p><strong>Linear Regression with Backwards Selection</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;leapBackward&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nvmax</code> (Maximum Number of Predictors)</li>
</ul>
<p>Required packages: <code>leaps</code></p>
<p><strong>Linear Regression with Forward Selection</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;leapForward&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nvmax</code> (Maximum Number of Predictors)</li>
</ul>
<p>Required packages: <code>leaps</code></p>
<p><strong>Linear Regression with Stepwise Selection</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;leapSeq&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nvmax</code> (Maximum Number of Predictors)</li>
</ul>
<p>Required packages: <code>leaps</code></p>
<p><strong>Linear Regression with Stepwise Selection</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lmStepAIC&#39;</span></code></pre>
<p>Type: Regression</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>MASS</code></p>
<p><strong>Quadratic Discriminant Analysis with Stepwise Feature Selection</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;stepQDA&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxvar</code> (Maximum #Variables)</li>
<li><code>direction</code> (Search Direction)</li>
</ul>
<p>Required packages: <code>klaR</code>, <code>MASS</code></p>
<p><strong>Ridge Regression with Variable Selection</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;foba&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>k</code> (#Variables Retained)</li>
<li><code>lambda</code> (L2 Penalty)</li>
</ul>
<p>Required packages: <code>foba</code></p>
<div id="Gaussian_Process">

</div>
</div>
<div id="gaussian-process" class="section level3">
<h3><span class="header-section-number">7.0.13</span> Gaussian Process</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Gaussian Process</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gaussprLinear&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Gaussian Process with Polynomial Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gaussprPoly&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Polynomial Degree)</li>
<li><code>scale</code> (Scale)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Gaussian Process with Radial Basis Function Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gaussprRadial&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Variational Bayesian Multinomial Probit Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;vbmpRadial&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>estimateTheta</code> (Theta Estimated)</li>
</ul>
<p>Required packages: <code>vbmp</code></p>
<div id="Generalized_Additive_Model">

</div>
</div>
<div id="generalized-additive-model" class="section level3">
<h3><span class="header-section-number">7.0.14</span> Generalized Additive Model</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Boosted Generalized Additive Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gamboost&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>prune</code> (AIC Prune?)</li>
</ul>
<p>Required packages: <code>mboost</code>, <code>plyr</code>, <code>import</code></p>
<p>Notes: The <code>prune</code> option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in <code>?mboost::mstop</code>. If pruning is not used, the ensemble makes predictions using the exact value of the <code>mstop</code> tuning parameter value.</p>
<p><strong>Generalized Additive Model using LOESS</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gamLoess&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>span</code> (Span)</li>
<li><code>degree</code> (Degree)</li>
</ul>
<p>Required packages: <code>gam</code></p>
<p>A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by <code>train</code>, the <code>gam</code> package is fully loaded when this model is used.</p>
<p><strong>Generalized Additive Model using Splines</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bam&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>select</code> (Feature Selection)</li>
<li><code>method</code> (Method)</li>
</ul>
<p>Required packages: <code>mgcv</code></p>
<p>A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by <code>train</code>, the <code>mgcv</code> package is fully loaded when this model is used.</p>
<p><strong>Generalized Additive Model using Splines</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gam&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>select</code> (Feature Selection)</li>
<li><code>method</code> (Method)</li>
</ul>
<p>Required packages: <code>mgcv</code></p>
<p>A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by <code>train</code>, the <code>mgcv</code> package is fully loaded when this model is used.</p>
<p><strong>Generalized Additive Model using Splines</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gamSpline&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>df</code> (Degrees of Freedom)</li>
</ul>
<p>Required packages: <code>gam</code></p>
<p>A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by <code>train</code>, the <code>gam</code> package is fully loaded when this model is used.</p>
<div id="Generalized_Linear_Model">

</div>
</div>
<div id="generalized-linear-model" class="section level3">
<h3><span class="header-section-number">7.0.15</span> Generalized Linear Model</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Bayesian Generalized Linear Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bayesglm&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>arm</code></p>
<p><strong>Boosted Generalized Linear Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmboost&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>prune</code> (AIC Prune?)</li>
</ul>
<p>Required packages: <code>plyr</code>, <code>mboost</code></p>
<p>A model-specific variable importance metric is available. Notes: The <code>prune</code> option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in <code>?mboost::mstop</code>. If pruning is not used, the ensemble makes predictions using the exact value of the <code>mstop</code> tuning parameter value.</p>
<p><strong>Ensembles of Generalized Linear Models</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;randomGLM&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxInteractionOrder</code> (Interaction Order)</li>
</ul>
<p>Required packages: <code>randomGLM</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>randomGLM</code> package is fully loaded when this model is used.</p>
<p><strong>Generalized Additive Model using LOESS</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gamLoess&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>span</code> (Span)</li>
<li><code>degree</code> (Degree)</li>
</ul>
<p>Required packages: <code>gam</code></p>
<p>A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by <code>train</code>, the <code>gam</code> package is fully loaded when this model is used.</p>
<p><strong>Generalized Additive Model using Splines</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bam&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>select</code> (Feature Selection)</li>
<li><code>method</code> (Method)</li>
</ul>
<p>Required packages: <code>mgcv</code></p>
<p>A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by <code>train</code>, the <code>mgcv</code> package is fully loaded when this model is used.</p>
<p><strong>Generalized Additive Model using Splines</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gam&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>select</code> (Feature Selection)</li>
<li><code>method</code> (Method)</li>
</ul>
<p>Required packages: <code>mgcv</code></p>
<p>A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by <code>train</code>, the <code>mgcv</code> package is fully loaded when this model is used.</p>
<p><strong>Generalized Additive Model using Splines</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gamSpline&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>df</code> (Degrees of Freedom)</li>
</ul>
<p>Required packages: <code>gam</code></p>
<p>A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by <code>train</code>, the <code>gam</code> package is fully loaded when this model is used.</p>
<p><strong>Generalized Linear Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glm&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Generalized Linear Model with Stepwise Feature Selection</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmStepAIC&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>MASS</code></p>
<p><strong>glmnet</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmnet_h2o&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Mixing Percentage)</li>
<li><code>lambda</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>h2o</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>glmnet</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmnet&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Mixing Percentage)</li>
<li><code>lambda</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>glmnet</code>, <code>Matrix</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Multi-Step Adaptive MCP-Net</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;msaenet&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alphas</code> (Alpha)</li>
<li><code>nsteps</code> (#Adaptive Estimation Steps)</li>
<li><code>scale</code> (Adaptive Weight Scaling Factor)</li>
</ul>
<p>Required packages: <code>msaenet</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Negative Binomial Generalized Linear Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glm.nb&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>MASS</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Penalized Ordinal Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ordinalNet&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Mixing Percentage)</li>
<li><code>criteria</code> (Selection Criterion)</li>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>ordinalNet</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available. Notes: Requires ordinalNet package version &gt;= 2.0</p>
<div id="Handle_Missing_Predictor_Data">

</div>
</div>
<div id="handle-missing-predictor-data" class="section level3">
<h3><span class="header-section-number">7.0.16</span> Handle Missing Predictor Data</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>AdaBoost.M1</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;AdaBoost.M1&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mfinal</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
<li><code>coeflearn</code> (Coefficient Type)</li>
</ul>
<p>Required packages: <code>adabag</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Bagged AdaBoost</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;AdaBag&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mfinal</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
</ul>
<p>Required packages: <code>adabag</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Boosted Classification Trees</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ada&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>iter</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
<li><code>nu</code> (Learning Rate)</li>
</ul>
<p>Required packages: <code>ada</code>, <code>plyr</code></p>
<p><strong>C5.0</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>trials</code> (# Boosting Iterations)</li>
<li><code>model</code> (Model Type)</li>
<li><code>winnow</code> (Winnow)</li>
</ul>
<p>Required packages: <code>C50</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>CART</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpart&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cp</code> (Complexity Parameter)</li>
</ul>
<p>Required packages: <code>rpart</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>CART</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpart1SE&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>rpart</code></p>
<p>A model-specific variable importance metric is available. Notes: This CART model replicates the same process used by the <code>rpart</code> function where the model complexity is determined using the one-standard error method. This procedure is replicated inside of the resampling done by <code>train</code> so that an external resampling estimate can be obtained.</p>
<p><strong>CART</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpart2&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxdepth</code> (Max Tree Depth)</li>
</ul>
<p>Required packages: <code>rpart</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>CART or Ordinal Responses</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpartScore&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cp</code> (Complexity Parameter)</li>
<li><code>split</code> (Split Function)</li>
<li><code>prune</code> (Pruning Measure)</li>
</ul>
<p>Required packages: <code>rpartScore</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Cost-Sensitive C5.0</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0Cost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>trials</code> (# Boosting Iterations)</li>
<li><code>model</code> (Model Type)</li>
<li><code>winnow</code> (Winnow)</li>
<li><code>cost</code> (Cost)</li>
</ul>
<p>Required packages: <code>C50</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Cost-Sensitive CART</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpartCost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cp</code> (Complexity Parameter)</li>
<li><code>Cost</code> (Cost)</li>
</ul>
<p>Required packages: <code>rpart</code>, <code>plyr</code></p>
<p><strong>Single C5.0 Ruleset</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0Rules&#39;</span></code></pre>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>C50</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Single C5.0 Tree</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0Tree&#39;</span></code></pre>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>C50</code></p>
<p>A model-specific variable importance metric is available.</p>
<div id="Implicit_Feature_Selection">

</div>
</div>
<div id="implicit-feature-selection" class="section level3">
<h3><span class="header-section-number">7.0.17</span> Implicit Feature Selection</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>AdaBoost Classification Trees</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;adaboost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nIter</code> (#Trees)</li>
<li><code>method</code> (Method)</li>
</ul>
<p>Required packages: <code>fastAdaboost</code></p>
<p><strong>AdaBoost.M1</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;AdaBoost.M1&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mfinal</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
<li><code>coeflearn</code> (Coefficient Type)</li>
</ul>
<p>Required packages: <code>adabag</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Bagged AdaBoost</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;AdaBag&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mfinal</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
</ul>
<p>Required packages: <code>adabag</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Bagged Flexible Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bagFDA&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Product Degree)</li>
<li><code>nprune</code> (#Terms)</li>
</ul>
<p>Required packages: <code>earth</code>, <code>mda</code></p>
<p>A model-specific variable importance metric is available. Notes: Unlike other packages used by <code>train</code>, the <code>earth</code> package is fully loaded when this model is used.</p>
<p><strong>Bagged MARS</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bagEarth&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nprune</code> (#Terms)</li>
<li><code>degree</code> (Product Degree)</li>
</ul>
<p>Required packages: <code>earth</code></p>
<p>A model-specific variable importance metric is available. Notes: Unlike other packages used by <code>train</code>, the <code>earth</code> package is fully loaded when this model is used.</p>
<p><strong>Bagged MARS using gCV Pruning</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bagEarthGCV&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Product Degree)</li>
</ul>
<p>Required packages: <code>earth</code></p>
<p>A model-specific variable importance metric is available. Notes: Unlike other packages used by <code>train</code>, the <code>earth</code> package is fully loaded when this model is used.</p>
<p><strong>Bayesian Additive Regression Trees</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bartMachine&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>num_trees</code> (#Trees)</li>
<li><code>k</code> (Prior Boundary)</li>
<li><code>alpha</code> (Base Terminal Node Hyperparameter)</li>
<li><code>beta</code> (Power Terminal Node Hyperparameter)</li>
<li><code>nu</code> (Degrees of Freedom)</li>
</ul>
<p>Required packages: <code>bartMachine</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Boosted Classification Trees</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ada&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>iter</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
<li><code>nu</code> (Learning Rate)</li>
</ul>
<p>Required packages: <code>ada</code>, <code>plyr</code></p>
<p><strong>Boosted Generalized Additive Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gamboost&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>prune</code> (AIC Prune?)</li>
</ul>
<p>Required packages: <code>mboost</code>, <code>plyr</code>, <code>import</code></p>
<p>Notes: The <code>prune</code> option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in <code>?mboost::mstop</code>. If pruning is not used, the ensemble makes predictions using the exact value of the <code>mstop</code> tuning parameter value.</p>
<p><strong>Boosted Linear Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;BstLm&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>nu</code> (Shrinkage)</li>
</ul>
<p>Required packages: <code>bst</code>, <code>plyr</code></p>
<p><strong>Boosted Logistic Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;LogitBoost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nIter</code> (# Boosting Iterations)</li>
</ul>
<p>Required packages: <code>caTools</code></p>
<p><strong>Boosted Smoothing Spline</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bstSm&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>nu</code> (Shrinkage)</li>
</ul>
<p>Required packages: <code>bst</code>, <code>plyr</code></p>
<p><strong>C4.5-like Trees</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;J48&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>C</code> (Confidence Threshold)</li>
<li><code>M</code> (Minimum Instances Per Leaf)</li>
</ul>
<p>Required packages: <code>RWeka</code></p>
<p><strong>C5.0</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>trials</code> (# Boosting Iterations)</li>
<li><code>model</code> (Model Type)</li>
<li><code>winnow</code> (Winnow)</li>
</ul>
<p>Required packages: <code>C50</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>CART</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpart&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cp</code> (Complexity Parameter)</li>
</ul>
<p>Required packages: <code>rpart</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>CART</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpart1SE&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>rpart</code></p>
<p>A model-specific variable importance metric is available. Notes: This CART model replicates the same process used by the <code>rpart</code> function where the model complexity is determined using the one-standard error method. This procedure is replicated inside of the resampling done by <code>train</code> so that an external resampling estimate can be obtained.</p>
<p><strong>CART</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpart2&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxdepth</code> (Max Tree Depth)</li>
</ul>
<p>Required packages: <code>rpart</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>CART or Ordinal Responses</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpartScore&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cp</code> (Complexity Parameter)</li>
<li><code>split</code> (Split Function)</li>
<li><code>prune</code> (Pruning Measure)</li>
</ul>
<p>Required packages: <code>rpartScore</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>CHi-squared Automated Interaction Detection</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;chaid&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha2</code> (Merging Threshold)</li>
<li><code>alpha3</code> (Splitting former Merged Threshold)</li>
<li><code>alpha4</code> (
Splitting former Merged Threshold)</li>
</ul>
<p>Required packages: <code>CHAID</code></p>
<p><strong>Conditional Inference Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;cforest&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>party</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Conditional Inference Tree</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ctree&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mincriterion</code> (1 - P-Value Threshold)</li>
</ul>
<p>Required packages: <code>party</code></p>
<p><strong>Conditional Inference Tree</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ctree2&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxdepth</code> (Max Tree Depth)</li>
<li><code>mincriterion</code> (1 - P-Value Threshold)</li>
</ul>
<p>Required packages: <code>party</code></p>
<p><strong>Cost-Sensitive C5.0</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0Cost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>trials</code> (# Boosting Iterations)</li>
<li><code>model</code> (Model Type)</li>
<li><code>winnow</code> (Winnow)</li>
<li><code>cost</code> (Cost)</li>
</ul>
<p>Required packages: <code>C50</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Cost-Sensitive CART</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpartCost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cp</code> (Complexity Parameter)</li>
<li><code>Cost</code> (Cost)</li>
</ul>
<p>Required packages: <code>rpart</code>, <code>plyr</code></p>
<p><strong>Cubist</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;cubist&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>committees</code> (#Committees)</li>
<li><code>neighbors</code> (#Instances)</li>
</ul>
<p>Required packages: <code>Cubist</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>DeepBoost</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;deepboost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>num_iter</code> (# Boosting Iterations)</li>
<li><code>tree_depth</code> (Tree Depth)</li>
<li><code>beta</code> (L1 Regularization)</li>
<li><code>lambda</code> (Tree Depth Regularization)</li>
<li><code>loss_type</code> (Loss)</li>
</ul>
<p>Required packages: <code>deepboost</code></p>
<p><strong>Elasticnet</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;enet&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>fraction</code> (Fraction of Full Solution)</li>
<li><code>lambda</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>elasticnet</code></p>
<p><strong>eXtreme Gradient Boosting</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;xgbDART&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nrounds</code> (# Boosting Iterations)</li>
<li><code>max_depth</code> (Max Tree Depth)</li>
<li><code>eta</code> (Shrinkage)</li>
<li><code>gamma</code> (Minimum Loss Reduction)</li>
<li><code>subsample</code> (Subsample Percentage)</li>
<li><code>colsample_bytree</code> (Subsample Ratio of Columns)</li>
<li><code>rate_drop</code> (Fraction of Trees Dropped)</li>
<li><code>skip_drop</code> (Prob. of Skipping Drop-out)</li>
<li><code>min_child_weight</code> (Minimum Sum of Instance Weight)</li>
</ul>
<p>Required packages: <code>xgboost</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>eXtreme Gradient Boosting</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;xgbLinear&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nrounds</code> (# Boosting Iterations)</li>
<li><code>lambda</code> (L2 Regularization)</li>
<li><code>alpha</code> (L1 Regularization)</li>
<li><code>eta</code> (Learning Rate)</li>
</ul>
<p>Required packages: <code>xgboost</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>eXtreme Gradient Boosting</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;xgbTree&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nrounds</code> (# Boosting Iterations)</li>
<li><code>max_depth</code> (Max Tree Depth)</li>
<li><code>eta</code> (Shrinkage)</li>
<li><code>gamma</code> (Minimum Loss Reduction)</li>
<li><code>colsample_bytree</code> (Subsample Ratio of Columns)</li>
<li><code>min_child_weight</code> (Minimum Sum of Instance Weight)</li>
<li><code>subsample</code> (Subsample Percentage)</li>
</ul>
<p>Required packages: <code>xgboost</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Flexible Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;fda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Product Degree)</li>
<li><code>nprune</code> (#Terms)</li>
</ul>
<p>Required packages: <code>earth</code>, <code>mda</code></p>
<p>A model-specific variable importance metric is available. Notes: Unlike other packages used by <code>train</code>, the <code>earth</code> package is fully loaded when this model is used.</p>
<p><strong>Generalized Linear Model with Stepwise Feature Selection</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmStepAIC&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>MASS</code></p>
<p><strong>glmnet</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmnet_h2o&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Mixing Percentage)</li>
<li><code>lambda</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>h2o</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>glmnet</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmnet&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Mixing Percentage)</li>
<li><code>lambda</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>glmnet</code>, <code>Matrix</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Gradient Boosting Machines</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gbm_h2o&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ntrees</code> (# Boosting Iterations)</li>
<li><code>max_depth</code> (Max Tree Depth)</li>
<li><code>min_rows</code> (Min. Terminal Node Size)</li>
<li><code>learn_rate</code> (Shrinkage)</li>
<li><code>col_sample_rate</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>h2o</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Least Angle Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lars&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>fraction</code> (Fraction)</li>
</ul>
<p>Required packages: <code>lars</code></p>
<p><strong>Least Angle Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lars2&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>step</code> (#Steps)</li>
</ul>
<p>Required packages: <code>lars</code></p>
<p><strong>Logistic Model Trees</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;LMT&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>iter</code> (# Iteratons)</li>
</ul>
<p>Required packages: <code>RWeka</code></p>
<p><strong>Model Rules</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;M5Rules&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>pruned</code> (Pruned)</li>
<li><code>smoothed</code> (Smoothed)</li>
</ul>
<p>Required packages: <code>RWeka</code></p>
<p><strong>Model Tree</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;M5&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>pruned</code> (Pruned)</li>
<li><code>smoothed</code> (Smoothed)</li>
<li><code>rules</code> (Rules)</li>
</ul>
<p>Required packages: <code>RWeka</code></p>
<p><strong>Multi-Step Adaptive MCP-Net</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;msaenet&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alphas</code> (Alpha)</li>
<li><code>nsteps</code> (#Adaptive Estimation Steps)</li>
<li><code>scale</code> (Adaptive Weight Scaling Factor)</li>
</ul>
<p>Required packages: <code>msaenet</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Multivariate Adaptive Regression Spline</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;earth&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nprune</code> (#Terms)</li>
<li><code>degree</code> (Product Degree)</li>
</ul>
<p>Required packages: <code>earth</code></p>
<p>A model-specific variable importance metric is available. Notes: Unlike other packages used by <code>train</code>, the <code>earth</code> package is fully loaded when this model is used.</p>
<p><strong>Multivariate Adaptive Regression Splines</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gcvEarth&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Product Degree)</li>
</ul>
<p>Required packages: <code>earth</code></p>
<p>A model-specific variable importance metric is available. Notes: Unlike other packages used by <code>train</code>, the <code>earth</code> package is fully loaded when this model is used.</p>
<p><strong>Nearest Shrunken Centroids</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pam&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>threshold</code> (Shrinkage Threshold)</li>
</ul>
<p>Required packages: <code>pamr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Non-Convex Penalized Quantile Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rqnc&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (L1 Penalty)</li>
<li><code>penalty</code> (Penalty Type)</li>
</ul>
<p>Required packages: <code>rqPen</code></p>
<p><strong>Oblique Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFlog&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>obliqueRF</code> package is fully loaded when this model is used.</p>
<p><strong>Oblique Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFpls&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>obliqueRF</code> package is fully loaded when this model is used.</p>
<p><strong>Oblique Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFridge&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>obliqueRF</code> package is fully loaded when this model is used.</p>
<p><strong>Oblique Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFsvm&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>obliqueRF</code> package is fully loaded when this model is used.</p>
<p><strong>Parallel Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;parRF&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>e1071</code>, <code>randomForest</code>, <code>foreach</code>, <code>import</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Penalized Linear Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;PenalizedLDA&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (L1 Penalty)</li>
<li><code>K</code> (#Discriminant Functions)</li>
</ul>
<p>Required packages: <code>penalizedLDA</code>, <code>plyr</code></p>
<p><strong>Penalized Linear Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;penalized&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda1</code> (L1 Penalty)</li>
<li><code>lambda2</code> (L2 Penalty)</li>
</ul>
<p>Required packages: <code>penalized</code></p>
<p><strong>Penalized Ordinal Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ordinalNet&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Mixing Percentage)</li>
<li><code>criteria</code> (Selection Criterion)</li>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>ordinalNet</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available. Notes: Requires ordinalNet package version &gt;= 2.0</p>
<p><strong>Quantile Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;qrf&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>quantregForest</code></p>
<p><strong>Quantile Regression with LASSO penalty</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rqlasso&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (L1 Penalty)</li>
</ul>
<p>Required packages: <code>rqPen</code></p>
<p><strong>Random Ferns</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rFerns&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>depth</code> (Fern Depth)</li>
</ul>
<p>Required packages: <code>rFerns</code></p>
<p><strong>Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ordinalRF&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nsets</code> (# score sets tried prior to the approximation)</li>
<li><code>ntreeperdiv</code> (# of trees (small RFs))</li>
<li><code>ntreefinal</code> (# of trees (final RF))</li>
</ul>
<p>Required packages: <code>e1071</code>, <code>ranger</code>, <code>dplyr</code>, <code>ordinalForest</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ranger&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
<li><code>splitrule</code> (Splitting Rule)</li>
<li><code>min.node.size</code> (Minimal Node Size)</li>
</ul>
<p>Required packages: <code>e1071</code>, <code>ranger</code>, <code>dplyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;Rborist&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>predFixed</code> (#Randomly Selected Predictors)</li>
<li><code>minNode</code> (Minimal Node Size)</li>
</ul>
<p>Required packages: <code>Rborist</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rf&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>randomForest</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Random Forest by Randomization</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;extraTrees&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (# Randomly Selected Predictors)</li>
<li><code>numRandomCuts</code> (# Random Cuts)</li>
</ul>
<p>Required packages: <code>extraTrees</code></p>
<p><strong>Random Forest Rule-Based Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rfRules&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
<li><code>maxdepth</code> (Maximum Rule Depth)</li>
</ul>
<p>Required packages: <code>randomForest</code>, <code>inTrees</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Regularized Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;RRF&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
<li><code>coefReg</code> (Regularization Value)</li>
<li><code>coefImp</code> (Importance Coefficient)</li>
</ul>
<p>Required packages: <code>randomForest</code>, <code>RRF</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Regularized Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;RRFglobal&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
<li><code>coefReg</code> (Regularization Value)</li>
</ul>
<p>Required packages: <code>RRF</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Relaxed Lasso</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;relaxo&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Penalty Parameter)</li>
<li><code>phi</code> (Relaxation Parameter)</li>
</ul>
<p>Required packages: <code>relaxo</code>, <code>plyr</code></p>
<p><strong>Rotation Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rotationForest&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K</code> (#Variable Subsets)</li>
<li><code>L</code> (Ensemble Size)</li>
</ul>
<p>Required packages: <code>rotationForest</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Rotation Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rotationForestCp&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K</code> (#Variable Subsets)</li>
<li><code>L</code> (Ensemble Size)</li>
<li><code>cp</code> (Complexity Parameter)</li>
</ul>
<p>Required packages: <code>rpart</code>, <code>plyr</code>, <code>rotationForest</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Rule-Based Classifier</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;JRip&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>NumOpt</code> (# Optimizations)</li>
<li><code>NumFolds</code> (# Folds)</li>
<li><code>MinWeights</code> (Min Weights)</li>
</ul>
<p>Required packages: <code>RWeka</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Rule-Based Classifier</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;PART&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>threshold</code> (Confidence Threshold)</li>
<li><code>pruned</code> (Pruning)</li>
</ul>
<p>Required packages: <code>RWeka</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Single C5.0 Ruleset</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0Rules&#39;</span></code></pre>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>C50</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Single C5.0 Tree</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0Tree&#39;</span></code></pre>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>C50</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Single Rule Classification</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;OneR&#39;</span></code></pre>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>RWeka</code></p>
<p><strong>Sparse Distance Weighted Discrimination</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;sdwd&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (L1 Penalty)</li>
<li><code>lambda2</code> (L2 Penalty)</li>
</ul>
<p>Required packages: <code>sdwd</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Sparse Linear Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;sparseLDA&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>NumVars</code> (# Predictors)</li>
<li><code>lambda</code> (Lambda)</li>
</ul>
<p>Required packages: <code>sparseLDA</code></p>
<p><strong>Sparse Mixture Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;smda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>NumVars</code> (# Predictors)</li>
<li><code>lambda</code> (Lambda)</li>
<li><code>R</code> (# Subclasses)</li>
</ul>
<p>Required packages: <code>sparseLDA</code></p>
<p><strong>Spike and Slab Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;spikeslab&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>vars</code> (Variables Retained)</li>
</ul>
<p>Required packages: <code>spikeslab</code>, <code>plyr</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>spikeslab</code> package is fully loaded when this model is used.</p>
<p><strong>Stochastic Gradient Boosting</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gbm&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>n.trees</code> (# Boosting Iterations)</li>
<li><code>interaction.depth</code> (Max Tree Depth)</li>
<li><code>shrinkage</code> (Shrinkage)</li>
<li><code>n.minobsinnode</code> (Min. Terminal Node Size)</li>
</ul>
<p>Required packages: <code>gbm</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>The Bayesian lasso</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;blasso&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sparsity</code> (Sparsity Threshold)</li>
</ul>
<p>Required packages: <code>monomvn</code></p>
<p>Notes: This model creates predictions using the mean of the posterior distributions but sets some parameters specifically to zero based on the tuning parameter <code>sparsity</code>. For example, when <code>sparsity = .5</code>, only coefficients where at least half the posterior estimates are nonzero are used.</p>
<p><strong>The lasso</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lasso&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>fraction</code> (Fraction of Full Solution)</li>
</ul>
<p>Required packages: <code>elasticnet</code></p>
<p><strong>Tree Models from Genetic Algorithms</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;evtree&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Complexity Parameter)</li>
</ul>
<p>Required packages: <code>evtree</code></p>
<p><strong>Tree-Based Ensembles</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;nodeHarvest&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxinter</code> (Maximum Interaction Depth)</li>
<li><code>mode</code> (Prediction Mode)</li>
</ul>
<p>Required packages: <code>nodeHarvest</code></p>
<p><strong>Weighted Subspace Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;wsrf&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>wsrf</code></p>
<div id="Kernel_Method">

</div>
</div>
<div id="kernel-method" class="section level3">
<h3><span class="header-section-number">7.0.18</span> Kernel Method</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Distance Weighted Discrimination with Polynomial Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dwdPoly&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>qval</code> (q)</li>
<li><code>degree</code> (Polynomial Degree)</li>
<li><code>scale</code> (Scale)</li>
</ul>
<p>Required packages: <code>kerndwd</code></p>
<p><strong>Distance Weighted Discrimination with Radial Basis Function Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dwdRadial&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>qval</code> (q)</li>
<li><code>sigma</code> (Sigma)</li>
</ul>
<p>Required packages: <code>kernlab</code>, <code>kerndwd</code></p>
<p><strong>Gaussian Process</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gaussprLinear&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Gaussian Process with Polynomial Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gaussprPoly&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Polynomial Degree)</li>
<li><code>scale</code> (Scale)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Gaussian Process with Radial Basis Function Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gaussprRadial&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>L2 Regularized Linear Support Vector Machines with Class Weights</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinearWeights2&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
<li><code>Loss</code> (Loss Function)</li>
<li><code>weight</code> (Class Weight)</li>
</ul>
<p>Required packages: <code>LiblineaR</code></p>
<p><strong>L2 Regularized Support Vector Machine (dual) with Linear Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinear3&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
<li><code>Loss</code> (Loss Function)</li>
</ul>
<p>Required packages: <code>LiblineaR</code></p>
<p><strong>Least Squares Support Vector Machine</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lssvmLinear&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>tau</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Least Squares Support Vector Machine with Polynomial Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lssvmPoly&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Polynomial Degree)</li>
<li><code>scale</code> (Scale)</li>
<li><code>tau</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Least Squares Support Vector Machine with Radial Basis Function Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lssvmRadial&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
<li><code>tau</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Linear Distance Weighted Discrimination</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dwdLinear&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>qval</code> (q)</li>
</ul>
<p>Required packages: <code>kerndwd</code></p>
<p><strong>Linear Support Vector Machines with Class Weights</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinearWeights&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
<li><code>weight</code> (Class Weight)</li>
</ul>
<p>Required packages: <code>e1071</code></p>
<p><strong>Oblique Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFsvm&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>obliqueRF</code> package is fully loaded when this model is used.</p>
<p><strong>Partial Least Squares</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;kernelpls&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ncomp</code> (#Components)</li>
</ul>
<p>Required packages: <code>pls</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Polynomial Kernel Regularized Least Squares</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;krlsPoly&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>degree</code> (Polynomial Degree)</li>
</ul>
<p>Required packages: <code>KRLS</code></p>
<p><strong>Radial Basis Function Kernel Regularized Least Squares</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;krlsRadial&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>sigma</code> (Sigma)</li>
</ul>
<p>Required packages: <code>KRLS</code>, <code>kernlab</code></p>
<p><strong>Relevance Vector Machines with Linear Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rvmLinear&#39;</span></code></pre>
<p>Type: Regression</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Relevance Vector Machines with Polynomial Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rvmPoly&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>scale</code> (Scale)</li>
<li><code>degree</code> (Polynomial Degree)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Relevance Vector Machines with Radial Basis Function Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rvmRadial&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Boundrange String Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmBoundrangeString&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>length</code> (length)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Class Weights</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmRadialWeights&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
<li><code>C</code> (Cost)</li>
<li><code>Weight</code> (Weight)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Exponential String Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmExpoString&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (lambda)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Linear Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinear&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Linear Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinear2&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
</ul>
<p>Required packages: <code>e1071</code></p>
<p><strong>Support Vector Machines with Polynomial Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmPoly&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Polynomial Degree)</li>
<li><code>scale</code> (Scale)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Radial Basis Function Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmRadial&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Radial Basis Function Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmRadialCost&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Radial Basis Function Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmRadialSigma&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p>Notes: This SVM model tunes over the cost parameter and the RBF kernel parameter sigma. In the latter case, using <code>tuneLength</code> will, at most, evaluate six values of the kernel parameter. This enables a broad search over the cost parameter and a relatively narrow search over <code>sigma</code></p>
<p><strong>Support Vector Machines with Spectrum String Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmSpectrumString&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>length</code> (length)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<div id="L1_Regularization">

</div>
</div>
<div id="l1-regularization" class="section level3">
<h3><span class="header-section-number">7.0.19</span> L1 Regularization</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Bayesian Ridge Regression (Model Averaged)</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;blassoAveraged&#39;</span></code></pre>
<p>Type: Regression</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>monomvn</code></p>
<p>Notes: This model makes predictions by averaging the predictions based on the posterior estimates of the regression coefficients. While it is possible that some of these posterior estimates are zero for non-informative predictors, the final predicted value may be a function of many (or even all) predictors.</p>
<p><strong>DeepBoost</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;deepboost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>num_iter</code> (# Boosting Iterations)</li>
<li><code>tree_depth</code> (Tree Depth)</li>
<li><code>beta</code> (L1 Regularization)</li>
<li><code>lambda</code> (Tree Depth Regularization)</li>
<li><code>loss_type</code> (Loss)</li>
</ul>
<p>Required packages: <code>deepboost</code></p>
<p><strong>Elasticnet</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;enet&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>fraction</code> (Fraction of Full Solution)</li>
<li><code>lambda</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>elasticnet</code></p>
<p><strong>glmnet</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmnet_h2o&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Mixing Percentage)</li>
<li><code>lambda</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>h2o</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>glmnet</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmnet&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Mixing Percentage)</li>
<li><code>lambda</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>glmnet</code>, <code>Matrix</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Least Angle Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lars&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>fraction</code> (Fraction)</li>
</ul>
<p>Required packages: <code>lars</code></p>
<p><strong>Least Angle Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lars2&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>step</code> (#Steps)</li>
</ul>
<p>Required packages: <code>lars</code></p>
<p><strong>Multi-Step Adaptive MCP-Net</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;msaenet&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alphas</code> (Alpha)</li>
<li><code>nsteps</code> (#Adaptive Estimation Steps)</li>
<li><code>scale</code> (Adaptive Weight Scaling Factor)</li>
</ul>
<p>Required packages: <code>msaenet</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Non-Convex Penalized Quantile Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rqnc&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (L1 Penalty)</li>
<li><code>penalty</code> (Penalty Type)</li>
</ul>
<p>Required packages: <code>rqPen</code></p>
<p><strong>Penalized Linear Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;PenalizedLDA&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (L1 Penalty)</li>
<li><code>K</code> (#Discriminant Functions)</li>
</ul>
<p>Required packages: <code>penalizedLDA</code>, <code>plyr</code></p>
<p><strong>Penalized Linear Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;penalized&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda1</code> (L1 Penalty)</li>
<li><code>lambda2</code> (L2 Penalty)</li>
</ul>
<p>Required packages: <code>penalized</code></p>
<p><strong>Penalized Ordinal Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ordinalNet&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Mixing Percentage)</li>
<li><code>criteria</code> (Selection Criterion)</li>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>ordinalNet</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available. Notes: Requires ordinalNet package version &gt;= 2.0</p>
<p><strong>Quantile Regression with LASSO penalty</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rqlasso&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (L1 Penalty)</li>
</ul>
<p>Required packages: <code>rqPen</code></p>
<p><strong>Regularized Logistic Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;regLogistic&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
<li><code>loss</code> (Loss Function)</li>
<li><code>epsilon</code> (Tolerance)</li>
</ul>
<p>Required packages: <code>LiblineaR</code></p>
<p><strong>Relaxed Lasso</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;relaxo&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Penalty Parameter)</li>
<li><code>phi</code> (Relaxation Parameter)</li>
</ul>
<p>Required packages: <code>relaxo</code>, <code>plyr</code></p>
<p><strong>Sparse Distance Weighted Discrimination</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;sdwd&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (L1 Penalty)</li>
<li><code>lambda2</code> (L2 Penalty)</li>
</ul>
<p>Required packages: <code>sdwd</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Sparse Linear Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;sparseLDA&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>NumVars</code> (# Predictors)</li>
<li><code>lambda</code> (Lambda)</li>
</ul>
<p>Required packages: <code>sparseLDA</code></p>
<p><strong>Sparse Mixture Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;smda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>NumVars</code> (# Predictors)</li>
<li><code>lambda</code> (Lambda)</li>
<li><code>R</code> (# Subclasses)</li>
</ul>
<p>Required packages: <code>sparseLDA</code></p>
<p><strong>Sparse Partial Least Squares</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;spls&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K</code> (#Components)</li>
<li><code>eta</code> (Threshold)</li>
<li><code>kappa</code> (Kappa)</li>
</ul>
<p>Required packages: <code>spls</code></p>
<p><strong>The Bayesian lasso</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;blasso&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sparsity</code> (Sparsity Threshold)</li>
</ul>
<p>Required packages: <code>monomvn</code></p>
<p>Notes: This model creates predictions using the mean of the posterior distributions but sets some parameters specifically to zero based on the tuning parameter <code>sparsity</code>. For example, when <code>sparsity = .5</code>, only coefficients where at least half the posterior estimates are nonzero are used.</p>
<p><strong>The lasso</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lasso&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>fraction</code> (Fraction of Full Solution)</li>
</ul>
<p>Required packages: <code>elasticnet</code></p>
<div id="L2_Regularization">

</div>
</div>
<div id="l2-regularization" class="section level3">
<h3><span class="header-section-number">7.0.20</span> L2 Regularization</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Bayesian Ridge Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bridge&#39;</span></code></pre>
<p>Type: Regression</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>monomvn</code></p>
<p><strong>Distance Weighted Discrimination with Polynomial Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dwdPoly&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>qval</code> (q)</li>
<li><code>degree</code> (Polynomial Degree)</li>
<li><code>scale</code> (Scale)</li>
</ul>
<p>Required packages: <code>kerndwd</code></p>
<p><strong>Distance Weighted Discrimination with Radial Basis Function Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dwdRadial&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>qval</code> (q)</li>
<li><code>sigma</code> (Sigma)</li>
</ul>
<p>Required packages: <code>kernlab</code>, <code>kerndwd</code></p>
<p><strong>glmnet</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmnet_h2o&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Mixing Percentage)</li>
<li><code>lambda</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>h2o</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>glmnet</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmnet&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Mixing Percentage)</li>
<li><code>lambda</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>glmnet</code>, <code>Matrix</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Linear Distance Weighted Discrimination</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dwdLinear&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>qval</code> (q)</li>
</ul>
<p>Required packages: <code>kerndwd</code></p>
<p><strong>Model Averaged Neural Network</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;avNNet&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>decay</code> (Weight Decay)</li>
<li><code>bag</code> (Bagging)</li>
</ul>
<p>Required packages: <code>nnet</code></p>
<p><strong>Multi-Layer Perceptron</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mlpWeightDecay&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>decay</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>RSNNS</code></p>
<p><strong>Multi-Layer Perceptron, multiple layers</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mlpWeightDecayML&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>layer1</code> (#Hidden Units layer1)</li>
<li><code>layer2</code> (#Hidden Units layer2)</li>
<li><code>layer3</code> (#Hidden Units layer3)</li>
<li><code>decay</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>RSNNS</code></p>
<p><strong>Multi-Step Adaptive MCP-Net</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;msaenet&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alphas</code> (Alpha)</li>
<li><code>nsteps</code> (#Adaptive Estimation Steps)</li>
<li><code>scale</code> (Adaptive Weight Scaling Factor)</li>
</ul>
<p>Required packages: <code>msaenet</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Multilayer Perceptron Network by Stochastic Gradient Descent</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mlpSGD&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>l2reg</code> (L2 Regularization)</li>
<li><code>lambda</code> (RMSE Gradient Scaling)</li>
<li><code>learn_rate</code> (Learning Rate)</li>
<li><code>momentum</code> (Momentum)</li>
<li><code>gamma</code> (Learning Rate Decay)</li>
<li><code>minibatchsz</code> (Batch Size)</li>
<li><code>repeats</code> (#Models)</li>
</ul>
<p>Required packages: <code>FCNN4R</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Multilayer Perceptron Network with Weight Decay</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mlpKerasDecay&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>lambda</code> (L2 Regularization)</li>
<li><code>batch_size</code> (Batch Size)</li>
<li><code>lr</code> (Learning Rate)</li>
<li><code>rho</code> (Rho)</li>
<li><code>decay</code> (Learning Rate Decay)</li>
<li><code>activation</code> (Activation Function)</li>
</ul>
<p>Required packages: <code>keras</code></p>
<p>Notes: After <code>train</code> completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use <code>keras::unsearlize_model(object$finalModel$object)</code> in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Unlike other packages used by <code>train</code>, the <code>dplyr</code> package is fully loaded when this model is used.</p>
<p><strong>Multilayer Perceptron Network with Weight Decay</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mlpKerasDecayCost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>lambda</code> (L2 Regularization)</li>
<li><code>batch_size</code> (Batch Size)</li>
<li><code>lr</code> (Learning Rate)</li>
<li><code>rho</code> (Rho)</li>
<li><code>decay</code> (Learning Rate Decay)</li>
<li><code>cost</code> (Cost)</li>
<li><code>activation</code> (Activation Function)</li>
</ul>
<p>Required packages: <code>keras</code></p>
<p>Notes: After <code>train</code> completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use <code>keras::unsearlize_model(object$finalModel$object)</code> in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Finally, the cost parameter weights the first class in the outcome vector. Unlike other packages used by <code>train</code>, the <code>dplyr</code> package is fully loaded when this model is used.</p>
<p><strong>Neural Network</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;nnet&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>decay</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>nnet</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Neural Networks with Feature Extraction</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pcaNNet&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>decay</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>nnet</code></p>
<p><strong>Oblique Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFridge&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>obliqueRF</code> package is fully loaded when this model is used.</p>
<p><strong>Penalized Linear Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;penalized&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda1</code> (L1 Penalty)</li>
<li><code>lambda2</code> (L2 Penalty)</li>
</ul>
<p>Required packages: <code>penalized</code></p>
<p><strong>Penalized Logistic Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;plr&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (L2 Penalty)</li>
<li><code>cp</code> (Complexity Parameter)</li>
</ul>
<p>Required packages: <code>stepPlr</code></p>
<p><strong>Penalized Multinomial Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;multinom&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>decay</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>nnet</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Penalized Ordinal Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ordinalNet&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Mixing Percentage)</li>
<li><code>criteria</code> (Selection Criterion)</li>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>ordinalNet</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available. Notes: Requires ordinalNet package version &gt;= 2.0</p>
<p><strong>Polynomial Kernel Regularized Least Squares</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;krlsPoly&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>degree</code> (Polynomial Degree)</li>
</ul>
<p>Required packages: <code>KRLS</code></p>
<p><strong>Quantile Regression Neural Network</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;qrnn&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>n.hidden</code> (#Hidden Units)</li>
<li><code>penalty</code> ( Weight Decay)</li>
<li><code>bag</code> (Bagged Models?)</li>
</ul>
<p>Required packages: <code>qrnn</code></p>
<p><strong>Radial Basis Function Kernel Regularized Least Squares</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;krlsRadial&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>sigma</code> (Sigma)</li>
</ul>
<p>Required packages: <code>KRLS</code>, <code>kernlab</code></p>
<p><strong>Radial Basis Function Network</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rbf&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
</ul>
<p>Required packages: <code>RSNNS</code></p>
<p><strong>Radial Basis Function Network</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rbfDDA&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>negativeThreshold</code> (Activation Limit for Conflicting Classes)</li>
</ul>
<p>Required packages: <code>RSNNS</code></p>
<p><strong>Regularized Logistic Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;regLogistic&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
<li><code>loss</code> (Loss Function)</li>
<li><code>epsilon</code> (Tolerance)</li>
</ul>
<p>Required packages: <code>LiblineaR</code></p>
<p><strong>Relaxed Lasso</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;relaxo&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Penalty Parameter)</li>
<li><code>phi</code> (Relaxation Parameter)</li>
</ul>
<p>Required packages: <code>relaxo</code>, <code>plyr</code></p>
<p><strong>Ridge Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ridge&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>elasticnet</code></p>
<p><strong>Ridge Regression with Variable Selection</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;foba&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>k</code> (#Variables Retained)</li>
<li><code>lambda</code> (L2 Penalty)</li>
</ul>
<p>Required packages: <code>foba</code></p>
<p><strong>Sparse Distance Weighted Discrimination</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;sdwd&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (L1 Penalty)</li>
<li><code>lambda2</code> (L2 Penalty)</li>
</ul>
<p>Required packages: <code>sdwd</code></p>
<p>A model-specific variable importance metric is available.</p>
<div id="Linear_Classifier">

</div>
</div>
<div id="linear-classifier" class="section level3">
<h3><span class="header-section-number">7.0.21</span> Linear Classifier</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Adjacent Categories Probability Model for Ordinal Data</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;vglmAdjCat&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>parallel</code> (Parallel Curves)</li>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>VGAM</code></p>
<p><strong>Bagged Logic Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;logicBag&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nleaves</code> (Maximum Number of Leaves)</li>
<li><code>ntrees</code> (Number of Trees)</li>
</ul>
<p>Required packages: <code>logicFS</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>logicFS</code> package is fully loaded when this model is used.</p>
<p><strong>Bayesian Generalized Linear Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bayesglm&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>arm</code></p>
<p><strong>Boosted Generalized Linear Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmboost&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>prune</code> (AIC Prune?)</li>
</ul>
<p>Required packages: <code>plyr</code>, <code>mboost</code></p>
<p>A model-specific variable importance metric is available. Notes: The <code>prune</code> option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in <code>?mboost::mstop</code>. If pruning is not used, the ensemble makes predictions using the exact value of the <code>mstop</code> tuning parameter value.</p>
<p><strong>Continuation Ratio Model for Ordinal Data</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;vglmContRatio&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>parallel</code> (Parallel Curves)</li>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>VGAM</code></p>
<p><strong>Cumulative Probability Model for Ordinal Data</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;vglmCumulative&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>parallel</code> (Parallel Curves)</li>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>VGAM</code></p>
<p><strong>Diagonal Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>model</code> (Model)</li>
<li><code>shrinkage</code> (Shrinkage Type)</li>
</ul>
<p>Required packages: <code>sparsediscrim</code></p>
<p><strong>Ensembles of Generalized Linear Models</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;randomGLM&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxInteractionOrder</code> (Interaction Order)</li>
</ul>
<p>Required packages: <code>randomGLM</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>randomGLM</code> package is fully loaded when this model is used.</p>
<p><strong>Factor-Based Linear Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;RFlda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>q</code> (# Factors)</li>
</ul>
<p>Required packages: <code>HiDimDA</code></p>
<p><strong>Gaussian Process</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gaussprLinear&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Generalized Linear Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glm&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Generalized Linear Model with Stepwise Feature Selection</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmStepAIC&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>MASS</code></p>
<p><strong>Generalized Partial Least Squares</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gpls&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K.prov</code> (#Components)</li>
</ul>
<p>Required packages: <code>gpls</code></p>
<p><strong>glmnet</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmnet_h2o&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Mixing Percentage)</li>
<li><code>lambda</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>h2o</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>glmnet</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmnet&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Mixing Percentage)</li>
<li><code>lambda</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>glmnet</code>, <code>Matrix</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Heteroscedastic Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;hda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>gamma</code> (Gamma)</li>
<li><code>lambda</code> (Lambda)</li>
<li><code>newdim</code> (Dimension of the Discriminative Subspace)</li>
</ul>
<p>Required packages: <code>hda</code></p>
<p><strong>High Dimensional Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;hdda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>threshold</code> (Threshold)</li>
<li><code>model</code> (Model Type)</li>
</ul>
<p>Required packages: <code>HDclassif</code></p>
<p><strong>High-Dimensional Regularized Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;hdrda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>gamma</code> (Gamma)</li>
<li><code>lambda</code> (Lambda)</li>
<li><code>shrinkage_type</code> (Shrinkage Type)</li>
</ul>
<p>Required packages: <code>sparsediscrim</code></p>
<p><strong>L2 Regularized Linear Support Vector Machines with Class Weights</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinearWeights2&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
<li><code>Loss</code> (Loss Function)</li>
<li><code>weight</code> (Class Weight)</li>
</ul>
<p>Required packages: <code>LiblineaR</code></p>
<p><strong>L2 Regularized Support Vector Machine (dual) with Linear Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinear3&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
<li><code>Loss</code> (Loss Function)</li>
</ul>
<p>Required packages: <code>LiblineaR</code></p>
<p><strong>Least Squares Support Vector Machine</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lssvmLinear&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>tau</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Linear Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>MASS</code></p>
<p><strong>Linear Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lda2&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>dimen</code> (#Discriminant Functions)</li>
</ul>
<p>Required packages: <code>MASS</code></p>
<p><strong>Linear Discriminant Analysis with Stepwise Feature Selection</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;stepLDA&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxvar</code> (Maximum #Variables)</li>
<li><code>direction</code> (Search Direction)</li>
</ul>
<p>Required packages: <code>klaR</code>, <code>MASS</code></p>
<p><strong>Linear Distance Weighted Discrimination</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dwdLinear&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>qval</code> (q)</li>
</ul>
<p>Required packages: <code>kerndwd</code></p>
<p><strong>Linear Support Vector Machines with Class Weights</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinearWeights&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
<li><code>weight</code> (Class Weight)</li>
</ul>
<p>Required packages: <code>e1071</code></p>
<p><strong>Localized Linear Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;loclda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>k</code> (#Nearest Neighbors)</li>
</ul>
<p>Required packages: <code>klaR</code></p>
<p><strong>Logic Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;logreg&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>treesize</code> (Maximum Number of Leaves)</li>
<li><code>ntrees</code> (Number of Trees)</li>
</ul>
<p>Required packages: <code>LogicReg</code></p>
<p><strong>Logistic Model Trees</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;LMT&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>iter</code> (# Iteratons)</li>
</ul>
<p>Required packages: <code>RWeka</code></p>
<p><strong>Maximum Uncertainty Linear Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;Mlda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>HiDimDA</code></p>
<p><strong>Multi-Step Adaptive MCP-Net</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;msaenet&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alphas</code> (Alpha)</li>
<li><code>nsteps</code> (#Adaptive Estimation Steps)</li>
<li><code>scale</code> (Adaptive Weight Scaling Factor)</li>
</ul>
<p>Required packages: <code>msaenet</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Nearest Shrunken Centroids</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pam&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>threshold</code> (Shrinkage Threshold)</li>
</ul>
<p>Required packages: <code>pamr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Ordered Logistic or Probit Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;polr&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>method</code> (parameter)</li>
</ul>
<p>Required packages: <code>MASS</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Partial Least Squares</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;kernelpls&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ncomp</code> (#Components)</li>
</ul>
<p>Required packages: <code>pls</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Partial Least Squares</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pls&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ncomp</code> (#Components)</li>
</ul>
<p>Required packages: <code>pls</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Partial Least Squares</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;simpls&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ncomp</code> (#Components)</li>
</ul>
<p>Required packages: <code>pls</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Partial Least Squares</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;widekernelpls&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ncomp</code> (#Components)</li>
</ul>
<p>Required packages: <code>pls</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Penalized Linear Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;PenalizedLDA&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (L1 Penalty)</li>
<li><code>K</code> (#Discriminant Functions)</li>
</ul>
<p>Required packages: <code>penalizedLDA</code>, <code>plyr</code></p>
<p><strong>Penalized Logistic Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;plr&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (L2 Penalty)</li>
<li><code>cp</code> (Complexity Parameter)</li>
</ul>
<p>Required packages: <code>stepPlr</code></p>
<p><strong>Penalized Multinomial Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;multinom&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>decay</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>nnet</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Penalized Ordinal Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ordinalNet&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Mixing Percentage)</li>
<li><code>criteria</code> (Selection Criterion)</li>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>ordinalNet</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available. Notes: Requires ordinalNet package version &gt;= 2.0</p>
<p><strong>Regularized Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>gamma</code> (Gamma)</li>
<li><code>lambda</code> (Lambda)</li>
</ul>
<p>Required packages: <code>klaR</code></p>
<p><strong>Regularized Linear Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rlda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>estimator</code> (Regularization Method)</li>
</ul>
<p>Required packages: <code>sparsediscrim</code></p>
<p><strong>Regularized Logistic Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;regLogistic&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
<li><code>loss</code> (Loss Function)</li>
<li><code>epsilon</code> (Tolerance)</li>
</ul>
<p>Required packages: <code>LiblineaR</code></p>
<p><strong>Robust Linear Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;Linda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>rrcov</code></p>
<p><strong>Robust Regularized Linear Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rrlda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Penalty Parameter)</li>
<li><code>hp</code> (Robustness Parameter)</li>
<li><code>penalty</code> (Penalty Type)</li>
</ul>
<p>Required packages: <code>rrlda</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>rrlda</code> package is fully loaded when this model is used.</p>
<p><strong>Robust SIMCA</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;RSimca&#39;</span></code></pre>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>rrcovHD</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>rrcovHD</code> package is fully loaded when this model is used.</p>
<p><strong>Shrinkage Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;sda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>diagonal</code> (Diagonalize)</li>
<li><code>lambda</code> (shrinkage)</li>
</ul>
<p>Required packages: <code>sda</code></p>
<p><strong>Sparse Distance Weighted Discrimination</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;sdwd&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (L1 Penalty)</li>
<li><code>lambda2</code> (L2 Penalty)</li>
</ul>
<p>Required packages: <code>sdwd</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Sparse Linear Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;sparseLDA&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>NumVars</code> (# Predictors)</li>
<li><code>lambda</code> (Lambda)</li>
</ul>
<p>Required packages: <code>sparseLDA</code></p>
<p><strong>Sparse Partial Least Squares</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;spls&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K</code> (#Components)</li>
<li><code>eta</code> (Threshold)</li>
<li><code>kappa</code> (Kappa)</li>
</ul>
<p>Required packages: <code>spls</code></p>
<p><strong>Stabilized Linear Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;slda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>ipred</code></p>
<p><strong>Support Vector Machines with Linear Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinear&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Linear Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinear2&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
</ul>
<p>Required packages: <code>e1071</code></p>
<div id="Linear_Regression">

</div>
</div>
<div id="linear-regression" class="section level3">
<h3><span class="header-section-number">7.0.22</span> Linear Regression</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Bagged Logic Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;logicBag&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nleaves</code> (Maximum Number of Leaves)</li>
<li><code>ntrees</code> (Number of Trees)</li>
</ul>
<p>Required packages: <code>logicFS</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>logicFS</code> package is fully loaded when this model is used.</p>
<p><strong>Bayesian Ridge Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bridge&#39;</span></code></pre>
<p>Type: Regression</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>monomvn</code></p>
<p><strong>Bayesian Ridge Regression (Model Averaged)</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;blassoAveraged&#39;</span></code></pre>
<p>Type: Regression</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>monomvn</code></p>
<p>Notes: This model makes predictions by averaging the predictions based on the posterior estimates of the regression coefficients. While it is possible that some of these posterior estimates are zero for non-informative predictors, the final predicted value may be a function of many (or even all) predictors.</p>
<p><strong>Boosted Linear Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;BstLm&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>nu</code> (Shrinkage)</li>
</ul>
<p>Required packages: <code>bst</code>, <code>plyr</code></p>
<p><strong>Cubist</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;cubist&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>committees</code> (#Committees)</li>
<li><code>neighbors</code> (#Instances)</li>
</ul>
<p>Required packages: <code>Cubist</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Elasticnet</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;enet&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>fraction</code> (Fraction of Full Solution)</li>
<li><code>lambda</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>elasticnet</code></p>
<p><strong>glmnet</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmnet_h2o&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Mixing Percentage)</li>
<li><code>lambda</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>h2o</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>glmnet</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmnet&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Mixing Percentage)</li>
<li><code>lambda</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>glmnet</code>, <code>Matrix</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Independent Component Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;icr&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>n.comp</code> (#Components)</li>
</ul>
<p>Required packages: <code>fastICA</code></p>
<p><strong>L2 Regularized Support Vector Machine (dual) with Linear Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinear3&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
<li><code>Loss</code> (Loss Function)</li>
</ul>
<p>Required packages: <code>LiblineaR</code></p>
<p><strong>Least Angle Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lars&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>fraction</code> (Fraction)</li>
</ul>
<p>Required packages: <code>lars</code></p>
<p><strong>Least Angle Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lars2&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>step</code> (#Steps)</li>
</ul>
<p>Required packages: <code>lars</code></p>
<p><strong>Linear Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lm&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>intercept</code> (intercept)</li>
</ul>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Linear Regression with Backwards Selection</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;leapBackward&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nvmax</code> (Maximum Number of Predictors)</li>
</ul>
<p>Required packages: <code>leaps</code></p>
<p><strong>Linear Regression with Forward Selection</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;leapForward&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nvmax</code> (Maximum Number of Predictors)</li>
</ul>
<p>Required packages: <code>leaps</code></p>
<p><strong>Linear Regression with Stepwise Selection</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;leapSeq&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nvmax</code> (Maximum Number of Predictors)</li>
</ul>
<p>Required packages: <code>leaps</code></p>
<p><strong>Linear Regression with Stepwise Selection</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lmStepAIC&#39;</span></code></pre>
<p>Type: Regression</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>MASS</code></p>
<p><strong>Logic Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;logreg&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>treesize</code> (Maximum Number of Leaves)</li>
<li><code>ntrees</code> (Number of Trees)</li>
</ul>
<p>Required packages: <code>LogicReg</code></p>
<p><strong>Model Rules</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;M5Rules&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>pruned</code> (Pruned)</li>
<li><code>smoothed</code> (Smoothed)</li>
</ul>
<p>Required packages: <code>RWeka</code></p>
<p><strong>Model Tree</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;M5&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>pruned</code> (Pruned)</li>
<li><code>smoothed</code> (Smoothed)</li>
<li><code>rules</code> (Rules)</li>
</ul>
<p>Required packages: <code>RWeka</code></p>
<p><strong>Multi-Step Adaptive MCP-Net</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;msaenet&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alphas</code> (Alpha)</li>
<li><code>nsteps</code> (#Adaptive Estimation Steps)</li>
<li><code>scale</code> (Adaptive Weight Scaling Factor)</li>
</ul>
<p>Required packages: <code>msaenet</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Non-Convex Penalized Quantile Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rqnc&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (L1 Penalty)</li>
<li><code>penalty</code> (Penalty Type)</li>
</ul>
<p>Required packages: <code>rqPen</code></p>
<p><strong>Non-Negative Least Squares</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;nnls&#39;</span></code></pre>
<p>Type: Regression</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>nnls</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Partial Least Squares</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;kernelpls&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ncomp</code> (#Components)</li>
</ul>
<p>Required packages: <code>pls</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Partial Least Squares</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pls&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ncomp</code> (#Components)</li>
</ul>
<p>Required packages: <code>pls</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Partial Least Squares</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;simpls&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ncomp</code> (#Components)</li>
</ul>
<p>Required packages: <code>pls</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Partial Least Squares</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;widekernelpls&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ncomp</code> (#Components)</li>
</ul>
<p>Required packages: <code>pls</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Penalized Linear Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;penalized&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda1</code> (L1 Penalty)</li>
<li><code>lambda2</code> (L2 Penalty)</li>
</ul>
<p>Required packages: <code>penalized</code></p>
<p><strong>Penalized Ordinal Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ordinalNet&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Mixing Percentage)</li>
<li><code>criteria</code> (Selection Criterion)</li>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>ordinalNet</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available. Notes: Requires ordinalNet package version &gt;= 2.0</p>
<p><strong>Principal Component Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pcr&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ncomp</code> (#Components)</li>
</ul>
<p>Required packages: <code>pls</code></p>
<p><strong>Quantile Regression with LASSO penalty</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rqlasso&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (L1 Penalty)</li>
</ul>
<p>Required packages: <code>rqPen</code></p>
<p><strong>Relaxed Lasso</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;relaxo&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Penalty Parameter)</li>
<li><code>phi</code> (Relaxation Parameter)</li>
</ul>
<p>Required packages: <code>relaxo</code>, <code>plyr</code></p>
<p><strong>Relevance Vector Machines with Linear Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rvmLinear&#39;</span></code></pre>
<p>Type: Regression</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Ridge Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ridge&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>elasticnet</code></p>
<p><strong>Ridge Regression with Variable Selection</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;foba&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>k</code> (#Variables Retained)</li>
<li><code>lambda</code> (L2 Penalty)</li>
</ul>
<p>Required packages: <code>foba</code></p>
<p><strong>Robust Linear Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rlm&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>intercept</code> (intercept)</li>
<li><code>psi</code> (psi)</li>
</ul>
<p>Required packages: <code>MASS</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Sparse Partial Least Squares</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;spls&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K</code> (#Components)</li>
<li><code>eta</code> (Threshold)</li>
<li><code>kappa</code> (Kappa)</li>
</ul>
<p>Required packages: <code>spls</code></p>
<p><strong>Spike and Slab Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;spikeslab&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>vars</code> (Variables Retained)</li>
</ul>
<p>Required packages: <code>spikeslab</code>, <code>plyr</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>spikeslab</code> package is fully loaded when this model is used.</p>
<p><strong>Supervised Principal Component Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;superpc&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>threshold</code> (Threshold)</li>
<li><code>n.components</code> (#Components)</li>
</ul>
<p>Required packages: <code>superpc</code></p>
<p><strong>Support Vector Machines with Linear Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinear&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Linear Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinear2&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
</ul>
<p>Required packages: <code>e1071</code></p>
<p><strong>The Bayesian lasso</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;blasso&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sparsity</code> (Sparsity Threshold)</li>
</ul>
<p>Required packages: <code>monomvn</code></p>
<p>Notes: This model creates predictions using the mean of the posterior distributions but sets some parameters specifically to zero based on the tuning parameter <code>sparsity</code>. For example, when <code>sparsity = .5</code>, only coefficients where at least half the posterior estimates are nonzero are used.</p>
<p><strong>The lasso</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lasso&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>fraction</code> (Fraction of Full Solution)</li>
</ul>
<p>Required packages: <code>elasticnet</code></p>
<div id="Logic_Regression">

</div>
</div>
<div id="logic-regression" class="section level3">
<h3><span class="header-section-number">7.0.23</span> Logic Regression</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Bagged Logic Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;logicBag&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nleaves</code> (Maximum Number of Leaves)</li>
<li><code>ntrees</code> (Number of Trees)</li>
</ul>
<p>Required packages: <code>logicFS</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>logicFS</code> package is fully loaded when this model is used.</p>
<p><strong>Logic Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;logreg&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>treesize</code> (Maximum Number of Leaves)</li>
<li><code>ntrees</code> (Number of Trees)</li>
</ul>
<p>Required packages: <code>LogicReg</code></p>
<div id="Logistic_Regression">

</div>
</div>
<div id="logistic-regression" class="section level3">
<h3><span class="header-section-number">7.0.24</span> Logistic Regression</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Adjacent Categories Probability Model for Ordinal Data</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;vglmAdjCat&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>parallel</code> (Parallel Curves)</li>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>VGAM</code></p>
<p><strong>Bagged Logic Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;logicBag&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nleaves</code> (Maximum Number of Leaves)</li>
<li><code>ntrees</code> (Number of Trees)</li>
</ul>
<p>Required packages: <code>logicFS</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>logicFS</code> package is fully loaded when this model is used.</p>
<p><strong>Bayesian Generalized Linear Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bayesglm&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>arm</code></p>
<p><strong>Boosted Logistic Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;LogitBoost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nIter</code> (# Boosting Iterations)</li>
</ul>
<p>Required packages: <code>caTools</code></p>
<p><strong>Continuation Ratio Model for Ordinal Data</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;vglmContRatio&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>parallel</code> (Parallel Curves)</li>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>VGAM</code></p>
<p><strong>Cumulative Probability Model for Ordinal Data</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;vglmCumulative&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>parallel</code> (Parallel Curves)</li>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>VGAM</code></p>
<p><strong>Generalized Partial Least Squares</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gpls&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K.prov</code> (#Components)</li>
</ul>
<p>Required packages: <code>gpls</code></p>
<p><strong>Logic Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;logreg&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>treesize</code> (Maximum Number of Leaves)</li>
<li><code>ntrees</code> (Number of Trees)</li>
</ul>
<p>Required packages: <code>LogicReg</code></p>
<p><strong>Logistic Model Trees</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;LMT&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>iter</code> (# Iteratons)</li>
</ul>
<p>Required packages: <code>RWeka</code></p>
<p><strong>Oblique Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFlog&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>obliqueRF</code> package is fully loaded when this model is used.</p>
<p><strong>Ordered Logistic or Probit Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;polr&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>method</code> (parameter)</li>
</ul>
<p>Required packages: <code>MASS</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Penalized Logistic Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;plr&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (L2 Penalty)</li>
<li><code>cp</code> (Complexity Parameter)</li>
</ul>
<p>Required packages: <code>stepPlr</code></p>
<p><strong>Penalized Multinomial Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;multinom&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>decay</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>nnet</code></p>
<p>A model-specific variable importance metric is available.</p>
<div id="Mixture_Model">

</div>
</div>
<div id="mixture-model" class="section level3">
<h3><span class="header-section-number">7.0.25</span> Mixture Model</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Adaptive Mixture Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;amdai&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>model</code> (Model Type)</li>
</ul>
<p>Required packages: <code>adaptDA</code></p>
<p><strong>Mixture Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>subclasses</code> (#Subclasses Per Class)</li>
</ul>
<p>Required packages: <code>mda</code></p>
<p><strong>Robust Mixture Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rmda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K</code> (#Subclasses Per Class)</li>
<li><code>model</code> (Model)</li>
</ul>
<p>Required packages: <code>robustDA</code></p>
<p><strong>Sparse Mixture Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;smda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>NumVars</code> (# Predictors)</li>
<li><code>lambda</code> (Lambda)</li>
<li><code>R</code> (# Subclasses)</li>
</ul>
<p>Required packages: <code>sparseLDA</code></p>
<div id="Model_Tree">

</div>
</div>
<div id="model-tree" class="section level3">
<h3><span class="header-section-number">7.0.26</span> Model Tree</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Cubist</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;cubist&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>committees</code> (#Committees)</li>
<li><code>neighbors</code> (#Instances)</li>
</ul>
<p>Required packages: <code>Cubist</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Logistic Model Trees</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;LMT&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>iter</code> (# Iteratons)</li>
</ul>
<p>Required packages: <code>RWeka</code></p>
<p><strong>Model Rules</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;M5Rules&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>pruned</code> (Pruned)</li>
<li><code>smoothed</code> (Smoothed)</li>
</ul>
<p>Required packages: <code>RWeka</code></p>
<p><strong>Model Tree</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;M5&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>pruned</code> (Pruned)</li>
<li><code>smoothed</code> (Smoothed)</li>
<li><code>rules</code> (Rules)</li>
</ul>
<p>Required packages: <code>RWeka</code></p>
<div id="Multivariate_Adaptive_Regression_Splines">

</div>
</div>
<div id="multivariate-adaptive-regression-splines" class="section level3">
<h3><span class="header-section-number">7.0.27</span> Multivariate Adaptive Regression Splines</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Bagged Flexible Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bagFDA&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Product Degree)</li>
<li><code>nprune</code> (#Terms)</li>
</ul>
<p>Required packages: <code>earth</code>, <code>mda</code></p>
<p>A model-specific variable importance metric is available. Notes: Unlike other packages used by <code>train</code>, the <code>earth</code> package is fully loaded when this model is used.</p>
<p><strong>Bagged MARS</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bagEarth&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nprune</code> (#Terms)</li>
<li><code>degree</code> (Product Degree)</li>
</ul>
<p>Required packages: <code>earth</code></p>
<p>A model-specific variable importance metric is available. Notes: Unlike other packages used by <code>train</code>, the <code>earth</code> package is fully loaded when this model is used.</p>
<p><strong>Bagged MARS using gCV Pruning</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bagEarthGCV&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Product Degree)</li>
</ul>
<p>Required packages: <code>earth</code></p>
<p>A model-specific variable importance metric is available. Notes: Unlike other packages used by <code>train</code>, the <code>earth</code> package is fully loaded when this model is used.</p>
<p><strong>Flexible Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;fda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Product Degree)</li>
<li><code>nprune</code> (#Terms)</li>
</ul>
<p>Required packages: <code>earth</code>, <code>mda</code></p>
<p>A model-specific variable importance metric is available. Notes: Unlike other packages used by <code>train</code>, the <code>earth</code> package is fully loaded when this model is used.</p>
<p><strong>Multivariate Adaptive Regression Spline</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;earth&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nprune</code> (#Terms)</li>
<li><code>degree</code> (Product Degree)</li>
</ul>
<p>Required packages: <code>earth</code></p>
<p>A model-specific variable importance metric is available. Notes: Unlike other packages used by <code>train</code>, the <code>earth</code> package is fully loaded when this model is used.</p>
<p><strong>Multivariate Adaptive Regression Splines</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gcvEarth&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Product Degree)</li>
</ul>
<p>Required packages: <code>earth</code></p>
<p>A model-specific variable importance metric is available. Notes: Unlike other packages used by <code>train</code>, the <code>earth</code> package is fully loaded when this model is used.</p>
<div id="Neural_Network">

</div>
</div>
<div id="neural-network" class="section level3">
<h3><span class="header-section-number">7.0.28</span> Neural Network</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Bayesian Regularized Neural Networks</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;brnn&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>neurons</code> (# Neurons)</li>
</ul>
<p>Required packages: <code>brnn</code></p>
<p><strong>Extreme Learning Machine</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;elm&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nhid</code> (#Hidden Units)</li>
<li><code>actfun</code> (Activation Function)</li>
</ul>
<p>Required packages: <code>elmNN</code></p>
<p>Notes: The package is no longer on CRAN but can be installed from the archive at <a href="https://cran.r-project.org/src/contrib/Archive/elmNN/" class="uri">https://cran.r-project.org/src/contrib/Archive/elmNN/</a></p>
<p><strong>Model Averaged Neural Network</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;avNNet&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>decay</code> (Weight Decay)</li>
<li><code>bag</code> (Bagging)</li>
</ul>
<p>Required packages: <code>nnet</code></p>
<p><strong>Monotone Multi-Layer Perceptron Neural Network</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;monmlp&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>hidden1</code> (#Hidden Units)</li>
<li><code>n.ensemble</code> (#Models)</li>
</ul>
<p>Required packages: <code>monmlp</code></p>
<p><strong>Multi-Layer Perceptron</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mlp&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
</ul>
<p>Required packages: <code>RSNNS</code></p>
<p><strong>Multi-Layer Perceptron</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mlpWeightDecay&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>decay</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>RSNNS</code></p>
<p><strong>Multi-Layer Perceptron, multiple layers</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mlpWeightDecayML&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>layer1</code> (#Hidden Units layer1)</li>
<li><code>layer2</code> (#Hidden Units layer2)</li>
<li><code>layer3</code> (#Hidden Units layer3)</li>
<li><code>decay</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>RSNNS</code></p>
<p><strong>Multi-Layer Perceptron, with multiple layers</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mlpML&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>layer1</code> (#Hidden Units layer1)</li>
<li><code>layer2</code> (#Hidden Units layer2)</li>
<li><code>layer3</code> (#Hidden Units layer3)</li>
</ul>
<p>Required packages: <code>RSNNS</code></p>
<p><strong>Multilayer Perceptron Network by Stochastic Gradient Descent</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mlpSGD&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>l2reg</code> (L2 Regularization)</li>
<li><code>lambda</code> (RMSE Gradient Scaling)</li>
<li><code>learn_rate</code> (Learning Rate)</li>
<li><code>momentum</code> (Momentum)</li>
<li><code>gamma</code> (Learning Rate Decay)</li>
<li><code>minibatchsz</code> (Batch Size)</li>
<li><code>repeats</code> (#Models)</li>
</ul>
<p>Required packages: <code>FCNN4R</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Multilayer Perceptron Network with Dropout</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mlpKerasDropout&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>dropout</code> (Dropout Rate)</li>
<li><code>batch_size</code> (Batch Size)</li>
<li><code>lr</code> (Learning Rate)</li>
<li><code>rho</code> (Rho)</li>
<li><code>decay</code> (Learning Rate Decay)</li>
<li><code>activation</code> (Activation Function)</li>
</ul>
<p>Required packages: <code>keras</code></p>
<p>Notes: After <code>train</code> completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use <code>keras::unsearlize_model(object$finalModel$object)</code> in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Unlike other packages used by <code>train</code>, the <code>dplyr</code> package is fully loaded when this model is used.</p>
<p><strong>Multilayer Perceptron Network with Dropout</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mlpKerasDropoutCost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>dropout</code> (Dropout Rate)</li>
<li><code>batch_size</code> (Batch Size)</li>
<li><code>lr</code> (Learning Rate)</li>
<li><code>rho</code> (Rho)</li>
<li><code>decay</code> (Learning Rate Decay)</li>
<li><code>cost</code> (Cost)</li>
<li><code>activation</code> (Activation Function)</li>
</ul>
<p>Required packages: <code>keras</code></p>
<p>Notes: After <code>train</code> completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use <code>keras::unsearlize_model(object$finalModel$object)</code> in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Finally, the cost parameter weights the first class in the outcome vector. Unlike other packages used by <code>train</code>, the <code>dplyr</code> package is fully loaded when this model is used.</p>
<p><strong>Multilayer Perceptron Network with Weight Decay</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mlpKerasDecay&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>lambda</code> (L2 Regularization)</li>
<li><code>batch_size</code> (Batch Size)</li>
<li><code>lr</code> (Learning Rate)</li>
<li><code>rho</code> (Rho)</li>
<li><code>decay</code> (Learning Rate Decay)</li>
<li><code>activation</code> (Activation Function)</li>
</ul>
<p>Required packages: <code>keras</code></p>
<p>Notes: After <code>train</code> completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use <code>keras::unsearlize_model(object$finalModel$object)</code> in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Unlike other packages used by <code>train</code>, the <code>dplyr</code> package is fully loaded when this model is used.</p>
<p><strong>Multilayer Perceptron Network with Weight Decay</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mlpKerasDecayCost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>lambda</code> (L2 Regularization)</li>
<li><code>batch_size</code> (Batch Size)</li>
<li><code>lr</code> (Learning Rate)</li>
<li><code>rho</code> (Rho)</li>
<li><code>decay</code> (Learning Rate Decay)</li>
<li><code>cost</code> (Cost)</li>
<li><code>activation</code> (Activation Function)</li>
</ul>
<p>Required packages: <code>keras</code></p>
<p>Notes: After <code>train</code> completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use <code>keras::unsearlize_model(object$finalModel$object)</code> in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Finally, the cost parameter weights the first class in the outcome vector. Unlike other packages used by <code>train</code>, the <code>dplyr</code> package is fully loaded when this model is used.</p>
<p><strong>Neural Network</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mxnet&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>layer1</code> (#Hidden Units in Layer 1)</li>
<li><code>layer2</code> (#Hidden Units in Layer 2)</li>
<li><code>layer3</code> (#Hidden Units in Layer 3)</li>
<li><code>learning.rate</code> (Learning Rate)</li>
<li><code>momentum</code> (Momentum)</li>
<li><code>dropout</code> (Dropout Rate)</li>
<li><code>activation</code> (Activation Function)</li>
</ul>
<p>Required packages: <code>mxnet</code></p>
<p>Notes: The <code>mxnet</code> package is not yet on CRAN. See <a href="http://mxnet.io" class="uri">http://mxnet.io</a> for installation instructions.</p>
<p><strong>Neural Network</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mxnetAdam&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>layer1</code> (#Hidden Units in Layer 1)</li>
<li><code>layer2</code> (#Hidden Units in Layer 2)</li>
<li><code>layer3</code> (#Hidden Units in Layer 3)</li>
<li><code>dropout</code> (Dropout Rate)</li>
<li><code>beta1</code> (beta1)</li>
<li><code>beta2</code> (beta2)</li>
<li><code>learningrate</code> (Learning Rate)</li>
<li><code>activation</code> (Activation Function)</li>
</ul>
<p>Required packages: <code>mxnet</code></p>
<p>Notes: The <code>mxnet</code> package is not yet on CRAN. See <a href="http://mxnet.io" class="uri">http://mxnet.io</a> for installation instructions. Users are strongly advised to define <code>num.round</code> themselves.</p>
<p><strong>Neural Network</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;neuralnet&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>layer1</code> (#Hidden Units in Layer 1)</li>
<li><code>layer2</code> (#Hidden Units in Layer 2)</li>
<li><code>layer3</code> (#Hidden Units in Layer 3)</li>
</ul>
<p>Required packages: <code>neuralnet</code></p>
<p><strong>Neural Network</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;nnet&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>decay</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>nnet</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Neural Networks with Feature Extraction</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pcaNNet&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>decay</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>nnet</code></p>
<p><strong>Penalized Multinomial Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;multinom&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>decay</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>nnet</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Quantile Regression Neural Network</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;qrnn&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>n.hidden</code> (#Hidden Units)</li>
<li><code>penalty</code> ( Weight Decay)</li>
<li><code>bag</code> (Bagged Models?)</li>
</ul>
<p>Required packages: <code>qrnn</code></p>
<p><strong>Radial Basis Function Network</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rbf&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
</ul>
<p>Required packages: <code>RSNNS</code></p>
<p><strong>Radial Basis Function Network</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rbfDDA&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>negativeThreshold</code> (Activation Limit for Conflicting Classes)</li>
</ul>
<p>Required packages: <code>RSNNS</code></p>
<p><strong>Stacked AutoEncoder Deep Neural Network</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dnn&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>layer1</code> (Hidden Layer 1)</li>
<li><code>layer2</code> (Hidden Layer 2)</li>
<li><code>layer3</code> (Hidden Layer 3)</li>
<li><code>hidden_dropout</code> (Hidden Dropouts)</li>
<li><code>visible_dropout</code> (Visible Dropout)</li>
</ul>
<p>Required packages: <code>deepnet</code></p>
<div id="Oblique_Tree">

</div>
</div>
<div id="oblique-tree" class="section level3">
<h3><span class="header-section-number">7.0.29</span> Oblique Tree</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Oblique Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFlog&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>obliqueRF</code> package is fully loaded when this model is used.</p>
<p><strong>Oblique Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFpls&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>obliqueRF</code> package is fully loaded when this model is used.</p>
<p><strong>Oblique Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFridge&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>obliqueRF</code> package is fully loaded when this model is used.</p>
<p><strong>Oblique Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFsvm&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>obliqueRF</code> package is fully loaded when this model is used.</p>
<div id="Ordinal_Outcomes">

</div>
</div>
<div id="ordinal-outcomes" class="section level3">
<h3><span class="header-section-number">7.0.30</span> Ordinal Outcomes</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Adjacent Categories Probability Model for Ordinal Data</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;vglmAdjCat&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>parallel</code> (Parallel Curves)</li>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>VGAM</code></p>
<p><strong>CART or Ordinal Responses</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpartScore&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cp</code> (Complexity Parameter)</li>
<li><code>split</code> (Split Function)</li>
<li><code>prune</code> (Pruning Measure)</li>
</ul>
<p>Required packages: <code>rpartScore</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Continuation Ratio Model for Ordinal Data</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;vglmContRatio&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>parallel</code> (Parallel Curves)</li>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>VGAM</code></p>
<p><strong>Cumulative Probability Model for Ordinal Data</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;vglmCumulative&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>parallel</code> (Parallel Curves)</li>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>VGAM</code></p>
<p><strong>Ordered Logistic or Probit Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;polr&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>method</code> (parameter)</li>
</ul>
<p>Required packages: <code>MASS</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Penalized Ordinal Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ordinalNet&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Mixing Percentage)</li>
<li><code>criteria</code> (Selection Criterion)</li>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>ordinalNet</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available. Notes: Requires ordinalNet package version &gt;= 2.0</p>
<p><strong>Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ordinalRF&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nsets</code> (# score sets tried prior to the approximation)</li>
<li><code>ntreeperdiv</code> (# of trees (small RFs))</li>
<li><code>ntreefinal</code> (# of trees (final RF))</li>
</ul>
<p>Required packages: <code>e1071</code>, <code>ranger</code>, <code>dplyr</code>, <code>ordinalForest</code></p>
<p>A model-specific variable importance metric is available.</p>
<div id="Partial_Least_Squares">

</div>
</div>
<div id="partial-least-squares" class="section level3">
<h3><span class="header-section-number">7.0.31</span> Partial Least Squares</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Generalized Partial Least Squares</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gpls&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K.prov</code> (#Components)</li>
</ul>
<p>Required packages: <code>gpls</code></p>
<p><strong>Oblique Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFpls&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>obliqueRF</code> package is fully loaded when this model is used.</p>
<p><strong>Partial Least Squares</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;kernelpls&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ncomp</code> (#Components)</li>
</ul>
<p>Required packages: <code>pls</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Partial Least Squares</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pls&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ncomp</code> (#Components)</li>
</ul>
<p>Required packages: <code>pls</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Partial Least Squares</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;simpls&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ncomp</code> (#Components)</li>
</ul>
<p>Required packages: <code>pls</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Partial Least Squares</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;widekernelpls&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ncomp</code> (#Components)</li>
</ul>
<p>Required packages: <code>pls</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Partial Least Squares Generalized Linear Models </strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;plsRglm&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nt</code> (#PLS Components)</li>
<li><code>alpha.pvals.expli</code> (p-Value threshold)</li>
</ul>
<p>Required packages: <code>plsRglm</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>plsRglm</code> package is fully loaded when this model is used.</p>
<p><strong>Sparse Partial Least Squares</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;spls&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K</code> (#Components)</li>
<li><code>eta</code> (Threshold)</li>
<li><code>kappa</code> (Kappa)</li>
</ul>
<p>Required packages: <code>spls</code></p>
<div id="Patient_Rule_Induction_Method">

</div>
</div>
<div id="patient-rule-induction-method" class="section level3">
<h3><span class="header-section-number">7.0.32</span> Patient Rule Induction Method</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Patient Rule Induction Method</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;PRIM&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>peel.alpha</code> (peeling quantile)</li>
<li><code>paste.alpha</code> (pasting quantile)</li>
<li><code>mass.min</code> (minimum mass)</li>
</ul>
<p>Required packages: <code>supervisedPRIM</code></p>
<div id="Polynomial_Model">

</div>
</div>
<div id="polynomial-model" class="section level3">
<h3><span class="header-section-number">7.0.33</span> Polynomial Model</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Diagonal Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>model</code> (Model)</li>
<li><code>shrinkage</code> (Shrinkage Type)</li>
</ul>
<p>Required packages: <code>sparsediscrim</code></p>
<p><strong>Distance Weighted Discrimination with Polynomial Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dwdPoly&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>qval</code> (q)</li>
<li><code>degree</code> (Polynomial Degree)</li>
<li><code>scale</code> (Scale)</li>
</ul>
<p>Required packages: <code>kerndwd</code></p>
<p><strong>Gaussian Process with Polynomial Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gaussprPoly&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Polynomial Degree)</li>
<li><code>scale</code> (Scale)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>High-Dimensional Regularized Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;hdrda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>gamma</code> (Gamma)</li>
<li><code>lambda</code> (Lambda)</li>
<li><code>shrinkage_type</code> (Shrinkage Type)</li>
</ul>
<p>Required packages: <code>sparsediscrim</code></p>
<p><strong>Least Squares Support Vector Machine with Polynomial Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lssvmPoly&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Polynomial Degree)</li>
<li><code>scale</code> (Scale)</li>
<li><code>tau</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Penalized Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Shrinkage Penalty Coefficient)</li>
</ul>
<p>Required packages: <code>mda</code></p>
<p><strong>Penalized Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pda2&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>df</code> (Degrees of Freedom)</li>
</ul>
<p>Required packages: <code>mda</code></p>
<p><strong>Polynomial Kernel Regularized Least Squares</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;krlsPoly&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>degree</code> (Polynomial Degree)</li>
</ul>
<p>Required packages: <code>KRLS</code></p>
<p><strong>Quadratic Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;qda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>MASS</code></p>
<p><strong>Quadratic Discriminant Analysis with Stepwise Feature Selection</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;stepQDA&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxvar</code> (Maximum #Variables)</li>
<li><code>direction</code> (Search Direction)</li>
</ul>
<p>Required packages: <code>klaR</code>, <code>MASS</code></p>
<p><strong>Regularized Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>gamma</code> (Gamma)</li>
<li><code>lambda</code> (Lambda)</li>
</ul>
<p>Required packages: <code>klaR</code></p>
<p><strong>Regularized Linear Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rlda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>estimator</code> (Regularization Method)</li>
</ul>
<p>Required packages: <code>sparsediscrim</code></p>
<p><strong>Relevance Vector Machines with Polynomial Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rvmPoly&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>scale</code> (Scale)</li>
<li><code>degree</code> (Polynomial Degree)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Robust Quadratic Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;QdaCov&#39;</span></code></pre>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>rrcov</code></p>
<p><strong>Support Vector Machines with Polynomial Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmPoly&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Polynomial Degree)</li>
<li><code>scale</code> (Scale)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<div id="Prototype_Models">

</div>
</div>
<div id="prototype-models" class="section level3">
<h3><span class="header-section-number">7.0.34</span> Prototype Models</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Cubist</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;cubist&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>committees</code> (#Committees)</li>
<li><code>neighbors</code> (#Instances)</li>
</ul>
<p>Required packages: <code>Cubist</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Greedy Prototype Selection</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;protoclass&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>eps</code> (Ball Size)</li>
<li><code>Minkowski</code> (Distance Order)</li>
</ul>
<p>Required packages: <code>proxy</code>, <code>protoclass</code></p>
<p><strong>k-Nearest Neighbors</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;kknn&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>kmax</code> (Max. #Neighbors)</li>
<li><code>distance</code> (Distance)</li>
<li><code>kernel</code> (Kernel)</li>
</ul>
<p>Required packages: <code>kknn</code></p>
<p><strong>k-Nearest Neighbors</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;knn&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>k</code> (#Neighbors)</li>
</ul>
<p><strong>Learning Vector Quantization</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lvq&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (Codebook Size)</li>
<li><code>k</code> (#Prototypes)</li>
</ul>
<p>Required packages: <code>class</code></p>
<p><strong>Nearest Shrunken Centroids</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pam&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>threshold</code> (Shrinkage Threshold)</li>
</ul>
<p>Required packages: <code>pamr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Optimal Weighted Nearest Neighbor Classifier</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ownn&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K</code> (#Neighbors)</li>
</ul>
<p>Required packages: <code>snn</code></p>
<p><strong>Stabilized Nearest Neighbor Classifier</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;snn&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Stabilization Parameter)</li>
</ul>
<p>Required packages: <code>snn</code></p>
<div id="Quantile_Regression">

</div>
</div>
<div id="quantile-regression" class="section level3">
<h3><span class="header-section-number">7.0.35</span> Quantile Regression</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Non-Convex Penalized Quantile Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rqnc&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (L1 Penalty)</li>
<li><code>penalty</code> (Penalty Type)</li>
</ul>
<p>Required packages: <code>rqPen</code></p>
<p><strong>Quantile Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;qrf&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>quantregForest</code></p>
<p><strong>Quantile Regression Neural Network</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;qrnn&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>n.hidden</code> (#Hidden Units)</li>
<li><code>penalty</code> ( Weight Decay)</li>
<li><code>bag</code> (Bagged Models?)</li>
</ul>
<p>Required packages: <code>qrnn</code></p>
<p><strong>Quantile Regression with LASSO penalty</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rqlasso&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (L1 Penalty)</li>
</ul>
<p>Required packages: <code>rqPen</code></p>
<div id="Radial_Basis_Function">

</div>
</div>
<div id="radial-basis-function" class="section level3">
<h3><span class="header-section-number">7.0.36</span> Radial Basis Function</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Distance Weighted Discrimination with Radial Basis Function Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dwdRadial&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>qval</code> (q)</li>
<li><code>sigma</code> (Sigma)</li>
</ul>
<p>Required packages: <code>kernlab</code>, <code>kerndwd</code></p>
<p><strong>Gaussian Process with Radial Basis Function Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gaussprRadial&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Least Squares Support Vector Machine with Radial Basis Function Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lssvmRadial&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
<li><code>tau</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Radial Basis Function Kernel Regularized Least Squares</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;krlsRadial&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>sigma</code> (Sigma)</li>
</ul>
<p>Required packages: <code>KRLS</code>, <code>kernlab</code></p>
<p><strong>Radial Basis Function Network</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rbf&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
</ul>
<p>Required packages: <code>RSNNS</code></p>
<p><strong>Radial Basis Function Network</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rbfDDA&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>negativeThreshold</code> (Activation Limit for Conflicting Classes)</li>
</ul>
<p>Required packages: <code>RSNNS</code></p>
<p><strong>Relevance Vector Machines with Radial Basis Function Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rvmRadial&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Class Weights</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmRadialWeights&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
<li><code>C</code> (Cost)</li>
<li><code>Weight</code> (Weight)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Radial Basis Function Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmRadial&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Radial Basis Function Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmRadialCost&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Radial Basis Function Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmRadialSigma&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p>Notes: This SVM model tunes over the cost parameter and the RBF kernel parameter sigma. In the latter case, using <code>tuneLength</code> will, at most, evaluate six values of the kernel parameter. This enables a broad search over the cost parameter and a relatively narrow search over <code>sigma</code></p>
<p><strong>Variational Bayesian Multinomial Probit Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;vbmpRadial&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>estimateTheta</code> (Theta Estimated)</li>
</ul>
<p>Required packages: <code>vbmp</code></p>
<div id="Random_Forest">

</div>
</div>
<div id="random-forest" class="section level3">
<h3><span class="header-section-number">7.0.37</span> Random Forest</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Conditional Inference Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;cforest&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>party</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Oblique Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFlog&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>obliqueRF</code> package is fully loaded when this model is used.</p>
<p><strong>Oblique Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFpls&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>obliqueRF</code> package is fully loaded when this model is used.</p>
<p><strong>Oblique Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFridge&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>obliqueRF</code> package is fully loaded when this model is used.</p>
<p><strong>Oblique Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFsvm&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>obliqueRF</code> package is fully loaded when this model is used.</p>
<p><strong>Parallel Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;parRF&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>e1071</code>, <code>randomForest</code>, <code>foreach</code>, <code>import</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Quantile Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;qrf&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>quantregForest</code></p>
<p><strong>Random Ferns</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rFerns&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>depth</code> (Fern Depth)</li>
</ul>
<p>Required packages: <code>rFerns</code></p>
<p><strong>Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ordinalRF&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nsets</code> (# score sets tried prior to the approximation)</li>
<li><code>ntreeperdiv</code> (# of trees (small RFs))</li>
<li><code>ntreefinal</code> (# of trees (final RF))</li>
</ul>
<p>Required packages: <code>e1071</code>, <code>ranger</code>, <code>dplyr</code>, <code>ordinalForest</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ranger&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
<li><code>splitrule</code> (Splitting Rule)</li>
<li><code>min.node.size</code> (Minimal Node Size)</li>
</ul>
<p>Required packages: <code>e1071</code>, <code>ranger</code>, <code>dplyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;Rborist&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>predFixed</code> (#Randomly Selected Predictors)</li>
<li><code>minNode</code> (Minimal Node Size)</li>
</ul>
<p>Required packages: <code>Rborist</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rf&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>randomForest</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Random Forest by Randomization</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;extraTrees&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (# Randomly Selected Predictors)</li>
<li><code>numRandomCuts</code> (# Random Cuts)</li>
</ul>
<p>Required packages: <code>extraTrees</code></p>
<p><strong>Random Forest Rule-Based Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rfRules&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
<li><code>maxdepth</code> (Maximum Rule Depth)</li>
</ul>
<p>Required packages: <code>randomForest</code>, <code>inTrees</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Regularized Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;RRF&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
<li><code>coefReg</code> (Regularization Value)</li>
<li><code>coefImp</code> (Importance Coefficient)</li>
</ul>
<p>Required packages: <code>randomForest</code>, <code>RRF</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Regularized Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;RRFglobal&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
<li><code>coefReg</code> (Regularization Value)</li>
</ul>
<p>Required packages: <code>RRF</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Weighted Subspace Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;wsrf&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>wsrf</code></p>
<div id="Regularization">

</div>
</div>
<div id="regularization" class="section level3">
<h3><span class="header-section-number">7.0.38</span> Regularization</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Bayesian Regularized Neural Networks</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;brnn&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>neurons</code> (# Neurons)</li>
</ul>
<p>Required packages: <code>brnn</code></p>
<p><strong>Diagonal Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>model</code> (Model)</li>
<li><code>shrinkage</code> (Shrinkage Type)</li>
</ul>
<p>Required packages: <code>sparsediscrim</code></p>
<p><strong>Heteroscedastic Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;hda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>gamma</code> (Gamma)</li>
<li><code>lambda</code> (Lambda)</li>
<li><code>newdim</code> (Dimension of the Discriminative Subspace)</li>
</ul>
<p>Required packages: <code>hda</code></p>
<p><strong>High-Dimensional Regularized Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;hdrda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>gamma</code> (Gamma)</li>
<li><code>lambda</code> (Lambda)</li>
<li><code>shrinkage_type</code> (Shrinkage Type)</li>
</ul>
<p>Required packages: <code>sparsediscrim</code></p>
<p><strong>Regularized Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>gamma</code> (Gamma)</li>
<li><code>lambda</code> (Lambda)</li>
</ul>
<p>Required packages: <code>klaR</code></p>
<p><strong>Regularized Linear Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rlda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>estimator</code> (Regularization Method)</li>
</ul>
<p>Required packages: <code>sparsediscrim</code></p>
<p><strong>Regularized Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;RRF&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
<li><code>coefReg</code> (Regularization Value)</li>
<li><code>coefImp</code> (Importance Coefficient)</li>
</ul>
<p>Required packages: <code>randomForest</code>, <code>RRF</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Regularized Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;RRFglobal&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
<li><code>coefReg</code> (Regularization Value)</li>
</ul>
<p>Required packages: <code>RRF</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Robust Regularized Linear Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rrlda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Penalty Parameter)</li>
<li><code>hp</code> (Robustness Parameter)</li>
<li><code>penalty</code> (Penalty Type)</li>
</ul>
<p>Required packages: <code>rrlda</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>rrlda</code> package is fully loaded when this model is used.</p>
<p><strong>Shrinkage Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;sda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>diagonal</code> (Diagonalize)</li>
<li><code>lambda</code> (shrinkage)</li>
</ul>
<p>Required packages: <code>sda</code></p>
<div id="Relevance_Vector_Machines">

</div>
</div>
<div id="relevance-vector-machines" class="section level3">
<h3><span class="header-section-number">7.0.39</span> Relevance Vector Machines</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Relevance Vector Machines with Linear Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rvmLinear&#39;</span></code></pre>
<p>Type: Regression</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Relevance Vector Machines with Polynomial Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rvmPoly&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>scale</code> (Scale)</li>
<li><code>degree</code> (Polynomial Degree)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Relevance Vector Machines with Radial Basis Function Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rvmRadial&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<div id="Ridge_Regression">

</div>
</div>
<div id="ridge-regression" class="section level3">
<h3><span class="header-section-number">7.0.40</span> Ridge Regression</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Oblique Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFridge&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>obliqueRF</code> package is fully loaded when this model is used.</p>
<p><strong>Ridge Regression with Variable Selection</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;foba&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>k</code> (#Variables Retained)</li>
<li><code>lambda</code> (L2 Penalty)</li>
</ul>
<p>Required packages: <code>foba</code></p>
<div id="Robust_Methods">

</div>
</div>
<div id="robust-methods" class="section level3">
<h3><span class="header-section-number">7.0.41</span> Robust Methods</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>L2 Regularized Linear Support Vector Machines with Class Weights</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinearWeights2&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
<li><code>Loss</code> (Loss Function)</li>
<li><code>weight</code> (Class Weight)</li>
</ul>
<p>Required packages: <code>LiblineaR</code></p>
<p><strong>L2 Regularized Support Vector Machine (dual) with Linear Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinear3&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
<li><code>Loss</code> (Loss Function)</li>
</ul>
<p>Required packages: <code>LiblineaR</code></p>
<p><strong>Linear Support Vector Machines with Class Weights</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinearWeights&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
<li><code>weight</code> (Class Weight)</li>
</ul>
<p>Required packages: <code>e1071</code></p>
<p><strong>Regularized Logistic Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;regLogistic&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
<li><code>loss</code> (Loss Function)</li>
<li><code>epsilon</code> (Tolerance)</li>
</ul>
<p>Required packages: <code>LiblineaR</code></p>
<p><strong>Relevance Vector Machines with Linear Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rvmLinear&#39;</span></code></pre>
<p>Type: Regression</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Relevance Vector Machines with Polynomial Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rvmPoly&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>scale</code> (Scale)</li>
<li><code>degree</code> (Polynomial Degree)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Relevance Vector Machines with Radial Basis Function Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rvmRadial&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Robust Mixture Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rmda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K</code> (#Subclasses Per Class)</li>
<li><code>model</code> (Model)</li>
</ul>
<p>Required packages: <code>robustDA</code></p>
<p><strong>Support Vector Machines with Boundrange String Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmBoundrangeString&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>length</code> (length)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Exponential String Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmExpoString&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (lambda)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Linear Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinear&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Linear Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinear2&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
</ul>
<p>Required packages: <code>e1071</code></p>
<p><strong>Support Vector Machines with Polynomial Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmPoly&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Polynomial Degree)</li>
<li><code>scale</code> (Scale)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Radial Basis Function Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmRadial&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Radial Basis Function Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmRadialSigma&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p>Notes: This SVM model tunes over the cost parameter and the RBF kernel parameter sigma. In the latter case, using <code>tuneLength</code> will, at most, evaluate six values of the kernel parameter. This enables a broad search over the cost parameter and a relatively narrow search over <code>sigma</code></p>
<p><strong>Support Vector Machines with Spectrum String Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmSpectrumString&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>length</code> (length)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<div id="Robust_Model">

</div>
</div>
<div id="robust-model" class="section level3">
<h3><span class="header-section-number">7.0.42</span> Robust Model</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Quantile Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;qrf&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>quantregForest</code></p>
<p><strong>Quantile Regression Neural Network</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;qrnn&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>n.hidden</code> (#Hidden Units)</li>
<li><code>penalty</code> ( Weight Decay)</li>
<li><code>bag</code> (Bagged Models?)</li>
</ul>
<p>Required packages: <code>qrnn</code></p>
<p><strong>Robust Linear Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;Linda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>rrcov</code></p>
<p><strong>Robust Linear Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rlm&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>intercept</code> (intercept)</li>
<li><code>psi</code> (psi)</li>
</ul>
<p>Required packages: <code>MASS</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Robust Regularized Linear Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rrlda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Penalty Parameter)</li>
<li><code>hp</code> (Robustness Parameter)</li>
<li><code>penalty</code> (Penalty Type)</li>
</ul>
<p>Required packages: <code>rrlda</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>rrlda</code> package is fully loaded when this model is used.</p>
<p><strong>Robust SIMCA</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;RSimca&#39;</span></code></pre>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>rrcovHD</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>rrcovHD</code> package is fully loaded when this model is used.</p>
<p><strong>SIMCA</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;CSimca&#39;</span></code></pre>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>rrcov</code>, <code>rrcovHD</code></p>
<div id="ROC_Curves">

</div>
</div>
<div id="roc-curves" class="section level3">
<h3><span class="header-section-number">7.0.43</span> ROC Curves</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>ROC-Based Classifier</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rocc&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>xgenes</code> (#Variables Retained)</li>
</ul>
<p>Required packages: <code>rocc</code></p>
<div id="Rule_Based_Model">

</div>
</div>
<div id="rule-based-model" class="section level3">
<h3><span class="header-section-number">7.0.44</span> Rule-Based Model</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Adaptive-Network-Based Fuzzy Inference System</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ANFIS&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>num.labels</code> (#Fuzzy Terms)</li>
<li><code>max.iter</code> (Max. Iterations)</li>
</ul>
<p>Required packages: <code>frbs</code></p>
<p><strong>C5.0</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>trials</code> (# Boosting Iterations)</li>
<li><code>model</code> (Model Type)</li>
<li><code>winnow</code> (Winnow)</li>
</ul>
<p>Required packages: <code>C50</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Cost-Sensitive C5.0</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0Cost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>trials</code> (# Boosting Iterations)</li>
<li><code>model</code> (Model Type)</li>
<li><code>winnow</code> (Winnow)</li>
<li><code>cost</code> (Cost)</li>
</ul>
<p>Required packages: <code>C50</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Cubist</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;cubist&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>committees</code> (#Committees)</li>
<li><code>neighbors</code> (#Instances)</li>
</ul>
<p>Required packages: <code>Cubist</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Dynamic Evolving Neural-Fuzzy Inference System </strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;DENFIS&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>Dthr</code> (Threshold)</li>
<li><code>max.iter</code> (Max. Iterations)</li>
</ul>
<p>Required packages: <code>frbs</code></p>
<p><strong>Fuzzy Inference Rules by Descent Method</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;FIR.DM&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>num.labels</code> (#Fuzzy Terms)</li>
<li><code>max.iter</code> (Max. Iterations)</li>
</ul>
<p>Required packages: <code>frbs</code></p>
<p><strong>Fuzzy Rules Using Chis Method</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;FRBCS.CHI&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>num.labels</code> (#Fuzzy Terms)</li>
<li><code>type.mf</code> (Membership Function)</li>
</ul>
<p>Required packages: <code>frbs</code></p>
<p><strong>Fuzzy Rules Using Genetic Cooperative-Competitive Learning and Pittsburgh</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;FH.GBML&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>max.num.rule</code> (Max. #Rules)</li>
<li><code>popu.size</code> (Population Size)</li>
<li><code>max.gen</code> (Max. Generations)</li>
</ul>
<p>Required packages: <code>frbs</code></p>
<p><strong>Fuzzy Rules Using the Structural Learning Algorithm on Vague Environment</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;SLAVE&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>num.labels</code> (#Fuzzy Terms)</li>
<li><code>max.iter</code> (Max. Iterations)</li>
<li><code>max.gen</code> (Max. Generations)</li>
</ul>
<p>Required packages: <code>frbs</code></p>
<p><strong>Fuzzy Rules via MOGUL</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;GFS.FR.MOGUL&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>max.gen</code> (Max. Generations)</li>
<li><code>max.iter</code> (Max. Iterations)</li>
<li><code>max.tune</code> (Max. Tuning Iterations)</li>
</ul>
<p>Required packages: <code>frbs</code></p>
<p><strong>Fuzzy Rules via Thrift</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;GFS.THRIFT&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>popu.size</code> (Population Size)</li>
<li><code>num.labels</code> (# Fuzzy Labels)</li>
<li><code>max.gen</code> (Max. Generations)</li>
</ul>
<p>Required packages: <code>frbs</code></p>
<p><strong>Fuzzy Rules with Weight Factor</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;FRBCS.W&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>num.labels</code> (#Fuzzy Terms)</li>
<li><code>type.mf</code> (Membership Function)</li>
</ul>
<p>Required packages: <code>frbs</code></p>
<p><strong>Genetic Lateral Tuning and Rule Selection of Linguistic Fuzzy Systems</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;GFS.LT.RS&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>popu.size</code> (Population Size)</li>
<li><code>num.labels</code> (# Fuzzy Labels)</li>
<li><code>max.gen</code> (Max. Generations)</li>
</ul>
<p>Required packages: <code>frbs</code></p>
<p><strong>Hybrid Neural Fuzzy Inference System</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;HYFIS&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>num.labels</code> (#Fuzzy Terms)</li>
<li><code>max.iter</code> (Max. Iterations)</li>
</ul>
<p>Required packages: <code>frbs</code></p>
<p><strong>Model Rules</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;M5Rules&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>pruned</code> (Pruned)</li>
<li><code>smoothed</code> (Smoothed)</li>
</ul>
<p>Required packages: <code>RWeka</code></p>
<p><strong>Model Tree</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;M5&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>pruned</code> (Pruned)</li>
<li><code>smoothed</code> (Smoothed)</li>
<li><code>rules</code> (Rules)</li>
</ul>
<p>Required packages: <code>RWeka</code></p>
<p><strong>Patient Rule Induction Method</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;PRIM&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>peel.alpha</code> (peeling quantile)</li>
<li><code>paste.alpha</code> (pasting quantile)</li>
<li><code>mass.min</code> (minimum mass)</li>
</ul>
<p>Required packages: <code>supervisedPRIM</code></p>
<p><strong>Random Forest Rule-Based Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rfRules&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
<li><code>maxdepth</code> (Maximum Rule Depth)</li>
</ul>
<p>Required packages: <code>randomForest</code>, <code>inTrees</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Rule-Based Classifier</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;JRip&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>NumOpt</code> (# Optimizations)</li>
<li><code>NumFolds</code> (# Folds)</li>
<li><code>MinWeights</code> (Min Weights)</li>
</ul>
<p>Required packages: <code>RWeka</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Rule-Based Classifier</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;PART&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>threshold</code> (Confidence Threshold)</li>
<li><code>pruned</code> (Pruning)</li>
</ul>
<p>Required packages: <code>RWeka</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Simplified TSK Fuzzy Rules</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;FS.HGD&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>num.labels</code> (#Fuzzy Terms)</li>
<li><code>max.iter</code> (Max. Iterations)</li>
</ul>
<p>Required packages: <code>frbs</code></p>
<p><strong>Single C5.0 Ruleset</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0Rules&#39;</span></code></pre>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>C50</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Single Rule Classification</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;OneR&#39;</span></code></pre>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>RWeka</code></p>
<p><strong>Subtractive Clustering and Fuzzy c-Means Rules</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;SBC&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>r.a</code> (Radius)</li>
<li><code>eps.high</code> (Upper Threshold)</li>
<li><code>eps.low</code> (Lower Threshold)</li>
</ul>
<p>Required packages: <code>frbs</code></p>
<p><strong>Wang and Mendel Fuzzy Rules</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;WM&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>num.labels</code> (#Fuzzy Terms)</li>
<li><code>type.mf</code> (Membership Function)</li>
</ul>
<p>Required packages: <code>frbs</code></p>
<div id="Self_Organising_Maps">

</div>
</div>
<div id="self-organising-maps" class="section level3">
<h3><span class="header-section-number">7.0.45</span> Self-Organising Maps</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Self-Organizing Maps</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;xyf&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>xdim</code> (Rows)</li>
<li><code>ydim</code> (Columns)</li>
<li><code>user.weights</code> (Layer Weight)</li>
<li><code>topo</code> (Topology)</li>
</ul>
<p>Required packages: <code>kohonen</code></p>
<p>Notes: As of version 3.0.0 of the kohonen package, the argument <code>user.weights</code> replaces the old <code>alpha</code> parameter. <code>user.weights</code> is usually a vector of relative weights such as <code>c(1, 3)</code> but is parameterized here as a proportion such as <code>c(1-.75, .75)</code> where the .75 is the value of the tuning parameter passed to <code>train</code> and indicates that the outcome layer has 3 times the weight as the predictor layer.</p>
<div id="String_Kernel">

</div>
</div>
<div id="string-kernel" class="section level3">
<h3><span class="header-section-number">7.0.46</span> String Kernel</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Support Vector Machines with Boundrange String Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmBoundrangeString&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>length</code> (length)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Exponential String Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmExpoString&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (lambda)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Spectrum String Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmSpectrumString&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>length</code> (length)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<div id="Support_Vector_Machines">

</div>
</div>
<div id="support-vector-machines" class="section level3">
<h3><span class="header-section-number">7.0.47</span> Support Vector Machines</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>L2 Regularized Linear Support Vector Machines with Class Weights</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinearWeights2&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
<li><code>Loss</code> (Loss Function)</li>
<li><code>weight</code> (Class Weight)</li>
</ul>
<p>Required packages: <code>LiblineaR</code></p>
<p><strong>L2 Regularized Support Vector Machine (dual) with Linear Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinear3&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
<li><code>Loss</code> (Loss Function)</li>
</ul>
<p>Required packages: <code>LiblineaR</code></p>
<p><strong>Least Squares Support Vector Machine</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lssvmLinear&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>tau</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Least Squares Support Vector Machine with Polynomial Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lssvmPoly&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Polynomial Degree)</li>
<li><code>scale</code> (Scale)</li>
<li><code>tau</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Least Squares Support Vector Machine with Radial Basis Function Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lssvmRadial&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
<li><code>tau</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Linear Support Vector Machines with Class Weights</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinearWeights&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
<li><code>weight</code> (Class Weight)</li>
</ul>
<p>Required packages: <code>e1071</code></p>
<p><strong>Support Vector Machines with Boundrange String Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmBoundrangeString&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>length</code> (length)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Class Weights</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmRadialWeights&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
<li><code>C</code> (Cost)</li>
<li><code>Weight</code> (Weight)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Exponential String Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmExpoString&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (lambda)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Linear Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinear&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Linear Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinear2&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
</ul>
<p>Required packages: <code>e1071</code></p>
<p><strong>Support Vector Machines with Polynomial Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmPoly&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Polynomial Degree)</li>
<li><code>scale</code> (Scale)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Radial Basis Function Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmRadial&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Radial Basis Function Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmRadialCost&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Radial Basis Function Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmRadialSigma&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p>Notes: This SVM model tunes over the cost parameter and the RBF kernel parameter sigma. In the latter case, using <code>tuneLength</code> will, at most, evaluate six values of the kernel parameter. This enables a broad search over the cost parameter and a relatively narrow search over <code>sigma</code></p>
<p><strong>Support Vector Machines with Spectrum String Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmSpectrumString&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>length</code> (length)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<div id="Supports_Class_Probabilities">

</div>
</div>
<div id="supports-class-probabilities" class="section level3">
<h3><span class="header-section-number">7.0.48</span> Supports Class Probabilities</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>AdaBoost Classification Trees</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;adaboost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nIter</code> (#Trees)</li>
<li><code>method</code> (Method)</li>
</ul>
<p>Required packages: <code>fastAdaboost</code></p>
<p><strong>AdaBoost.M1</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;AdaBoost.M1&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mfinal</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
<li><code>coeflearn</code> (Coefficient Type)</li>
</ul>
<p>Required packages: <code>adabag</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Adaptive Mixture Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;amdai&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>model</code> (Model Type)</li>
</ul>
<p>Required packages: <code>adaptDA</code></p>
<p><strong>Adjacent Categories Probability Model for Ordinal Data</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;vglmAdjCat&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>parallel</code> (Parallel Curves)</li>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>VGAM</code></p>
<p><strong>Bagged AdaBoost</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;AdaBag&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mfinal</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
</ul>
<p>Required packages: <code>adabag</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Bagged CART</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;treebag&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>ipred</code>, <code>plyr</code>, <code>e1071</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Bagged Flexible Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bagFDA&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Product Degree)</li>
<li><code>nprune</code> (#Terms)</li>
</ul>
<p>Required packages: <code>earth</code>, <code>mda</code></p>
<p>A model-specific variable importance metric is available. Notes: Unlike other packages used by <code>train</code>, the <code>earth</code> package is fully loaded when this model is used.</p>
<p><strong>Bagged Logic Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;logicBag&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nleaves</code> (Maximum Number of Leaves)</li>
<li><code>ntrees</code> (Number of Trees)</li>
</ul>
<p>Required packages: <code>logicFS</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>logicFS</code> package is fully loaded when this model is used.</p>
<p><strong>Bagged MARS</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bagEarth&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nprune</code> (#Terms)</li>
<li><code>degree</code> (Product Degree)</li>
</ul>
<p>Required packages: <code>earth</code></p>
<p>A model-specific variable importance metric is available. Notes: Unlike other packages used by <code>train</code>, the <code>earth</code> package is fully loaded when this model is used.</p>
<p><strong>Bagged MARS using gCV Pruning</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bagEarthGCV&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Product Degree)</li>
</ul>
<p>Required packages: <code>earth</code></p>
<p>A model-specific variable importance metric is available. Notes: Unlike other packages used by <code>train</code>, the <code>earth</code> package is fully loaded when this model is used.</p>
<p><strong>Bagged Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bag&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>vars</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>caret</code></p>
<p><strong>Bayesian Additive Regression Trees</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bartMachine&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>num_trees</code> (#Trees)</li>
<li><code>k</code> (Prior Boundary)</li>
<li><code>alpha</code> (Base Terminal Node Hyperparameter)</li>
<li><code>beta</code> (Power Terminal Node Hyperparameter)</li>
<li><code>nu</code> (Degrees of Freedom)</li>
</ul>
<p>Required packages: <code>bartMachine</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Bayesian Generalized Linear Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bayesglm&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>arm</code></p>
<p><strong>Binary Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;binda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda.freqs</code> (Shrinkage Intensity)</li>
</ul>
<p>Required packages: <code>binda</code></p>
<p><strong>Boosted Classification Trees</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ada&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>iter</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
<li><code>nu</code> (Learning Rate)</li>
</ul>
<p>Required packages: <code>ada</code>, <code>plyr</code></p>
<p><strong>Boosted Generalized Additive Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gamboost&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>prune</code> (AIC Prune?)</li>
</ul>
<p>Required packages: <code>mboost</code>, <code>plyr</code>, <code>import</code></p>
<p>Notes: The <code>prune</code> option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in <code>?mboost::mstop</code>. If pruning is not used, the ensemble makes predictions using the exact value of the <code>mstop</code> tuning parameter value.</p>
<p><strong>Boosted Generalized Linear Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmboost&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>prune</code> (AIC Prune?)</li>
</ul>
<p>Required packages: <code>plyr</code>, <code>mboost</code></p>
<p>A model-specific variable importance metric is available. Notes: The <code>prune</code> option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in <code>?mboost::mstop</code>. If pruning is not used, the ensemble makes predictions using the exact value of the <code>mstop</code> tuning parameter value.</p>
<p><strong>Boosted Logistic Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;LogitBoost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nIter</code> (# Boosting Iterations)</li>
</ul>
<p>Required packages: <code>caTools</code></p>
<p><strong>Boosted Tree</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;blackboost&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
</ul>
<p>Required packages: <code>party</code>, <code>mboost</code>, <code>plyr</code>, <code>partykit</code></p>
<p><strong>C4.5-like Trees</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;J48&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>C</code> (Confidence Threshold)</li>
<li><code>M</code> (Minimum Instances Per Leaf)</li>
</ul>
<p>Required packages: <code>RWeka</code></p>
<p><strong>C5.0</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>trials</code> (# Boosting Iterations)</li>
<li><code>model</code> (Model Type)</li>
<li><code>winnow</code> (Winnow)</li>
</ul>
<p>Required packages: <code>C50</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>CART</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpart&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cp</code> (Complexity Parameter)</li>
</ul>
<p>Required packages: <code>rpart</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>CART</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpart1SE&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>rpart</code></p>
<p>A model-specific variable importance metric is available. Notes: This CART model replicates the same process used by the <code>rpart</code> function where the model complexity is determined using the one-standard error method. This procedure is replicated inside of the resampling done by <code>train</code> so that an external resampling estimate can be obtained.</p>
<p><strong>CART</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpart2&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxdepth</code> (Max Tree Depth)</li>
</ul>
<p>Required packages: <code>rpart</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>CHi-squared Automated Interaction Detection</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;chaid&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha2</code> (Merging Threshold)</li>
<li><code>alpha3</code> (Splitting former Merged Threshold)</li>
<li><code>alpha4</code> (
Splitting former Merged Threshold)</li>
</ul>
<p>Required packages: <code>CHAID</code></p>
<p><strong>Conditional Inference Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;cforest&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>party</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Conditional Inference Tree</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ctree&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mincriterion</code> (1 - P-Value Threshold)</li>
</ul>
<p>Required packages: <code>party</code></p>
<p><strong>Conditional Inference Tree</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ctree2&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxdepth</code> (Max Tree Depth)</li>
<li><code>mincriterion</code> (1 - P-Value Threshold)</li>
</ul>
<p>Required packages: <code>party</code></p>
<p><strong>Continuation Ratio Model for Ordinal Data</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;vglmContRatio&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>parallel</code> (Parallel Curves)</li>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>VGAM</code></p>
<p><strong>Cumulative Probability Model for Ordinal Data</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;vglmCumulative&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>parallel</code> (Parallel Curves)</li>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>VGAM</code></p>
<p><strong>Diagonal Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>model</code> (Model)</li>
<li><code>shrinkage</code> (Shrinkage Type)</li>
</ul>
<p>Required packages: <code>sparsediscrim</code></p>
<p><strong>Distance Weighted Discrimination with Polynomial Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dwdPoly&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>qval</code> (q)</li>
<li><code>degree</code> (Polynomial Degree)</li>
<li><code>scale</code> (Scale)</li>
</ul>
<p>Required packages: <code>kerndwd</code></p>
<p><strong>Distance Weighted Discrimination with Radial Basis Function Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dwdRadial&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>qval</code> (q)</li>
<li><code>sigma</code> (Sigma)</li>
</ul>
<p>Required packages: <code>kernlab</code>, <code>kerndwd</code></p>
<p><strong>Ensembles of Generalized Linear Models</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;randomGLM&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxInteractionOrder</code> (Interaction Order)</li>
</ul>
<p>Required packages: <code>randomGLM</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>randomGLM</code> package is fully loaded when this model is used.</p>
<p><strong>eXtreme Gradient Boosting</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;xgbDART&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nrounds</code> (# Boosting Iterations)</li>
<li><code>max_depth</code> (Max Tree Depth)</li>
<li><code>eta</code> (Shrinkage)</li>
<li><code>gamma</code> (Minimum Loss Reduction)</li>
<li><code>subsample</code> (Subsample Percentage)</li>
<li><code>colsample_bytree</code> (Subsample Ratio of Columns)</li>
<li><code>rate_drop</code> (Fraction of Trees Dropped)</li>
<li><code>skip_drop</code> (Prob. of Skipping Drop-out)</li>
<li><code>min_child_weight</code> (Minimum Sum of Instance Weight)</li>
</ul>
<p>Required packages: <code>xgboost</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>eXtreme Gradient Boosting</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;xgbLinear&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nrounds</code> (# Boosting Iterations)</li>
<li><code>lambda</code> (L2 Regularization)</li>
<li><code>alpha</code> (L1 Regularization)</li>
<li><code>eta</code> (Learning Rate)</li>
</ul>
<p>Required packages: <code>xgboost</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>eXtreme Gradient Boosting</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;xgbTree&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nrounds</code> (# Boosting Iterations)</li>
<li><code>max_depth</code> (Max Tree Depth)</li>
<li><code>eta</code> (Shrinkage)</li>
<li><code>gamma</code> (Minimum Loss Reduction)</li>
<li><code>colsample_bytree</code> (Subsample Ratio of Columns)</li>
<li><code>min_child_weight</code> (Minimum Sum of Instance Weight)</li>
<li><code>subsample</code> (Subsample Percentage)</li>
</ul>
<p>Required packages: <code>xgboost</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Flexible Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;fda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Product Degree)</li>
<li><code>nprune</code> (#Terms)</li>
</ul>
<p>Required packages: <code>earth</code>, <code>mda</code></p>
<p>A model-specific variable importance metric is available. Notes: Unlike other packages used by <code>train</code>, the <code>earth</code> package is fully loaded when this model is used.</p>
<p><strong>Gaussian Process</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gaussprLinear&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Gaussian Process with Polynomial Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gaussprPoly&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Polynomial Degree)</li>
<li><code>scale</code> (Scale)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Gaussian Process with Radial Basis Function Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gaussprRadial&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Generalized Additive Model using LOESS</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gamLoess&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>span</code> (Span)</li>
<li><code>degree</code> (Degree)</li>
</ul>
<p>Required packages: <code>gam</code></p>
<p>A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by <code>train</code>, the <code>gam</code> package is fully loaded when this model is used.</p>
<p><strong>Generalized Additive Model using Splines</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bam&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>select</code> (Feature Selection)</li>
<li><code>method</code> (Method)</li>
</ul>
<p>Required packages: <code>mgcv</code></p>
<p>A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by <code>train</code>, the <code>mgcv</code> package is fully loaded when this model is used.</p>
<p><strong>Generalized Additive Model using Splines</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gam&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>select</code> (Feature Selection)</li>
<li><code>method</code> (Method)</li>
</ul>
<p>Required packages: <code>mgcv</code></p>
<p>A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by <code>train</code>, the <code>mgcv</code> package is fully loaded when this model is used.</p>
<p><strong>Generalized Additive Model using Splines</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gamSpline&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>df</code> (Degrees of Freedom)</li>
</ul>
<p>Required packages: <code>gam</code></p>
<p>A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Unlike other packages used by <code>train</code>, the <code>gam</code> package is fully loaded when this model is used.</p>
<p><strong>Generalized Linear Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glm&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Generalized Linear Model with Stepwise Feature Selection</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmStepAIC&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>MASS</code></p>
<p><strong>Generalized Partial Least Squares</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gpls&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K.prov</code> (#Components)</li>
</ul>
<p>Required packages: <code>gpls</code></p>
<p><strong>glmnet</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmnet_h2o&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Mixing Percentage)</li>
<li><code>lambda</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>h2o</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>glmnet</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmnet&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Mixing Percentage)</li>
<li><code>lambda</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>glmnet</code>, <code>Matrix</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Gradient Boosting Machines</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gbm_h2o&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ntrees</code> (# Boosting Iterations)</li>
<li><code>max_depth</code> (Max Tree Depth)</li>
<li><code>min_rows</code> (Min. Terminal Node Size)</li>
<li><code>learn_rate</code> (Shrinkage)</li>
<li><code>col_sample_rate</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>h2o</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Heteroscedastic Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;hda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>gamma</code> (Gamma)</li>
<li><code>lambda</code> (Lambda)</li>
<li><code>newdim</code> (Dimension of the Discriminative Subspace)</li>
</ul>
<p>Required packages: <code>hda</code></p>
<p><strong>High Dimensional Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;hdda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>threshold</code> (Threshold)</li>
<li><code>model</code> (Model Type)</li>
</ul>
<p>Required packages: <code>HDclassif</code></p>
<p><strong>High-Dimensional Regularized Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;hdrda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>gamma</code> (Gamma)</li>
<li><code>lambda</code> (Lambda)</li>
<li><code>shrinkage_type</code> (Shrinkage Type)</li>
</ul>
<p>Required packages: <code>sparsediscrim</code></p>
<p><strong>k-Nearest Neighbors</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;kknn&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>kmax</code> (Max. #Neighbors)</li>
<li><code>distance</code> (Distance)</li>
<li><code>kernel</code> (Kernel)</li>
</ul>
<p>Required packages: <code>kknn</code></p>
<p><strong>k-Nearest Neighbors</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;knn&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>k</code> (#Neighbors)</li>
</ul>
<p><strong>Linear Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>MASS</code></p>
<p><strong>Linear Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;lda2&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>dimen</code> (#Discriminant Functions)</li>
</ul>
<p>Required packages: <code>MASS</code></p>
<p><strong>Linear Discriminant Analysis with Stepwise Feature Selection</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;stepLDA&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxvar</code> (Maximum #Variables)</li>
<li><code>direction</code> (Search Direction)</li>
</ul>
<p>Required packages: <code>klaR</code>, <code>MASS</code></p>
<p><strong>Linear Distance Weighted Discrimination</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dwdLinear&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>qval</code> (q)</li>
</ul>
<p>Required packages: <code>kerndwd</code></p>
<p><strong>Linear Support Vector Machines with Class Weights</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinearWeights&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
<li><code>weight</code> (Class Weight)</li>
</ul>
<p>Required packages: <code>e1071</code></p>
<p><strong>Localized Linear Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;loclda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>k</code> (#Nearest Neighbors)</li>
</ul>
<p>Required packages: <code>klaR</code></p>
<p><strong>Logic Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;logreg&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>treesize</code> (Maximum Number of Leaves)</li>
<li><code>ntrees</code> (Number of Trees)</li>
</ul>
<p>Required packages: <code>LogicReg</code></p>
<p><strong>Logistic Model Trees</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;LMT&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>iter</code> (# Iteratons)</li>
</ul>
<p>Required packages: <code>RWeka</code></p>
<p><strong>Mixture Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>subclasses</code> (#Subclasses Per Class)</li>
</ul>
<p>Required packages: <code>mda</code></p>
<p><strong>Model Averaged Naive Bayes Classifier</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;manb&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>smooth</code> (Smoothing Parameter)</li>
<li><code>prior</code> (Prior Probability)</li>
</ul>
<p>Required packages: <code>bnclassify</code></p>
<p><strong>Model Averaged Neural Network</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;avNNet&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>decay</code> (Weight Decay)</li>
<li><code>bag</code> (Bagging)</li>
</ul>
<p>Required packages: <code>nnet</code></p>
<p><strong>Monotone Multi-Layer Perceptron Neural Network</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;monmlp&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>hidden1</code> (#Hidden Units)</li>
<li><code>n.ensemble</code> (#Models)</li>
</ul>
<p>Required packages: <code>monmlp</code></p>
<p><strong>Multi-Layer Perceptron</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mlp&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
</ul>
<p>Required packages: <code>RSNNS</code></p>
<p><strong>Multi-Layer Perceptron</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mlpWeightDecay&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>decay</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>RSNNS</code></p>
<p><strong>Multi-Layer Perceptron, multiple layers</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mlpWeightDecayML&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>layer1</code> (#Hidden Units layer1)</li>
<li><code>layer2</code> (#Hidden Units layer2)</li>
<li><code>layer3</code> (#Hidden Units layer3)</li>
<li><code>decay</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>RSNNS</code></p>
<p><strong>Multi-Layer Perceptron, with multiple layers</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mlpML&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>layer1</code> (#Hidden Units layer1)</li>
<li><code>layer2</code> (#Hidden Units layer2)</li>
<li><code>layer3</code> (#Hidden Units layer3)</li>
</ul>
<p>Required packages: <code>RSNNS</code></p>
<p><strong>Multi-Step Adaptive MCP-Net</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;msaenet&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alphas</code> (Alpha)</li>
<li><code>nsteps</code> (#Adaptive Estimation Steps)</li>
<li><code>scale</code> (Adaptive Weight Scaling Factor)</li>
</ul>
<p>Required packages: <code>msaenet</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Multilayer Perceptron Network by Stochastic Gradient Descent</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mlpSGD&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>l2reg</code> (L2 Regularization)</li>
<li><code>lambda</code> (RMSE Gradient Scaling)</li>
<li><code>learn_rate</code> (Learning Rate)</li>
<li><code>momentum</code> (Momentum)</li>
<li><code>gamma</code> (Learning Rate Decay)</li>
<li><code>minibatchsz</code> (Batch Size)</li>
<li><code>repeats</code> (#Models)</li>
</ul>
<p>Required packages: <code>FCNN4R</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Multilayer Perceptron Network with Dropout</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mlpKerasDropout&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>dropout</code> (Dropout Rate)</li>
<li><code>batch_size</code> (Batch Size)</li>
<li><code>lr</code> (Learning Rate)</li>
<li><code>rho</code> (Rho)</li>
<li><code>decay</code> (Learning Rate Decay)</li>
<li><code>activation</code> (Activation Function)</li>
</ul>
<p>Required packages: <code>keras</code></p>
<p>Notes: After <code>train</code> completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use <code>keras::unsearlize_model(object$finalModel$object)</code> in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Unlike other packages used by <code>train</code>, the <code>dplyr</code> package is fully loaded when this model is used.</p>
<p><strong>Multilayer Perceptron Network with Dropout</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mlpKerasDropoutCost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>dropout</code> (Dropout Rate)</li>
<li><code>batch_size</code> (Batch Size)</li>
<li><code>lr</code> (Learning Rate)</li>
<li><code>rho</code> (Rho)</li>
<li><code>decay</code> (Learning Rate Decay)</li>
<li><code>cost</code> (Cost)</li>
<li><code>activation</code> (Activation Function)</li>
</ul>
<p>Required packages: <code>keras</code></p>
<p>Notes: After <code>train</code> completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use <code>keras::unsearlize_model(object$finalModel$object)</code> in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Finally, the cost parameter weights the first class in the outcome vector. Unlike other packages used by <code>train</code>, the <code>dplyr</code> package is fully loaded when this model is used.</p>
<p><strong>Multilayer Perceptron Network with Weight Decay</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mlpKerasDecay&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>lambda</code> (L2 Regularization)</li>
<li><code>batch_size</code> (Batch Size)</li>
<li><code>lr</code> (Learning Rate)</li>
<li><code>rho</code> (Rho)</li>
<li><code>decay</code> (Learning Rate Decay)</li>
<li><code>activation</code> (Activation Function)</li>
</ul>
<p>Required packages: <code>keras</code></p>
<p>Notes: After <code>train</code> completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use <code>keras::unsearlize_model(object$finalModel$object)</code> in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Unlike other packages used by <code>train</code>, the <code>dplyr</code> package is fully loaded when this model is used.</p>
<p><strong>Multilayer Perceptron Network with Weight Decay</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mlpKerasDecayCost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>lambda</code> (L2 Regularization)</li>
<li><code>batch_size</code> (Batch Size)</li>
<li><code>lr</code> (Learning Rate)</li>
<li><code>rho</code> (Rho)</li>
<li><code>decay</code> (Learning Rate Decay)</li>
<li><code>cost</code> (Cost)</li>
<li><code>activation</code> (Activation Function)</li>
</ul>
<p>Required packages: <code>keras</code></p>
<p>Notes: After <code>train</code> completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use <code>keras::unsearlize_model(object$finalModel$object)</code> in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Finally, the cost parameter weights the first class in the outcome vector. Unlike other packages used by <code>train</code>, the <code>dplyr</code> package is fully loaded when this model is used.</p>
<p><strong>Multivariate Adaptive Regression Spline</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;earth&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nprune</code> (#Terms)</li>
<li><code>degree</code> (Product Degree)</li>
</ul>
<p>Required packages: <code>earth</code></p>
<p>A model-specific variable importance metric is available. Notes: Unlike other packages used by <code>train</code>, the <code>earth</code> package is fully loaded when this model is used.</p>
<p><strong>Multivariate Adaptive Regression Splines</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gcvEarth&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Product Degree)</li>
</ul>
<p>Required packages: <code>earth</code></p>
<p>A model-specific variable importance metric is available. Notes: Unlike other packages used by <code>train</code>, the <code>earth</code> package is fully loaded when this model is used.</p>
<p><strong>Naive Bayes</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;naive_bayes&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>laplace</code> (Laplace Correction)</li>
<li><code>usekernel</code> (Distribution Type)</li>
<li><code>adjust</code> (Bandwidth Adjustment)</li>
</ul>
<p>Required packages: <code>naivebayes</code></p>
<p><strong>Naive Bayes</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;nb&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>fL</code> (Laplace Correction)</li>
<li><code>usekernel</code> (Distribution Type)</li>
<li><code>adjust</code> (Bandwidth Adjustment)</li>
</ul>
<p>Required packages: <code>klaR</code></p>
<p><strong>Naive Bayes Classifier</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;nbDiscrete&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>smooth</code> (Smoothing Parameter)</li>
</ul>
<p>Required packages: <code>bnclassify</code></p>
<p><strong>Naive Bayes Classifier with Attribute Weighting</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;awnb&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>smooth</code> (Smoothing Parameter)</li>
</ul>
<p>Required packages: <code>bnclassify</code></p>
<p><strong>Nearest Shrunken Centroids</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pam&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>threshold</code> (Shrinkage Threshold)</li>
</ul>
<p>Required packages: <code>pamr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Neural Network</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mxnet&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>layer1</code> (#Hidden Units in Layer 1)</li>
<li><code>layer2</code> (#Hidden Units in Layer 2)</li>
<li><code>layer3</code> (#Hidden Units in Layer 3)</li>
<li><code>learning.rate</code> (Learning Rate)</li>
<li><code>momentum</code> (Momentum)</li>
<li><code>dropout</code> (Dropout Rate)</li>
<li><code>activation</code> (Activation Function)</li>
</ul>
<p>Required packages: <code>mxnet</code></p>
<p>Notes: The <code>mxnet</code> package is not yet on CRAN. See <a href="http://mxnet.io" class="uri">http://mxnet.io</a> for installation instructions.</p>
<p><strong>Neural Network</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mxnetAdam&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>layer1</code> (#Hidden Units in Layer 1)</li>
<li><code>layer2</code> (#Hidden Units in Layer 2)</li>
<li><code>layer3</code> (#Hidden Units in Layer 3)</li>
<li><code>dropout</code> (Dropout Rate)</li>
<li><code>beta1</code> (beta1)</li>
<li><code>beta2</code> (beta2)</li>
<li><code>learningrate</code> (Learning Rate)</li>
<li><code>activation</code> (Activation Function)</li>
</ul>
<p>Required packages: <code>mxnet</code></p>
<p>Notes: The <code>mxnet</code> package is not yet on CRAN. See <a href="http://mxnet.io" class="uri">http://mxnet.io</a> for installation instructions. Users are strongly advised to define <code>num.round</code> themselves.</p>
<p><strong>Neural Network</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;nnet&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>decay</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>nnet</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Neural Networks with Feature Extraction</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pcaNNet&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>decay</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>nnet</code></p>
<p><strong>Non-Informative Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;null&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>No tuning parameters for this model</p>
<p>Notes: Since this model always predicts the same value, R-squared values will always be estimated to be NA.</p>
<p><strong>Oblique Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFlog&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>obliqueRF</code> package is fully loaded when this model is used.</p>
<p><strong>Oblique Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFpls&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>obliqueRF</code> package is fully loaded when this model is used.</p>
<p><strong>Oblique Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFridge&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>obliqueRF</code> package is fully loaded when this model is used.</p>
<p><strong>Oblique Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFsvm&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>obliqueRF</code> package is fully loaded when this model is used.</p>
<p><strong>Ordered Logistic or Probit Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;polr&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>method</code> (parameter)</li>
</ul>
<p>Required packages: <code>MASS</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Parallel Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;parRF&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>e1071</code>, <code>randomForest</code>, <code>foreach</code>, <code>import</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Partial Least Squares</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;kernelpls&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ncomp</code> (#Components)</li>
</ul>
<p>Required packages: <code>pls</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Partial Least Squares</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pls&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ncomp</code> (#Components)</li>
</ul>
<p>Required packages: <code>pls</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Partial Least Squares</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;simpls&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ncomp</code> (#Components)</li>
</ul>
<p>Required packages: <code>pls</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Partial Least Squares</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;widekernelpls&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ncomp</code> (#Components)</li>
</ul>
<p>Required packages: <code>pls</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Partial Least Squares Generalized Linear Models </strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;plsRglm&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nt</code> (#PLS Components)</li>
<li><code>alpha.pvals.expli</code> (p-Value threshold)</li>
</ul>
<p>Required packages: <code>plsRglm</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>plsRglm</code> package is fully loaded when this model is used.</p>
<p><strong>Patient Rule Induction Method</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;PRIM&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>peel.alpha</code> (peeling quantile)</li>
<li><code>paste.alpha</code> (pasting quantile)</li>
<li><code>mass.min</code> (minimum mass)</li>
</ul>
<p>Required packages: <code>supervisedPRIM</code></p>
<p><strong>Penalized Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Shrinkage Penalty Coefficient)</li>
</ul>
<p>Required packages: <code>mda</code></p>
<p><strong>Penalized Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;pda2&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>df</code> (Degrees of Freedom)</li>
</ul>
<p>Required packages: <code>mda</code></p>
<p><strong>Penalized Logistic Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;plr&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (L2 Penalty)</li>
<li><code>cp</code> (Complexity Parameter)</li>
</ul>
<p>Required packages: <code>stepPlr</code></p>
<p><strong>Penalized Multinomial Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;multinom&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>decay</code> (Weight Decay)</li>
</ul>
<p>Required packages: <code>nnet</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Penalized Ordinal Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ordinalNet&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Mixing Percentage)</li>
<li><code>criteria</code> (Selection Criterion)</li>
<li><code>link</code> (Link Function)</li>
</ul>
<p>Required packages: <code>ordinalNet</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available. Notes: Requires ordinalNet package version &gt;= 2.0</p>
<p><strong>Quadratic Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;qda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>MASS</code></p>
<p><strong>Quadratic Discriminant Analysis with Stepwise Feature Selection</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;stepQDA&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxvar</code> (Maximum #Variables)</li>
<li><code>direction</code> (Search Direction)</li>
</ul>
<p>Required packages: <code>klaR</code>, <code>MASS</code></p>
<p><strong>Radial Basis Function Network</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rbf&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
</ul>
<p>Required packages: <code>RSNNS</code></p>
<p><strong>Radial Basis Function Network</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rbfDDA&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>negativeThreshold</code> (Activation Limit for Conflicting Classes)</li>
</ul>
<p>Required packages: <code>RSNNS</code></p>
<p><strong>Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ordinalRF&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nsets</code> (# score sets tried prior to the approximation)</li>
<li><code>ntreeperdiv</code> (# of trees (small RFs))</li>
<li><code>ntreefinal</code> (# of trees (final RF))</li>
</ul>
<p>Required packages: <code>e1071</code>, <code>ranger</code>, <code>dplyr</code>, <code>ordinalForest</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ranger&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
<li><code>splitrule</code> (Splitting Rule)</li>
<li><code>min.node.size</code> (Minimal Node Size)</li>
</ul>
<p>Required packages: <code>e1071</code>, <code>ranger</code>, <code>dplyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;Rborist&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>predFixed</code> (#Randomly Selected Predictors)</li>
<li><code>minNode</code> (Minimal Node Size)</li>
</ul>
<p>Required packages: <code>Rborist</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rf&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>randomForest</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Random Forest by Randomization</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;extraTrees&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (# Randomly Selected Predictors)</li>
<li><code>numRandomCuts</code> (# Random Cuts)</li>
</ul>
<p>Required packages: <code>extraTrees</code></p>
<p><strong>Regularized Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>gamma</code> (Gamma)</li>
<li><code>lambda</code> (Lambda)</li>
</ul>
<p>Required packages: <code>klaR</code></p>
<p><strong>Regularized Linear Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rlda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>estimator</code> (Regularization Method)</li>
</ul>
<p>Required packages: <code>sparsediscrim</code></p>
<p><strong>Regularized Logistic Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;regLogistic&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
<li><code>loss</code> (Loss Function)</li>
<li><code>epsilon</code> (Tolerance)</li>
</ul>
<p>Required packages: <code>LiblineaR</code></p>
<p><strong>Regularized Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;RRF&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
<li><code>coefReg</code> (Regularization Value)</li>
<li><code>coefImp</code> (Importance Coefficient)</li>
</ul>
<p>Required packages: <code>randomForest</code>, <code>RRF</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Regularized Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;RRFglobal&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
<li><code>coefReg</code> (Regularization Value)</li>
</ul>
<p>Required packages: <code>RRF</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Robust Linear Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;Linda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>rrcov</code></p>
<p><strong>Robust Mixture Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rmda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K</code> (#Subclasses Per Class)</li>
<li><code>model</code> (Model)</li>
</ul>
<p>Required packages: <code>robustDA</code></p>
<p><strong>Robust Quadratic Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;QdaCov&#39;</span></code></pre>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>rrcov</code></p>
<p><strong>Robust Regularized Linear Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rrlda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Penalty Parameter)</li>
<li><code>hp</code> (Robustness Parameter)</li>
<li><code>penalty</code> (Penalty Type)</li>
</ul>
<p>Required packages: <code>rrlda</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>rrlda</code> package is fully loaded when this model is used.</p>
<p><strong>Rotation Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rotationForest&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K</code> (#Variable Subsets)</li>
<li><code>L</code> (Ensemble Size)</li>
</ul>
<p>Required packages: <code>rotationForest</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Rotation Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rotationForestCp&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K</code> (#Variable Subsets)</li>
<li><code>L</code> (Ensemble Size)</li>
<li><code>cp</code> (Complexity Parameter)</li>
</ul>
<p>Required packages: <code>rpart</code>, <code>plyr</code>, <code>rotationForest</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Rule-Based Classifier</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;JRip&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>NumOpt</code> (# Optimizations)</li>
<li><code>NumFolds</code> (# Folds)</li>
<li><code>MinWeights</code> (Min Weights)</li>
</ul>
<p>Required packages: <code>RWeka</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Rule-Based Classifier</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;PART&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>threshold</code> (Confidence Threshold)</li>
<li><code>pruned</code> (Pruning)</li>
</ul>
<p>Required packages: <code>RWeka</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Self-Organizing Maps</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;xyf&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>xdim</code> (Rows)</li>
<li><code>ydim</code> (Columns)</li>
<li><code>user.weights</code> (Layer Weight)</li>
<li><code>topo</code> (Topology)</li>
</ul>
<p>Required packages: <code>kohonen</code></p>
<p>Notes: As of version 3.0.0 of the kohonen package, the argument <code>user.weights</code> replaces the old <code>alpha</code> parameter. <code>user.weights</code> is usually a vector of relative weights such as <code>c(1, 3)</code> but is parameterized here as a proportion such as <code>c(1-.75, .75)</code> where the .75 is the value of the tuning parameter passed to <code>train</code> and indicates that the outcome layer has 3 times the weight as the predictor layer.</p>
<p><strong>Semi-Naive Structure Learner Wrapper</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;nbSearch&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>k</code> (#Folds)</li>
<li><code>epsilon</code> (Minimum Absolute Improvement)</li>
<li><code>smooth</code> (Smoothing Parameter)</li>
<li><code>final_smooth</code> (Final Smoothing Parameter)</li>
<li><code>direction</code> (Search Direction)</li>
</ul>
<p>Required packages: <code>bnclassify</code></p>
<p><strong>Shrinkage Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;sda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>diagonal</code> (Diagonalize)</li>
<li><code>lambda</code> (shrinkage)</li>
</ul>
<p>Required packages: <code>sda</code></p>
<p><strong>Single C5.0 Ruleset</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0Rules&#39;</span></code></pre>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>C50</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Single C5.0 Tree</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0Tree&#39;</span></code></pre>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>C50</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Single Rule Classification</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;OneR&#39;</span></code></pre>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>RWeka</code></p>
<p><strong>Sparse Distance Weighted Discrimination</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;sdwd&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (L1 Penalty)</li>
<li><code>lambda2</code> (L2 Penalty)</li>
</ul>
<p>Required packages: <code>sdwd</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Sparse Linear Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;sparseLDA&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>NumVars</code> (# Predictors)</li>
<li><code>lambda</code> (Lambda)</li>
</ul>
<p>Required packages: <code>sparseLDA</code></p>
<p><strong>Sparse Partial Least Squares</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;spls&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K</code> (#Components)</li>
<li><code>eta</code> (Threshold)</li>
<li><code>kappa</code> (Kappa)</li>
</ul>
<p>Required packages: <code>spls</code></p>
<p><strong>Stabilized Linear Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;slda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>ipred</code></p>
<p><strong>Stacked AutoEncoder Deep Neural Network</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dnn&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>layer1</code> (Hidden Layer 1)</li>
<li><code>layer2</code> (Hidden Layer 2)</li>
<li><code>layer3</code> (Hidden Layer 3)</li>
<li><code>hidden_dropout</code> (Hidden Dropouts)</li>
<li><code>visible_dropout</code> (Visible Dropout)</li>
</ul>
<p>Required packages: <code>deepnet</code></p>
<p><strong>Stochastic Gradient Boosting</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gbm&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>n.trees</code> (# Boosting Iterations)</li>
<li><code>interaction.depth</code> (Max Tree Depth)</li>
<li><code>shrinkage</code> (Shrinkage)</li>
<li><code>n.minobsinnode</code> (Min. Terminal Node Size)</li>
</ul>
<p>Required packages: <code>gbm</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Support Vector Machines with Boundrange String Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmBoundrangeString&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>length</code> (length)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Class Weights</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmRadialWeights&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
<li><code>C</code> (Cost)</li>
<li><code>Weight</code> (Weight)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Exponential String Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmExpoString&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (lambda)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Linear Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinear&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Linear Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinear2&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
</ul>
<p>Required packages: <code>e1071</code></p>
<p><strong>Support Vector Machines with Polynomial Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmPoly&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>degree</code> (Polynomial Degree)</li>
<li><code>scale</code> (Scale)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Radial Basis Function Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmRadial&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Radial Basis Function Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmRadialCost&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Radial Basis Function Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmRadialSigma&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p>Notes: This SVM model tunes over the cost parameter and the RBF kernel parameter sigma. In the latter case, using <code>tuneLength</code> will, at most, evaluate six values of the kernel parameter. This enables a broad search over the cost parameter and a relatively narrow search over <code>sigma</code></p>
<p><strong>Support Vector Machines with Spectrum String Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmSpectrumString&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>length</code> (length)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Tree Augmented Naive Bayes Classifier</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;tan&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>score</code> (Score Function)</li>
<li><code>smooth</code> (Smoothing Parameter)</li>
</ul>
<p>Required packages: <code>bnclassify</code></p>
<p><strong>Tree Augmented Naive Bayes Classifier Structure Learner Wrapper</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;tanSearch&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>k</code> (#Folds)</li>
<li><code>epsilon</code> (Minimum Absolute Improvement)</li>
<li><code>smooth</code> (Smoothing Parameter)</li>
<li><code>final_smooth</code> (Final Smoothing Parameter)</li>
<li><code>sp</code> (Super-Parent)</li>
</ul>
<p>Required packages: <code>bnclassify</code></p>
<p><strong>Tree Augmented Naive Bayes Classifier with Attribute Weighting</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;awtan&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>score</code> (Score Function)</li>
<li><code>smooth</code> (Smoothing Parameter)</li>
</ul>
<p>Required packages: <code>bnclassify</code></p>
<p><strong>Tree Models from Genetic Algorithms</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;evtree&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Complexity Parameter)</li>
</ul>
<p>Required packages: <code>evtree</code></p>
<p><strong>Tree-Based Ensembles</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;nodeHarvest&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxinter</code> (Maximum Interaction Depth)</li>
<li><code>mode</code> (Prediction Mode)</li>
</ul>
<p>Required packages: <code>nodeHarvest</code></p>
<p><strong>Variational Bayesian Multinomial Probit Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;vbmpRadial&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>estimateTheta</code> (Theta Estimated)</li>
</ul>
<p>Required packages: <code>vbmp</code></p>
<p><strong>Weighted Subspace Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;wsrf&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>wsrf</code></p>
<div id="Text_Mining">

</div>
</div>
<div id="text-mining" class="section level3">
<h3><span class="header-section-number">7.0.49</span> Text Mining</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>Support Vector Machines with Boundrange String Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmBoundrangeString&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>length</code> (length)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Exponential String Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmExpoString&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (lambda)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Support Vector Machines with Spectrum String Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmSpectrumString&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>length</code> (length)</li>
<li><code>C</code> (Cost)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<div id="Tree_Based_Model">

</div>
</div>
<div id="tree-based-model" class="section level3">
<h3><span class="header-section-number">7.0.50</span> Tree-Based Model</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>AdaBoost Classification Trees</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;adaboost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nIter</code> (#Trees)</li>
<li><code>method</code> (Method)</li>
</ul>
<p>Required packages: <code>fastAdaboost</code></p>
<p><strong>AdaBoost.M1</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;AdaBoost.M1&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mfinal</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
<li><code>coeflearn</code> (Coefficient Type)</li>
</ul>
<p>Required packages: <code>adabag</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Bagged AdaBoost</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;AdaBag&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mfinal</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
</ul>
<p>Required packages: <code>adabag</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Bagged CART</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;treebag&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>ipred</code>, <code>plyr</code>, <code>e1071</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Bayesian Additive Regression Trees</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bartMachine&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>num_trees</code> (#Trees)</li>
<li><code>k</code> (Prior Boundary)</li>
<li><code>alpha</code> (Base Terminal Node Hyperparameter)</li>
<li><code>beta</code> (Power Terminal Node Hyperparameter)</li>
<li><code>nu</code> (Degrees of Freedom)</li>
</ul>
<p>Required packages: <code>bartMachine</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Boosted Classification Trees</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ada&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>iter</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
<li><code>nu</code> (Learning Rate)</li>
</ul>
<p>Required packages: <code>ada</code>, <code>plyr</code></p>
<p><strong>Boosted Logistic Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;LogitBoost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nIter</code> (# Boosting Iterations)</li>
</ul>
<p>Required packages: <code>caTools</code></p>
<p><strong>Boosted Tree</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;blackboost&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
</ul>
<p>Required packages: <code>party</code>, <code>mboost</code>, <code>plyr</code>, <code>partykit</code></p>
<p><strong>Boosted Tree</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bstTree&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
<li><code>nu</code> (Shrinkage)</li>
</ul>
<p>Required packages: <code>bst</code>, <code>plyr</code></p>
<p><strong>C4.5-like Trees</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;J48&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>C</code> (Confidence Threshold)</li>
<li><code>M</code> (Minimum Instances Per Leaf)</li>
</ul>
<p>Required packages: <code>RWeka</code></p>
<p><strong>C5.0</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>trials</code> (# Boosting Iterations)</li>
<li><code>model</code> (Model Type)</li>
<li><code>winnow</code> (Winnow)</li>
</ul>
<p>Required packages: <code>C50</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>CART</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpart&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cp</code> (Complexity Parameter)</li>
</ul>
<p>Required packages: <code>rpart</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>CART</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpart1SE&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>rpart</code></p>
<p>A model-specific variable importance metric is available. Notes: This CART model replicates the same process used by the <code>rpart</code> function where the model complexity is determined using the one-standard error method. This procedure is replicated inside of the resampling done by <code>train</code> so that an external resampling estimate can be obtained.</p>
<p><strong>CART</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpart2&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxdepth</code> (Max Tree Depth)</li>
</ul>
<p>Required packages: <code>rpart</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>CART or Ordinal Responses</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpartScore&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cp</code> (Complexity Parameter)</li>
<li><code>split</code> (Split Function)</li>
<li><code>prune</code> (Pruning Measure)</li>
</ul>
<p>Required packages: <code>rpartScore</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>CHi-squared Automated Interaction Detection</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;chaid&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha2</code> (Merging Threshold)</li>
<li><code>alpha3</code> (Splitting former Merged Threshold)</li>
<li><code>alpha4</code> (
Splitting former Merged Threshold)</li>
</ul>
<p>Required packages: <code>CHAID</code></p>
<p><strong>Conditional Inference Tree</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ctree&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mincriterion</code> (1 - P-Value Threshold)</li>
</ul>
<p>Required packages: <code>party</code></p>
<p><strong>Conditional Inference Tree</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ctree2&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxdepth</code> (Max Tree Depth)</li>
<li><code>mincriterion</code> (1 - P-Value Threshold)</li>
</ul>
<p>Required packages: <code>party</code></p>
<p><strong>Cost-Sensitive C5.0</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0Cost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>trials</code> (# Boosting Iterations)</li>
<li><code>model</code> (Model Type)</li>
<li><code>winnow</code> (Winnow)</li>
<li><code>cost</code> (Cost)</li>
</ul>
<p>Required packages: <code>C50</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Cost-Sensitive CART</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpartCost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cp</code> (Complexity Parameter)</li>
<li><code>Cost</code> (Cost)</li>
</ul>
<p>Required packages: <code>rpart</code>, <code>plyr</code></p>
<p><strong>DeepBoost</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;deepboost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>num_iter</code> (# Boosting Iterations)</li>
<li><code>tree_depth</code> (Tree Depth)</li>
<li><code>beta</code> (L1 Regularization)</li>
<li><code>lambda</code> (Tree Depth Regularization)</li>
<li><code>loss_type</code> (Loss)</li>
</ul>
<p>Required packages: <code>deepboost</code></p>
<p><strong>eXtreme Gradient Boosting</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;xgbDART&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nrounds</code> (# Boosting Iterations)</li>
<li><code>max_depth</code> (Max Tree Depth)</li>
<li><code>eta</code> (Shrinkage)</li>
<li><code>gamma</code> (Minimum Loss Reduction)</li>
<li><code>subsample</code> (Subsample Percentage)</li>
<li><code>colsample_bytree</code> (Subsample Ratio of Columns)</li>
<li><code>rate_drop</code> (Fraction of Trees Dropped)</li>
<li><code>skip_drop</code> (Prob. of Skipping Drop-out)</li>
<li><code>min_child_weight</code> (Minimum Sum of Instance Weight)</li>
</ul>
<p>Required packages: <code>xgboost</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>eXtreme Gradient Boosting</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;xgbTree&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nrounds</code> (# Boosting Iterations)</li>
<li><code>max_depth</code> (Max Tree Depth)</li>
<li><code>eta</code> (Shrinkage)</li>
<li><code>gamma</code> (Minimum Loss Reduction)</li>
<li><code>colsample_bytree</code> (Subsample Ratio of Columns)</li>
<li><code>min_child_weight</code> (Minimum Sum of Instance Weight)</li>
<li><code>subsample</code> (Subsample Percentage)</li>
</ul>
<p>Required packages: <code>xgboost</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Gradient Boosting Machines</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gbm_h2o&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>ntrees</code> (# Boosting Iterations)</li>
<li><code>max_depth</code> (Max Tree Depth)</li>
<li><code>min_rows</code> (Min. Terminal Node Size)</li>
<li><code>learn_rate</code> (Shrinkage)</li>
<li><code>col_sample_rate</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>h2o</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Model Tree</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;M5&#39;</span></code></pre>
<p>Type: Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>pruned</code> (Pruned)</li>
<li><code>smoothed</code> (Smoothed)</li>
<li><code>rules</code> (Rules)</li>
</ul>
<p>Required packages: <code>RWeka</code></p>
<p><strong>Rotation Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rotationForest&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K</code> (#Variable Subsets)</li>
<li><code>L</code> (Ensemble Size)</li>
</ul>
<p>Required packages: <code>rotationForest</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Rotation Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rotationForestCp&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K</code> (#Variable Subsets)</li>
<li><code>L</code> (Ensemble Size)</li>
<li><code>cp</code> (Complexity Parameter)</li>
</ul>
<p>Required packages: <code>rpart</code>, <code>plyr</code>, <code>rotationForest</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Single C5.0 Tree</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0Tree&#39;</span></code></pre>
<p>Type: Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>C50</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Stochastic Gradient Boosting</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gbm&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>n.trees</code> (# Boosting Iterations)</li>
<li><code>interaction.depth</code> (Max Tree Depth)</li>
<li><code>shrinkage</code> (Shrinkage)</li>
<li><code>n.minobsinnode</code> (Min. Terminal Node Size)</li>
</ul>
<p>Required packages: <code>gbm</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Tree Models from Genetic Algorithms</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;evtree&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Complexity Parameter)</li>
</ul>
<p>Required packages: <code>evtree</code></p>
<p><strong>Tree-Based Ensembles</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;nodeHarvest&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxinter</code> (Maximum Interaction Depth)</li>
<li><code>mode</code> (Prediction Mode)</li>
</ul>
<p>Required packages: <code>nodeHarvest</code></p>
<div id="Two_Class_Only">

</div>
</div>
<div id="two-class-only" class="section level3">
<h3><span class="header-section-number">7.0.51</span> Two Class Only</h3>
<p>(back to <a href="train-models-by-tag.html#top">contents</a>)</p>
<p><strong>AdaBoost Classification Trees</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;adaboost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nIter</code> (#Trees)</li>
<li><code>method</code> (Method)</li>
</ul>
<p>Required packages: <code>fastAdaboost</code></p>
<p><strong>Bagged Logic Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;logicBag&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nleaves</code> (Maximum Number of Leaves)</li>
<li><code>ntrees</code> (Number of Trees)</li>
</ul>
<p>Required packages: <code>logicFS</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>logicFS</code> package is fully loaded when this model is used.</p>
<p><strong>Bayesian Additive Regression Trees</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;bartMachine&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>num_trees</code> (#Trees)</li>
<li><code>k</code> (Prior Boundary)</li>
<li><code>alpha</code> (Base Terminal Node Hyperparameter)</li>
<li><code>beta</code> (Power Terminal Node Hyperparameter)</li>
<li><code>nu</code> (Degrees of Freedom)</li>
</ul>
<p>Required packages: <code>bartMachine</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Binary Discriminant Analysis</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;binda&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda.freqs</code> (Shrinkage Intensity)</li>
</ul>
<p>Required packages: <code>binda</code></p>
<p><strong>Boosted Classification Trees</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ada&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>iter</code> (#Trees)</li>
<li><code>maxdepth</code> (Max Tree Depth)</li>
<li><code>nu</code> (Learning Rate)</li>
</ul>
<p>Required packages: <code>ada</code>, <code>plyr</code></p>
<p><strong>Boosted Generalized Additive Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;gamboost&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>prune</code> (AIC Prune?)</li>
</ul>
<p>Required packages: <code>mboost</code>, <code>plyr</code>, <code>import</code></p>
<p>Notes: The <code>prune</code> option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in <code>?mboost::mstop</code>. If pruning is not used, the ensemble makes predictions using the exact value of the <code>mstop</code> tuning parameter value.</p>
<p><strong>Boosted Generalized Linear Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmboost&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mstop</code> (# Boosting Iterations)</li>
<li><code>prune</code> (AIC Prune?)</li>
</ul>
<p>Required packages: <code>plyr</code>, <code>mboost</code></p>
<p>A model-specific variable importance metric is available. Notes: The <code>prune</code> option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in <code>?mboost::mstop</code>. If pruning is not used, the ensemble makes predictions using the exact value of the <code>mstop</code> tuning parameter value.</p>
<p><strong>CHi-squared Automated Interaction Detection</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;chaid&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha2</code> (Merging Threshold)</li>
<li><code>alpha3</code> (Splitting former Merged Threshold)</li>
<li><code>alpha4</code> (
Splitting former Merged Threshold)</li>
</ul>
<p>Required packages: <code>CHAID</code></p>
<p><strong>Cost-Sensitive C5.0</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;C5.0Cost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>trials</code> (# Boosting Iterations)</li>
<li><code>model</code> (Model Type)</li>
<li><code>winnow</code> (Winnow)</li>
<li><code>cost</code> (Cost)</li>
</ul>
<p>Required packages: <code>C50</code>, <code>plyr</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Cost-Sensitive CART</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rpartCost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cp</code> (Complexity Parameter)</li>
<li><code>Cost</code> (Cost)</li>
</ul>
<p>Required packages: <code>rpart</code>, <code>plyr</code></p>
<p><strong>DeepBoost</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;deepboost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>num_iter</code> (# Boosting Iterations)</li>
<li><code>tree_depth</code> (Tree Depth)</li>
<li><code>beta</code> (L1 Regularization)</li>
<li><code>lambda</code> (Tree Depth Regularization)</li>
<li><code>loss_type</code> (Loss)</li>
</ul>
<p>Required packages: <code>deepboost</code></p>
<p><strong>Distance Weighted Discrimination with Polynomial Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dwdPoly&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>qval</code> (q)</li>
<li><code>degree</code> (Polynomial Degree)</li>
<li><code>scale</code> (Scale)</li>
</ul>
<p>Required packages: <code>kerndwd</code></p>
<p><strong>Distance Weighted Discrimination with Radial Basis Function Kernel</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dwdRadial&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>qval</code> (q)</li>
<li><code>sigma</code> (Sigma)</li>
</ul>
<p>Required packages: <code>kernlab</code>, <code>kerndwd</code></p>
<p><strong>Generalized Linear Model</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glm&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Generalized Linear Model with Stepwise Feature Selection</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmStepAIC&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>No tuning parameters for this model</p>
<p>Required packages: <code>MASS</code></p>
<p><strong>glmnet</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;glmnet_h2o&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>alpha</code> (Mixing Percentage)</li>
<li><code>lambda</code> (Regularization Parameter)</li>
</ul>
<p>Required packages: <code>h2o</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>L2 Regularized Linear Support Vector Machines with Class Weights</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinearWeights2&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
<li><code>Loss</code> (Loss Function)</li>
<li><code>weight</code> (Class Weight)</li>
</ul>
<p>Required packages: <code>LiblineaR</code></p>
<p><strong>Linear Distance Weighted Discrimination</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;dwdLinear&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>lambda</code> (Regularization Parameter)</li>
<li><code>qval</code> (q)</li>
</ul>
<p>Required packages: <code>kerndwd</code></p>
<p><strong>Linear Support Vector Machines with Class Weights</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmLinearWeights&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>cost</code> (Cost)</li>
<li><code>weight</code> (Class Weight)</li>
</ul>
<p>Required packages: <code>e1071</code></p>
<p><strong>Logic Regression</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;logreg&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>treesize</code> (Maximum Number of Leaves)</li>
<li><code>ntrees</code> (Number of Trees)</li>
</ul>
<p>Required packages: <code>LogicReg</code></p>
<p><strong>Multilayer Perceptron Network with Dropout</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mlpKerasDropoutCost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>dropout</code> (Dropout Rate)</li>
<li><code>batch_size</code> (Batch Size)</li>
<li><code>lr</code> (Learning Rate)</li>
<li><code>rho</code> (Rho)</li>
<li><code>decay</code> (Learning Rate Decay)</li>
<li><code>cost</code> (Cost)</li>
<li><code>activation</code> (Activation Function)</li>
</ul>
<p>Required packages: <code>keras</code></p>
<p>Notes: After <code>train</code> completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use <code>keras::unsearlize_model(object$finalModel$object)</code> in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Finally, the cost parameter weights the first class in the outcome vector. Unlike other packages used by <code>train</code>, the <code>dplyr</code> package is fully loaded when this model is used.</p>
<p><strong>Multilayer Perceptron Network with Weight Decay</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;mlpKerasDecayCost&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>size</code> (#Hidden Units)</li>
<li><code>lambda</code> (L2 Regularization)</li>
<li><code>batch_size</code> (Batch Size)</li>
<li><code>lr</code> (Learning Rate)</li>
<li><code>rho</code> (Rho)</li>
<li><code>decay</code> (Learning Rate Decay)</li>
<li><code>cost</code> (Cost)</li>
<li><code>activation</code> (Activation Function)</li>
</ul>
<p>Required packages: <code>keras</code></p>
<p>Notes: After <code>train</code> completes, the keras model object is serialized so that it can be used between R session. When predicting, the code will temporarily unsearalize the object. To make the predictions more efficient, the user might want to use <code>keras::unsearlize_model(object$finalModel$object)</code> in the current R session so that that operation is only done once. Also, this model cannot be run in parallel due to the nature of how tensorflow does the computations. Finally, the cost parameter weights the first class in the outcome vector. Unlike other packages used by <code>train</code>, the <code>dplyr</code> package is fully loaded when this model is used.</p>
<p><strong>Oblique Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFlog&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>obliqueRF</code> package is fully loaded when this model is used.</p>
<p><strong>Oblique Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFpls&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>obliqueRF</code> package is fully loaded when this model is used.</p>
<p><strong>Oblique Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFridge&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>obliqueRF</code> package is fully loaded when this model is used.</p>
<p><strong>Oblique Random Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;ORFsvm&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>mtry</code> (#Randomly Selected Predictors)</li>
</ul>
<p>Required packages: <code>obliqueRF</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>obliqueRF</code> package is fully loaded when this model is used.</p>
<p><strong>Partial Least Squares Generalized Linear Models </strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;plsRglm&#39;</span></code></pre>
<p>Type: Classification, Regression</p>
<p>Tuning parameters:</p>
<ul>
<li><code>nt</code> (#PLS Components)</li>
<li><code>alpha.pvals.expli</code> (p-Value threshold)</li>
</ul>
<p>Required packages: <code>plsRglm</code></p>
<p>Notes: Unlike other packages used by <code>train</code>, the <code>plsRglm</code> package is fully loaded when this model is used.</p>
<p><strong>Rotation Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rotationForest&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K</code> (#Variable Subsets)</li>
<li><code>L</code> (Ensemble Size)</li>
</ul>
<p>Required packages: <code>rotationForest</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Rotation Forest</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;rotationForestCp&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>K</code> (#Variable Subsets)</li>
<li><code>L</code> (Ensemble Size)</li>
<li><code>cp</code> (Complexity Parameter)</li>
</ul>
<p>Required packages: <code>rpart</code>, <code>plyr</code>, <code>rotationForest</code></p>
<p>A model-specific variable importance metric is available.</p>
<p><strong>Support Vector Machines with Class Weights</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;svmRadialWeights&#39;</span></code></pre>
<p>Type: Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>sigma</code> (Sigma)</li>
<li><code>C</code> (Cost)</li>
<li><code>Weight</code> (Weight)</li>
</ul>
<p>Required packages: <code>kernlab</code></p>
<p><strong>Tree-Based Ensembles</strong></p>
<pre class="sourceCode r"><code class="sourceCode r">  method =<span class="st"> &#39;nodeHarvest&#39;</span></code></pre>
<p>Type: Regression, Classification</p>
<p>Tuning parameters:</p>
<ul>
<li><code>maxinter</code> (Maximum Interaction Depth)</li>
<li><code>mode</code> (Prediction Mode)</li>
</ul>
<p>Required packages: <code>nodeHarvest</code></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="available-models.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="models-clustered-by-tag-similarity.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
