[
["index.html", "The caret Package 1 Introduction", " The caret Package Max Kuhn 2016-11-29 1 Introduction The caret package (short for _C_lassification _A_nd _RE_gression _T_raining) is a set of functions that attempt to streamline the process for creating predictive models. The package contains tools for: data splitting pre-processing feature selection model tuning using resampling variable importance estimation as well as other functionality. There are many different modeling functions in R. Some have different syntax for model training and/or prediction. The package started off as a way to provide a uniform interface the functions themselves, as well as a way to standardize common tasks (such parameter tuning and variable importance). The current release version can be found on CRAN and the project is hosted on github. Some resources: The book Applied Predictive Modeling features caret and over 40 other R packages. It is on sale at Amazon or the the publisher’s website. There is a companion website too. There is also a paper on caret in the Journal of Statistical Software. The example data can be obtained here(the predictors) and here (the outcomes). There is a webinar for the package on Youtube that was organized and recorded by Ray DiGiacomo Jr for the Orange County R User Group. At useR! 2014, I was interviewed and discussed the package and the book. DataCamp has a beginner’s tutorial on machine learning in R using caret. You can always email me with questions,comments or suggestions. These HTML pages were created using bookdown. "],
["visualizations.html", "2 Visualizations", " 2 Visualizations The featurePlot function is a wrapper for different lattice plots to visualize the data. For example, the following figures show the default plot for continuous outcomes generated using the featurePlot function. For classification data sets, the iris data are used for illustration. str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... Scatterplot Matrix library(AppliedPredictiveModeling) transparentTheme(trans = .4) library(caret) featurePlot(x = iris[, 1:4], y = iris$Species, plot = &quot;pairs&quot;, ## Add a key at the top auto.key = list(columns = 3)) Scatterplot Matrix with Ellipses featurePlot(x = iris[, 1:4], y = iris$Species, plot = &quot;ellipse&quot;, ## Add a key at the top auto.key = list(columns = 3)) Overlayed Density Plots transparentTheme(trans = .9) featurePlot(x = iris[, 1:4], y = iris$Species, plot = &quot;density&quot;, ## Pass in options to xyplot() to ## make it prettier scales = list(x = list(relation=&quot;free&quot;), y = list(relation=&quot;free&quot;)), adjust = 1.5, pch = &quot;|&quot;, layout = c(4, 1), auto.key = list(columns = 3)) Box Plots featurePlot(x = iris[, 1:4], y = iris$Species, plot = &quot;box&quot;, ## Pass in options to bwplot() scales = list(y = list(relation=&quot;free&quot;), x = list(rot = 90)), layout = c(4,1 ), auto.key = list(columns = 2)) Scatter Plots For regression, the Boston Housing data is used: library(mlbench) data(BostonHousing) regVar &lt;- c(&quot;age&quot;, &quot;lstat&quot;, &quot;tax&quot;) str(BostonHousing[, regVar]) ## &#39;data.frame&#39;: 506 obs. of 3 variables: ## $ age : num 65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ... ## $ lstat: num 4.98 9.14 4.03 2.94 5.33 ... ## $ tax : num 296 242 242 222 222 222 311 311 311 311 ... When the predictors are continuous, featurePlot can be used to create scatter plots of each of the predictors with the outcome. For example: theme1 &lt;- trellis.par.get() theme1$plot.symbol$col = rgb(.2, .2, .2, .4) theme1$plot.symbol$pch = 16 theme1$plot.line$col = rgb(1, 0, 0, .7) theme1$plot.line$lwd &lt;- 2 trellis.par.set(theme1) featurePlot(x = BostonHousing[, regVar], y = BostonHousing$medv, plot = &quot;scatter&quot;, layout = c(3, 1)) Note that the x-axis scales are different. The function automatically uses scales = list(y = list(relation = &quot;free&quot;)) so you don’t have to add it. We can also pass in options to the lattice function xyplot. For example, we can add a scatter plot smoother by passing in new options: featurePlot(x = BostonHousing[, regVar], y = BostonHousing$medv, plot = &quot;scatter&quot;, type = c(&quot;p&quot;, &quot;smooth&quot;), span = .5, layout = c(3, 1)) The options degree and span control the smoothness of the smoother. "],
["pre-processing.html", "3 Pre-Processing 3.1 Creating Dummy Variables 3.2 Zero- and Near Zero-Variance Predictors 3.3 Identifying Correlated Predictors 3.4 Linear Dependencies 3.5 The preProcess Function 3.6 Centering and Scaling 3.7 Imputation 3.8 Transforming Predictors 3.9 Putting It All Together 3.10 Class Distance Calculations", " 3 Pre-Processing Creating Dummy Variables Zero- and Near Zero-Variance Predictors Identifying Correlated Predictors Linear Dependencies The preProcess Function Centering and Scaling Imputation Transforming Predictors Putting It All Together Class Distance Calculations caret includes several functions to pre-process the predictor data. It assumes that all of the data are numeric (i.e. factors have been converted to dummy variables via model.matrix, dummyVars or other means). 3.1 Creating Dummy Variables The function dummyVars can be used to generate a complete (less than full rank parameterized) set of dummy variables from one or more factors. The function takes a formula and a data set and outputs an object that can be used to create the dummy variables using the predict method. For example, the etitanic data set in the earth package includes two factors: 1st, 2nd, 3rd) and sex (with levels female, male). The base R function model.matrix would generate the following variables: library(earth) data(etitanic) head(model.matrix(survived ~ ., data = etitanic)) ## (Intercept) pclass2nd pclass3rd sexmale age sibsp parch ## 1 1 0 0 0 29.0000 0 0 ## 2 1 0 0 1 0.9167 1 2 ## 3 1 0 0 0 2.0000 1 2 ## 4 1 0 0 1 30.0000 1 2 ## 5 1 0 0 0 25.0000 1 2 ## 6 1 0 0 1 48.0000 0 0 Using dummyVars: dummies &lt;- dummyVars(survived ~ ., data = etitanic) head(predict(dummies, newdata = etitanic)) ## pclass.1st pclass.2nd pclass.3rd sex.female sex.male age sibsp parch ## 1 1 0 0 1 0 29.0000 0 0 ## 2 1 0 0 0 1 0.9167 1 2 ## 3 1 0 0 1 0 2.0000 1 2 ## 4 1 0 0 0 1 30.0000 1 2 ## 5 1 0 0 1 0 25.0000 1 2 ## 6 1 0 0 0 1 48.0000 0 0 Note there is no intercept and each factor has a dummy variable for each level, so this parameterization may not be useful for some model functions, such as lm. 3.2 Zero- and Near Zero-Variance Predictors In some situations, the data generating mechanism can create predictors that only have a single unique value (i.e. a “zero-variance predictor”). For many models (excluding tree-based models), this may cause the model to crash or the fit to be unstable. Similarly, predictors might have only a handful of unique values that occur with very low frequencies. For example, in the drug resistance data, the nR11 descriptor (number of 11-membered rings) data have a few unique numeric values that are highly unbalanced: data(mdrr) data.frame(table(mdrrDescr$nR11)) ## Var1 Freq ## 1 0 501 ## 2 1 4 ## 3 2 23 The concern here that these predictors may become zero-variance predictors when the data are split into cross-validation/bootstrap sub-samples or that a few samples may have an undue influence on the model. These “near-zero-variance” predictors may need to be identified and eliminated prior to modeling. To identify these types of predictors, the following two metrics can be calculated: the frequency of the most prevalent value over the second most frequent value (called the “frequency ratio’’), which would be near one for well-behaved predictors and very large for highly-unbalanced data and the “percent of unique values’’ is the number of unique values divided by the total number of samples (times 100) that approaches zero as the granularity of the data increases If the frequency ratio is greater than a pre-specified threshold and the unique value percentage is less than a threshold, we might consider a predictor to be near zero-variance. We would not want to falsely identify data that have low granularity but are evenly distributed, such as data from a discrete uniform distribution. Using both criteria should not falsely detect such predictors. Looking at the MDRR data, the nearZeroVar function can be used to identify near zero-variance variables (the saveMetrics argument can be used to show the details and usually defaults to FALSE): nzv &lt;- nearZeroVar(mdrrDescr, saveMetrics= TRUE) nzv[nzv$nzv,][1:10,] ## freqRatio percentUnique zeroVar nzv ## nTB 23.00000 0.3787879 FALSE TRUE ## nBR 131.00000 0.3787879 FALSE TRUE ## nI 527.00000 0.3787879 FALSE TRUE ## nR03 527.00000 0.3787879 FALSE TRUE ## nR08 527.00000 0.3787879 FALSE TRUE ## nR11 21.78261 0.5681818 FALSE TRUE ## nR12 57.66667 0.3787879 FALSE TRUE ## D.Dr03 527.00000 0.3787879 FALSE TRUE ## D.Dr07 123.50000 5.8712121 FALSE TRUE ## D.Dr08 527.00000 0.3787879 FALSE TRUE dim(mdrrDescr) ## [1] 528 342 nzv &lt;- nearZeroVar(mdrrDescr) filteredDescr &lt;- mdrrDescr[, -nzv] dim(filteredDescr) ## [1] 528 297 By default, nearZeroVar will return the positions of the variables that are flagged to be problematic. 3.3 Identifying Correlated Predictors While there are some models that thrive on correlated predictors (such as pls), other models may benefit from reducing the level of correlation between the predictors. Given a correlation matrix, the findCorrelation function uses the following algorithm to flag predictors for removal: descrCor &lt;- cor(filteredDescr) highCorr &lt;- sum(abs(descrCor[upper.tri(descrCor)]) &gt; .999) For the previous MDRR data, there are rI(highCorr) descriptors that are almost perfectly correlated (|correlation| &gt; 0.999), such as the total information index of atomic composition (IAC) and the total information content index (neighborhood symmetry of 0-order) (TIC0) (correlation = 1). The code chunk below shows the effect of removing descriptors with absolute correlations above 0.75. descrCor &lt;- cor(filteredDescr) summary(descrCor[upper.tri(descrCor)]) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -0.99610 -0.05373 0.25010 0.26080 0.65530 1.00000 highlyCorDescr &lt;- findCorrelation(descrCor, cutoff = .75) filteredDescr &lt;- filteredDescr[,-highlyCorDescr] descrCor2 &lt;- cor(filteredDescr) summary(descrCor2[upper.tri(descrCor2)]) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -0.70730 -0.05378 0.04418 0.06692 0.18860 0.74460 3.4 Linear Dependencies The function findLinearCombos uses the QR decomposition of a matrix to enumerate sets of linear combinations (if they exist). For example, consider the following matrix that is could have been produced by a less-than-full-rank parameterizations of a two-way experimental layout: ltfrDesign &lt;- matrix(0, nrow=6, ncol=6) ltfrDesign[,1] &lt;- c(1, 1, 1, 1, 1, 1) ltfrDesign[,2] &lt;- c(1, 1, 1, 0, 0, 0) ltfrDesign[,3] &lt;- c(0, 0, 0, 1, 1, 1) ltfrDesign[,4] &lt;- c(1, 0, 0, 1, 0, 0) ltfrDesign[,5] &lt;- c(0, 1, 0, 0, 1, 0) ltfrDesign[,6] &lt;- c(0, 0, 1, 0, 0, 1) Note that columns two and three add up to the first column. Similarly, columns four, five and six add up the first column. findLinearCombos will return a list that enumerates these dependencies. For each linear combination, it will incrementally remove columns from the matrix and test to see if the dependencies have been resolved. findLinearCombos will also return a vector of column positions can be removed to eliminate the linear dependencies: comboInfo &lt;- findLinearCombos(ltfrDesign) comboInfo ## $linearCombos ## $linearCombos[[1]] ## [1] 3 1 2 ## ## $linearCombos[[2]] ## [1] 6 1 4 5 ## ## ## $remove ## [1] 3 6 ltfrDesign[, -comboInfo$remove] ## [,1] [,2] [,3] [,4] ## [1,] 1 1 1 0 ## [2,] 1 1 0 1 ## [3,] 1 1 0 0 ## [4,] 1 0 1 0 ## [5,] 1 0 0 1 ## [6,] 1 0 0 0 These types of dependencies can arise when large numbers of binary chemical fingerprints are used to describe the structure of a molecule. 3.5 The preProcess Function The preProcess class can be used for many operations on predictors, including centering and scaling. The function preProcess estimates the required parameters for each operation and predict.preProcess is used to apply them to specific data sets. This function can also be interfaces when calling the train function. Several types of techniques are described in the next few sections and then another example is used to demonstrate how multiple methods can be used. Note that, in all cases, the preProcess function estimates whatever it requires from a specific data set (e.g. the training set) and then applies these transformations to any data set without recomputing the values 3.6 Centering and Scaling In the example below, the half of the MDRR data are used to estimate the location and scale of the predictors. The function preProcess doesn’t actually pre-process the data. predict.preProcess is used to pre-process this and other data sets. set.seed(96) inTrain &lt;- sample(seq(along = mdrrClass), length(mdrrClass)/2) training &lt;- filteredDescr[inTrain,] test &lt;- filteredDescr[-inTrain,] trainMDRR &lt;- mdrrClass[inTrain] testMDRR &lt;- mdrrClass[-inTrain] preProcValues &lt;- preProcess(training, method = c(&quot;center&quot;, &quot;scale&quot;)) trainTransformed &lt;- predict(preProcValues, training) testTransformed &lt;- predict(preProcValues, test) The preProcess option &quot;ranges&quot; scales the data to the interval between zero and one. 3.7 Imputation preProcess can be used to impute data sets based only on information in the training set. One method of doing this is with K-nearest neighbors. For an arbitrary sample, the K closest neighbors are found in the training set and the value for the predictor is imputed using these values (e.g. using the mean). Using this approach will automatically trigger preProcess to center and scale the data, regardless of what is in the method argument. Alternatively, bagged trees can also be used to impute. For each predictor in the data, a bagged tree is created using all of the other predictors in the training set. When a new sample has a missing predictor value, the bagged model is used to predict the value. While, in theory, this is a more powerful method of imputing, the computational costs are much higher than the nearest neighbor technique. 3.8 Transforming Predictors In some cases, there is a need to use principal component analysis (PCA) to transform the data to a smaller sub–space where the new variable are uncorrelated with one another. The preProcess class can apply this transformation by including &quot;pca&quot; in the method argument. Doing this will also force scaling of the predictors. Note that when PCA is requested, predict.preProcess changes the column names to PC1, PC2 and so on. Similarly, independent component analysis (ICA) can also be used to find new variables that are linear combinations of the original set such that the components are independent (as opposed to uncorrelated in PCA). The new variables will be labeled as IC1, IC2 and so on. The “spatial sign” transformation (Serneels et al, 2006) projects the data for a predictor to the unit circle in p dimensions, where p is the number of predictors. Essentially, a vector of data is divided by its norm. The two figures below show two centered and scaled descriptors from the MDRR data before and after the spatial sign transformation. The predictors should be centered and scaled before applying this transformation. library(AppliedPredictiveModeling) transparentTheme(trans = .4) plotSubset &lt;- data.frame(scale(mdrrDescr[, c(&quot;nC&quot;, &quot;X4v&quot;)])) xyplot(nC ~ X4v, data = plotSubset, groups = mdrrClass, auto.key = list(columns = 2)) After the spatial sign: transformed &lt;- spatialSign(plotSubset) transformed &lt;- as.data.frame(transformed) xyplot(nC ~ X4v, data = transformed, groups = mdrrClass, auto.key = list(columns = 2)) Another option, &quot;BoxCox&quot; will estimate a Box–Cox transformation on the predictors if the data are greater than zero. preProcValues2 &lt;- preProcess(training, method = &quot;BoxCox&quot;) trainBC &lt;- predict(preProcValues2, training) testBC &lt;- predict(preProcValues2, test) preProcValues2 ## Created from 264 samples and 31 variables ## ## Pre-processing: ## - Box-Cox transformation (31) ## - ignored (0) ## ## Lambda estimates for Box-Cox transformation: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -2.0000 -0.2000 0.3000 0.4097 1.7000 2.0000 The NA values correspond to the predictors that could not be transformed. This transformation requires the data to be greater than zero. Two similar transformations, the Yeo-Johnson and exponential transformation of Manly (1976) can also be used in preProcess. 3.9 Putting It All Together In Applied Predictive Modeling there is a case study where the execution times of jobs in a high performance computing environment are being predicted. The data are: library(AppliedPredictiveModeling) data(schedulingData) str(schedulingData) ## &#39;data.frame&#39;: 4331 obs. of 8 variables: ## $ Protocol : Factor w/ 14 levels &quot;A&quot;,&quot;C&quot;,&quot;D&quot;,&quot;E&quot;,..: 4 4 4 4 4 4 4 4 4 4 ... ## $ Compounds : num 997 97 101 93 100 100 105 98 101 95 ... ## $ InputFields: num 137 103 75 76 82 82 88 95 91 92 ... ## $ Iterations : num 20 20 10 20 20 20 20 20 20 20 ... ## $ NumPending : num 0 0 0 0 0 0 0 0 0 0 ... ## $ Hour : num 14 13.8 13.8 10.1 10.4 ... ## $ Day : Factor w/ 7 levels &quot;Mon&quot;,&quot;Tue&quot;,&quot;Wed&quot;,..: 2 2 4 5 5 3 5 5 5 3 ... ## $ Class : Factor w/ 4 levels &quot;VF&quot;,&quot;F&quot;,&quot;M&quot;,&quot;L&quot;: 2 1 1 1 1 1 1 1 1 1 ... The data are a mix of categorical and numeric predictors. Suppose we want to use the Yeo-Johnson transformation on the continuous predictors then center and scale them. Let’s also suppose that we will be running a tree-based models so we might want to keep the factors as factors (as opposed to creating dummy variables). We run the function on all the columns except the last, which is the outcome. pp_hpc &lt;- preProcess(schedulingData[, -8], method = c(&quot;center&quot;, &quot;scale&quot;, &quot;YeoJohnson&quot;)) pp_hpc ## Created from 4331 samples and 7 variables ## ## Pre-processing: ## - centered (5) ## - ignored (2) ## - scaled (5) ## - Yeo-Johnson transformation (5) ## ## Lambda estimates for Yeo-Johnson transformation: ## -0.08, -0.03, -1.05, -1.1, 1.44 transformed &lt;- predict(pp_hpc, newdata = schedulingData[, -8]) head(transformed) ## Protocol Compounds InputFields Iterations NumPending Hour Day ## 1 E 1.2289589 -0.6324538 -0.06155877 -0.554123 0.004586502 Tue ## 2 E -0.6065822 -0.8120451 -0.06155877 -0.554123 -0.043733215 Tue ## 3 E -0.5719530 -1.0131509 -2.78949011 -0.554123 -0.034967191 Thu ## 4 E -0.6427734 -1.0047281 -0.06155877 -0.554123 -0.964170760 Fri ## 5 E -0.5804710 -0.9564501 -0.06155877 -0.554123 -0.902085029 Fri ## 6 E -0.5804710 -0.9564501 -0.06155877 -0.554123 0.698108779 Wed The two predictors labeled as “ignored” in the output are the two factor predictors. These are not altered but the numeric predictors are transformed. However, the predictor for the number of pending jobs, has a very sparse and unbalanced distribution: mean(schedulingData$NumPending == 0) ## [1] 0.7561764 For some other models, this might be an issue (especially if we resample or down-sample the data). We can add a filter to check for zero- or near zero-variance predictors prior to running the pre-processing calculations: pp_no_nzv &lt;- preProcess(schedulingData[, -8], method = c(&quot;center&quot;, &quot;scale&quot;, &quot;YeoJohnson&quot;, &quot;nzv&quot;)) pp_no_nzv ## Created from 4331 samples and 7 variables ## ## Pre-processing: ## - centered (4) ## - ignored (2) ## - removed (1) ## - scaled (4) ## - Yeo-Johnson transformation (4) ## ## Lambda estimates for Yeo-Johnson transformation: ## -0.08, -0.03, -1.05, 1.44 predict(pp_no_nzv, newdata = schedulingData[1:6, -8]) ## Protocol Compounds InputFields Iterations Hour Day ## 1 E 1.2289589 -0.6324538 -0.06155877 0.004586502 Tue ## 2 E -0.6065822 -0.8120451 -0.06155877 -0.043733215 Tue ## 3 E -0.5719530 -1.0131509 -2.78949011 -0.034967191 Thu ## 4 E -0.6427734 -1.0047281 -0.06155877 -0.964170760 Fri ## 5 E -0.5804710 -0.9564501 -0.06155877 -0.902085029 Fri ## 6 E -0.5804710 -0.9564501 -0.06155877 0.698108779 Wed Note that one predictor is labeled as “removed” and the processed data lack the sparse predictor. 3.10 Class Distance Calculations caret contains functions to generate new predictors variables based on distances to class centroids (similar to how linear discriminant analysis works). For each level of a factor variable, the class centroid and covariance matrix is calculated. For new samples, the Mahalanobis distance to each of the class centroids is computed and can be used as an additional predictor. This can be helpful for non-linear models when the true decision boundary is actually linear. In cases where there are more predictors within a class than samples, the classDist function has arguments called pca and keep arguments that allow for principal components analysis within each class to be used to avoid issues with singular covariance matrices. predict.classDist is then used to generate the class distances. By default, the distances are logged, but this can be changed via the trans argument to predict.classDist. As an example, we can used the MDRR data. centroids &lt;- classDist(trainBC, trainMDRR) distances &lt;- predict(centroids, testBC) distances &lt;- as.data.frame(distances) head(distances) ## dist.Active dist.Inactive ## PROMETHAZINE 5.810607 4.098229 ## ACEPROMETAZINE 4.272003 4.169292 ## PYRATHIAZINE 4.570192 4.224053 ## THIORIDAZINE 4.548315 5.064125 ## MESORIDAZINE 4.621708 5.080362 ## SULFORIDAZINE 5.344699 5.145311 This image shows a scatterplot matrix of the class distances for the held-out samples: xyplot(dist.Active ~ dist.Inactive, data = distances, groups = testMDRR, auto.key = list(columns = 2)) "],
["data-splitting.html", "4 Data Splitting 4.1 Simple Splitting Based on the Outcome 4.2 Splitting Based on the Predictors 4.3 Data Splitting for Time Series", " 4 Data Splitting Contents Simple Splitting Based on the Outcome Splitting Based on the Predictors Data Splitting for Time Series 4.1 Simple Splitting Based on the Outcome The function createDataPartition can be used to create balanced splits of the data. If the y argument to this function is a factor, the random sampling occurs within each class and should preserve the overall class distribution of the data. For example, to create a single 80/20% split of the iris data: library(caret) set.seed(3456) trainIndex &lt;- createDataPartition(iris$Species, p = .8, list = FALSE, times = 1) head(trainIndex) ## Resample1 ## [1,] 1 ## [2,] 2 ## [3,] 4 ## [4,] 5 ## [5,] 6 ## [6,] 8 irisTrain &lt;- iris[ trainIndex,] irisTest &lt;- iris[-trainIndex,] The list = FALSE avoids returns the data as a list. This function also has an argument, times, that can create multiple splits at once; the data indices are returned in a list of integer vectors. Similarly, createResample can be used to make simple bootstrap samples and createFolds can be used to generate balanced cross–validation groupings from a set of data. 4.2 Splitting Based on the Predictors Also, the function maxDissim can be used to create sub–samples using a maximum dissimilarity approach (Willett, 1999). Suppose there is a data set A with m samples and a larger data set B with n samples. We may want to create a sub–sample from B that is diverse when compared to A. To do this, for each sample in B, the function calculates the m dissimilarities between each point in A. The most dissimilar point in B is added to A and the process continues. There are many methods in R to calculate dissimilarity. caret uses the proxy package. See the manual for that package for a list of available measures. Also, there are many ways to calculate which sample is “most dissimilar”. The argument obj can be used to specify any function that returns a scalar measure. caret includes two functions, minDiss and sumDiss, that can be used to maximize the minimum and total dissimilarities, respectfully. As an example, the figure below shows a scatter plot of two chemical descriptors for the Cox2 data. Using an initial random sample of 5 compounds, we can select 20 more compounds from the data so that the new compounds are most dissimilar from the initial 5 that were specified. The panels in the figure show the results using several combinations of distance metrics and scoring functions. For these data, the distance measure has less of an impact than the scoring method for determining which compounds are most dissimilar. library(mlbench) data(BostonHousing) testing &lt;- scale(BostonHousing[, c(&quot;age&quot;, &quot;nox&quot;)]) set.seed(5) ## A random sample of 5 data points startSet &lt;- sample(1:dim(testing)[1], 5) samplePool &lt;- testing[-startSet,] start &lt;- testing[startSet,] newSamp &lt;- maxDissim(start, samplePool, n = 20) head(newSamp) ## [1] 461 406 49 308 469 76 The visualization below shows the data set (small points), the starting samples (larger blue points) and the order in which the other 20 samples are added. 4.3 Data Splitting for Time Series Simple random sampling of time series is probably not the best way to resample times series data. Hyndman and Athanasopoulos (2013) discuss rolling forecasting origin techniques that move the training and test sets in time. caret contains a function called createTimeSlices that can create the indices for this type of splitting. The three parameters for this type of splitting are: initialWindow: the initial number of consecutive values in each training set sample horizon: The number of consecutive values in test set sample fixedWindow: A logical: if FALSE, the training set always start at the first sample and the training set size will vary over data splits. As an example, suppose we have a time series with 20 data points. We can fix initialWindow = 5 and look at different settings of the other two arguments. In the plot below, rows in each panel correspond to different data splits (i.e. resamples) and the columns correspond to different data points. Also, red indicates samples that are in included in the training set and the blue indicates samples in the test set. "],
["model-training-and-tuning.html", "5 Model Training and Tuning 5.1 Model Training and Parameter Tuning 5.2 An Example 5.3 Basic Parameter Tuning 5.4 Notes on Reproducibility 5.5 Customizing the Tuning Process 5.6 Choosing the Final Model 5.7 Extracting Predictions and Class Probabilities 5.8 Exploring and Comparing Resampling Distributions 5.9 Fitting Models Without Parameter Tuning", " 5 Model Training and Tuning Contents Model Training and Parameter Tuning An Example Basic Parameter Tuning Notes on Reproducibility Customizing the Tuning Process Pre-Processing Options Alternate Tuning Grids Plotting the Resampling Profile The trainControl Function Alternate Performance Metrics Choosing the Final Model Extracting Predictions and Class Probabilities Exploring and Comparing Resampling Distributions Within-Model Between-Models Fitting Models Without Parameter Tuning 5.1 Model Training and Parameter Tuning The caret package has several functions that attempt to streamline the model building and evaluation process. The train function can be used to evaluate, using resampling, the effect of model tuning parameters on performance choose the “optimal” model across these parameters estimate model performance from a training set First, a specific model must be chosen. Currently, 233 are available using caret; see train Model List or train Models By Tag for details. On these pages, there are lists of tuning parameters that can potentially be optimized. User-defined models can also be created. The first step in tuning the model (line 1 in the algorithm above is to choose a set of parameters to evaluate. For example, if fitting a Partial Least Squares (PLS) model, the number of PLS components to evaluate must be specified. Once the model and tuning parameter values have been defined, the type of resampling should be also be specified. Currently, k-fold cross-validation (once or repeated), leave-one-out cross-validation and bootstrap (simple estimation or the 632 rule) resampling methods can be used by train. After resampling, the process produces a profile of performance measures is available to guide the user as to which tuning parameter values should be chosen. By default, the function automatically chooses the tuning parameters associated with the best value, although different algorithms can be used (see details below). 5.2 An Example The Sonar data are available in the mlbench package. Here, we load the data: library(mlbench) data(Sonar) str(Sonar[, 1:10]) ## &#39;data.frame&#39;: 208 obs. of 10 variables: ## $ V1 : num 0.02 0.0453 0.0262 0.01 0.0762 0.0286 0.0317 0.0519 0.0223 0.0164 ... ## $ V2 : num 0.0371 0.0523 0.0582 0.0171 0.0666 0.0453 0.0956 0.0548 0.0375 0.0173 ... ## $ V3 : num 0.0428 0.0843 0.1099 0.0623 0.0481 ... ## $ V4 : num 0.0207 0.0689 0.1083 0.0205 0.0394 ... ## $ V5 : num 0.0954 0.1183 0.0974 0.0205 0.059 ... ## $ V6 : num 0.0986 0.2583 0.228 0.0368 0.0649 ... ## $ V7 : num 0.154 0.216 0.243 0.11 0.121 ... ## $ V8 : num 0.16 0.348 0.377 0.128 0.247 ... ## $ V9 : num 0.3109 0.3337 0.5598 0.0598 0.3564 ... ## $ V10: num 0.211 0.287 0.619 0.126 0.446 ... The function createDataPartition can be used to create a stratified random sample of the data into training and test sets: library(caret) set.seed(998) inTraining &lt;- createDataPartition(Sonar$Class, p = .75, list = FALSE) training &lt;- Sonar[ inTraining,] testing &lt;- Sonar[-inTraining,] We will use these data illustrate functionality on this (and other) pages. 5.3 Basic Parameter Tuning By default, simple bootstrap resampling is used for line 3 in the algorithm above. Others are availible, such as repeated K-fold cross-validation, leave-one-out etc. The function trainControl can be used to specifiy the type of resampling: fitControl &lt;- trainControl(## 10-fold CV method = &quot;repeatedcv&quot;, number = 10, ## repeated ten times repeats = 10) More information about trainControl is given in a section below. The first two arguments to train are the predictor and outcome data objects, respectively. The third argument, method, specifies the type of model (see train Model List or train Models By Tag). To illustrate, we will fit a boosted tree model via the gbm package. The basic syntax for fitting this model using repeated cross-validation is shown below: set.seed(825) gbmFit1 &lt;- train(Class ~ ., data = training, method = &quot;gbm&quot;, trControl = fitControl, ## This last option is actually one ## for gbm() that passes through verbose = FALSE) gbmFit1 ## Stochastic Gradient Boosting ## ## 157 samples ## 60 predictor ## 2 classes: &#39;M&#39;, &#39;R&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 10 times) ## Summary of sample sizes: 142, 142, 140, 142, 142, 141, ... ## Resampling results across tuning parameters: ## ## interaction.depth n.trees Accuracy Kappa ## 1 50 0.7609191 0.5163703 ## 1 100 0.7934216 0.5817734 ## 1 150 0.7977230 0.5897796 ## 2 50 0.7858235 0.5667749 ## 2 100 0.8188897 0.6316548 ## 2 150 0.8194363 0.6329037 ## 3 50 0.7895686 0.5726290 ## 3 100 0.8130564 0.6195719 ## 3 150 0.8221348 0.6383441 ## ## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1 ## ## Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 10 ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were n.trees = 150, ## interaction.depth = 3, shrinkage = 0.1 and n.minobsinnode = 10. For a gradient boosting machine (GBM) model, there are three main tuning parameters: number of iterations, i.e. trees, (called n.trees in the gbm function) complexity of the tree, called interaction.depth learning rate: how quickly the algorithm adapts, called shrinkage the minimum number of training set samples in a node to commence splitting (n.minobsinnode) The default values tested for this model are shown in the first two columns (shrinkage and n.minobsinnode are not shown beause the grid set of candidate models all use a single value for these tuning parameters). The column labeled “Accuracy” is the overall agreement rate averaged over cross-validation iterations. The agreement standard deviation is also calculated from the cross-validation results. The column “Kappa” is Cohen’s (unweighted) Kappa statistic averaged across the resampling results. train works with specific models (see train Model List or train Models By Tag). For these models, train can automatically create a grid of tuning parameters. By default, if p is the number of tuning parameters, the grid size is 3^p. As another example, regularized discriminant analysis (RDA) models have two parameters (gamma and lambda), both of which lie between zero and one. The default training grid would produce nine combinations in this two-dimensional space. There are several notes regarding specific model behaviors for train. There is additional functionality in train that is described in the next section. 5.4 Notes on Reproducibility Many models utilize random numbers during the phase where parameters are estimated. Also, the resampling indices are chosen using random numbers. There are two main ways to control the randomness in order to assure reproducible results. There are two approaches to ensuring that the same resamples are used between calls to train. The first is to use set.seed just prior to calling train. The first use of random numbers is to create the resampling information. Alternatively, if you would like to use specific splits of the data, the index argument of the trainControl function can be used. This is briefly discussed below. When the models are created inside of resampling, the seeds can also be set. While setting the seed prior to calling train may guarantee that the same random numbers are used, this is unlikely to be the case when parallel processing is used (depending which technology is utilized). To set the model fitting seeds, trainControl has an additional argument called seeds that can be used. The value for this argument is a list of integer vectors that are used as seeds. The help page for trainControl describes the appropriate format for this option. How random numbers are used is highly dependent on the package author. There are rare cases where the underlying model function does not control the random number seed, especially if the computations are conducted in C code. Also, please note that some packages load random numbers when loaded (directly or via namespace) and this may effect reproducibility. 5.5 Customizing the Tuning Process There are a few ways to customize the process of selecting tuning/complexity parameters and building the final model. 5.5.1 Pre-Processing Options As previously mentioned,train can pre-process the data in various ways prior to model fitting. The function preProcess is automatically used. This function can be used for centering and scaling, imputation (see details below), applying the spatial sign transformation and feature extraction via principal component analysis or independent component analysis. To specify what pre-processing should occur, the train function has an argument called preProcess. This argument takes a character string of methods that would normally be passed to the method argument of the preProcess function. Additional options to the preProcess function can be passed via the trainControl function. These processing steps would be applied during any predictions generated using predict.train, extractPrediction or extractProbs (see details later in this document). The pre-processing would not be applied to predictions that directly use the object$finalModel object. For imputation, there are three methods currently implemented: k-nearest neighbors takes a sample with missing values and finds the k closest samples in the training set. The average of the k training set values for that predictor are used as a substitute for the original data. When calculating the distances to the training set samples, the predictors used in the calculation are the ones with no missing values for that sample and no missing values in the training set. another approach is to fit a bagged tree model for each predictor using the training set samples. This is usually a fairly accurate model and can handle missing values. When a predictor for a sample requires imputation, the values for the other predictors are fed through the bagged tree and the prediction is used as the new value. This model can have significant computational cost. the median of the predictor’s training set values can be used to estimate the missing data. If there are missing values in the training set, PCA and ICA models only use complete samples. 5.5.2 Alternate Tuning Grids The tuning parameter grid can be specified by the user. The argument tuneGrid can take a data frame with columns for each tuning parameter. The column names should be the same as the fitting function’s arguments. For the previously mentioned RDA example, the names would be gamma and lambda. train will tune the model over each combination of values in the rows. For the boosted tree model, we can fix the learning rate and evaluate more than three values of n.trees: gbmGrid &lt;- expand.grid(interaction.depth = c(1, 5, 9), n.trees = (1:30)*50, shrinkage = 0.1, n.minobsinnode = 20) nrow(gbmGrid) set.seed(825) gbmFit2 &lt;- train(Class ~ ., data = training, method = &quot;gbm&quot;, trControl = fitControl, verbose = FALSE, ## Now specify the exact models ## to evaluate: tuneGrid = gbmGrid) gbmFit2 ## Stochastic Gradient Boosting ## ## 157 samples ## 60 predictor ## 2 classes: &#39;M&#39;, &#39;R&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 10 times) ## Summary of sample sizes: 142, 142, 140, 142, 142, 141, ... ## Resampling results across tuning parameters: ## ## interaction.depth n.trees Accuracy Kappa ## 1 50 0.75 0.50 ## 1 100 0.78 0.55 ## 1 150 0.79 0.58 ## 1 200 0.80 0.60 ## 1 250 0.80 0.60 ## 1 300 0.80 0.60 ## : : : : ## 9 1350 0.82 0.64 ## 9 1400 0.82 0.64 ## 9 1450 0.82 0.64 ## 9 1500 0.82 0.64 ## ## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1 ## ## Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 20 ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were n.trees = 1000, ## interaction.depth = 5, shrinkage = 0.1 and n.minobsinnode = 20. If there are missing values in the training set, PCA and ICA models only use complete samples. Another option is to use a random sample of possible tuning parameter combinations, i.e. “random search”(pdf). This functionality is described on this page. To use a random search, use the option search = &quot;random&quot; in the call to trainControl. In this situation, the tuneLength parameter defines the total number of parameter combinations that will be evaluated. 5.5.3 Plotting the Resampling Profile The plot function can be used to examine the relationship between the estimates of performance and the tuning parameters. For example, a simple invokation of the function shows the results for the first performance measure: trellis.par.set(caretTheme()) plot(gbmFit2) Other performance metrics can be shown using the metric option: trellis.par.set(caretTheme()) plot(gbmFit2, metric = &quot;Kappa&quot;) Other types of plot are also available. See ?plot.train for more details. The code below shows a heatmap of the results: trellis.par.set(caretTheme()) plot(gbmFit2, metric = &quot;Kappa&quot;, plotType = &quot;level&quot;, scales = list(x = list(rot = 90))) A ggplot method can also be used: ggplot(gbmFit2) ## Warning: Ignoring unknown aesthetics: shape There are also plot functions that show more detailed representations of the resampled estimates. See ?xyplot.train for more details. From these plots, a different set of tuning parameters may be desired. To change the final values without starting the whole process again, the update.train can be used to refit the final model. See ?update.train 5.5.4 The trainControl Function The function trainControl generates parameters that further control how models are created, with possible values: method: The resampling method: &quot;boot&quot;, &quot;cv&quot;, &quot;LOOCV&quot;, &quot;LGOCV&quot;, &quot;repeatedcv&quot;, &quot;timeslice&quot;, &quot;none&quot; and &quot;oob&quot;. The last value, out-of-bag estimates, can only be used by random forest, bagged trees, bagged earth, bagged flexible discriminant analysis, or conditional tree forest models. GBM models are not included (the gbm package maintainer has indicated that it would not be a good idea to choose tuning parameter values based on the model OOB error estimates with boosted trees). Also, for leave-one-out cross-validation, no uncertainty estimates are given for the resampled performance measures. number and repeats: number controls with the number of folds in K-fold cross-validation or number of resampling iterations for bootstrapping and leave-group-out cross-validation. repeats applied only to repeated K-fold cross-validation. Suppose that method = &quot;repeatedcv&quot;, number = 10 and repeats = 3,then three separate 10-fold cross-validations are used as the resampling scheme. verboseIter: A logical for printing a training log. returnData: A logical for saving the data into a slot called trainingData. p: For leave-group out cross-validation: the training percentage For method = &quot;timeslice&quot;, trainControl has options initialWindow, horizon and fixedWindow that govern how cross-validation can be used for time series data. classProbs: a logical value determining whether class probabilities should be computed for held-out samples during resample. index and indexOut: optional lists with elements for each resampling iteration. Each list element is the sample rows used for training at that iteration or should be held-out. When these values are not specified, train will generate them. summaryFunction: a function to computed alternate performance summaries. selectionFunction: a function to choose the optimal tuning parameters. and examples. PCAthresh, ICAcomp and k: these are all options to pass to the preProcess function (when used). returnResamp: a character string containing one of the following values: &quot;all&quot;, &quot;final&quot; or &quot;none&quot;. This specifies how much of the resampled performance measures to save. allowParallel: a logical that governs whether train should use parallel processing (if availible). There are several other options not discussed here. 5.5.5 Alternate Performance Metrics The user can change the metric used to determine the best settings. By default, RMSE and R2 are computed for regression while accuracy and Kappa are computed for classification. Also by default, the parameter values are chosen using RMSE and accuracy, respectively for regression and classification. The metric argument of the train function allows the user to control which the optimality criterion is used. For example, in problems where there are a low percentage of samples in one class, using metric = &quot;Kappa&quot; can improve quality of the final model. If none of these parameters are satisfactory, the user can also compute custom performance metrics. The trainControl function has a argument called summaryFunction that specifies a function for computing performance. The function should have these arguments: data is a reference for a data frame or matrix with columns called obs and pred for the observed and predicted outcome values (either numeric data for regression or character values for classification). Currently, class probabilities are not passed to the function. The values in data are the held-out predictions (and their associated reference values) for a single combination of tuning parameters. If the classProbs argument of the trainControl object is set to TRUE, additional columns in data will be present that contains the class probabilities. The names of these columns are the same as the class levels. Also, if weights were specified in the call to train, a column called weights will also be in the data set. lev is a character string that has the outcome factor levels taken from the training data. For regression, a value of NULL is passed into the function. model is a character string for the model being used (i.e. the value passed to the method argument of train). The output to the function should be a vector of numeric summary metrics with non-null names. By default, train evaluate classification models in terms of the predicted classes. Optionally, class probabilities can also be used to measure performance. To obtain predicted class probabilities within the resampling process, the argument classProbs in trainControl must be set to TRUE. This merges columns of probabilities into the predictions generated from each resample (there is a column per class and the column names are the class names). As shown in the last section, custom functions can be used to calculate performance scores that are averaged over the resamples. Another built-in function, twoClassSummary, will compute the sensitivity, specificity and area under the ROC curve: head(twoClassSummary) ## ## 1 function (data, lev = NULL, model = NULL) ## 2 { ## 3 lvls &lt;- levels(data$obs) ## 4 if (length(lvls) &gt; 2) ## 5 stop(paste(&quot;Your outcome has&quot;, length(lvls), &quot;levels. The twoClassSummary() function isn&#39;t appropriate.&quot;)) ## 6 requireNamespaceQuietStop(&quot;ModelMetrics&quot;) To rebuild the boosted tree model using this criterion, we can see the relationship between the tuning parameters and the area under the ROC curve using the following code: fitControl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 10, ## Estimate class probabilities classProbs = TRUE, ## Evaluate performance using ## the following function summaryFunction = twoClassSummary) set.seed(825) gbmFit3 &lt;- train(Class ~ ., data = training, method = &quot;gbm&quot;, trControl = fitControl, verbose = FALSE, tuneGrid = gbmGrid, ## Specify which metric to optimize metric = &quot;ROC&quot;) gbmFit3 ## Stochastic Gradient Boosting ## ## 157 samples ## 60 predictor ## 2 classes: &#39;M&#39;, &#39;R&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 10 times) ## Summary of sample sizes: 142, 142, 140, 142, 142, 141, ... ## Resampling results across tuning parameters: ## ## interaction.depth n.trees ROC Sens Spec ## 1 50 0.86 0.80 0.70 ## 1 100 0.87 0.82 0.73 ## 1 150 0.87 0.83 0.75 ## 1 200 0.87 0.84 0.76 ## 1 250 0.88 0.84 0.76 ## 1 300 0.88 0.84 0.76 ## : : : : : ## 9 1350 0.89 0.87 0.76 ## 9 1400 0.89 0.87 0.77 ## 9 1450 0.89 0.87 0.77 ## 9 1500 0.89 0.87 0.77 ## ## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1 ## ## Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 20 ## ROC was used to select the optimal model using the largest value. ## The final values used for the model were n.trees = 650, ## interaction.depth = 5, shrinkage = 0.1 and n.minobsinnode = 20. In this case, the average area under the ROC curve associated with the optimal tuning parameters was 0.896 across the 100 resamples. 5.6 Choosing the Final Model Another method for customizing the tuning process is to modify the algorithm that is used to select the “best” parameter values, given the performance numbers. By default, the train function chooses the model with the largest performance value (or smallest, for mean squared error in regression models). Other schemes for selecting model can be used. Breiman et al (1984) suggested the “one standard error rule” for simple tree-based models. In this case, the model with the best performance value is identified and, using resampling, we can estimate the standard error of performance. The final model used was the simplest model within one standard error of the (empirically) best model. With simple trees this makes sense, since these models will start to over-fit as they become more and more specific to the training data. train allows the user to specify alternate rules for selecting the final model. The argument selectionFunction can be used to supply a function to algorithmically determine the final model. There are three existing functions in the package: best is chooses the largest/smallest value, oneSE attempts to capture the spirit of Breiman et al (1984) and tolerance selects the least complex model within some percent tolerance of the best value. See ?best for more details. User-defined functions can be used, as long as they have the following arguments: x is a data frame containing the tune parameters and their associated performance metrics. Each row corresponds to a different tuning parameter combination. metric a character string indicating which performance metric should be optimized (this is passed in directly from the metric argument of train. maximize is a single logical value indicating whether larger values of the performance metric are better (this is also directly passed from the call to train). The function should output a single integer indicating which row in x is chosen. As an example, if we chose the previous boosted tree model on the basis of overall accuracy, we would choose: n.trees = 650, interaction.depth = 5, shrinkage = 0.1, n.minobsinnode = 20. However, the scale in this plots is fairly tight, with accuracy values ranging from 0.859 to 0.896. A less complex model (e.g. fewer, more shallow trees) might also yield acceptable accuracy. The tolerance function could be used to find a less complex model based on (x-xbest)/xbestx 100, which is the percent difference. For example, to select parameter values based on a 2% loss of performance: whichTwoPct &lt;- tolerance(gbmFit3$results, metric = &quot;ROC&quot;, tol = 2, maximize = TRUE) cat(&quot;best model within 2 pct of best:\\n&quot;) ## best model within 2 pct of best: gbmFit3$results[whichTwoPct,1:6] ## shrinkage interaction.depth n.minobsinnode n.trees ROC Sens ## 31 0.1 5 20 50 0.8809623 0.8348611 This indicates that we can get a less complex model with an area under the ROC curve of 0.881 (compared to the “pick the best” value of 0.896). The main issue with these functions is related to ordering the models from simplest to complex. In some cases, this is easy (e.g. simple trees, partial least squares), but in cases such as this model, the ordering of models is subjective. For example, is a boosted tree model using 100 iterations and a tree depth of 2 more complex than one with 50 iterations and a depth of 8? The package makes some choices regarding the orderings. In the case of boosted trees, the package assumes that increasing the number of iterations adds complexity at a faster rate than increasing the tree depth, so models are ordered on the number of iterations then ordered with depth. See ?best for more examples for specific models. 5.7 Extracting Predictions and Class Probabilities As previously mentioned, objects produced by the train function contain the “optimized” model in the finalModel sub-object. Predictions can be made from these objects as usual. In some cases, such as pls or gbm objects, additional parameters from the optimized fit may need to be specified. In these cases, the train objects uses the results of the parameter optimization to predict new samples. For example, if predictions were create using predict.gbm, the user would have to specify the number of trees directly (there is no default). Also, for binary classification, the predictions from this function take the form of the probability of one of the classes, so extra steps are required to convert this to a factor vector. predict.train automatically handles these details for this (and for other models). Also, there are very few standard syntaxes for model predictions in R. For example, to get class probabilities, many predict methods have an argument called type that is used to specify whether the classes or probabilities should be generated. Different packages use different values of type, such as &quot;prob&quot;, &quot;posterior&quot;, &quot;response&quot;, &quot;probability&quot; or &quot;raw&quot;. In other cases, completely different syntax is used. For predict.train, the type options are standardized to be &quot;class&quot; and &quot;prob&quot; (the underlying code matches these to the appropriate choices for each model. For example: predict(gbmFit3, newdata = head(testing)) ## [1] R R R R M M ## Levels: M R predict(gbmFit3, newdata = head(testing), type = &quot;prob&quot;) ## M R ## 1 9.799645e-04 0.9990200355 ## 2 1.825908e-04 0.9998174092 ## 3 5.373401e-08 0.9999999463 ## 4 1.693365e-03 0.9983066351 ## 5 9.999348e-01 0.0000651877 ## 6 9.862454e-01 0.0137546480 5.8 Exploring and Comparing Resampling Distributions 5.8.1 Within-Model There are several lattice functions than can be used to explore relationships between tuning parameters and the resampling results for a specific model: xyplot and stripplot can be used to plot resampling statistics against (numeric) tuning parameters. histogram and densityplot can also be used to look at distributions of the tuning parameters across tuning parameters. For example, the following statements create a density plot: Note that if you are interested in plotting the resampling results across multiple tuning parameters, the option resamples = &quot;all&quot; should be used in the control object. 5.8.2 Between-Models The caret package also includes functions to characterize the differences between models (generated using train, sbf or rfe) via their resampling distributions. These functions are based on the work of Hothorn et al. (2005) and Eugster et al (2008). First, a support vector machine model is fit to the Sonar data. The data are centered and scaled using the preProc argument. Note that the same random number seed is set prior to the model that is identical to the seed used for the boosted tree model. This ensures that the same resampling sets are used, which will come in handy when we compare the resampling profiles between models. set.seed(825) svmFit &lt;- train(Class ~ ., data = training, method = &quot;svmRadial&quot;, trControl = fitControl, preProc = c(&quot;center&quot;, &quot;scale&quot;), tuneLength = 8, metric = &quot;ROC&quot;) svmFit ## Support Vector Machines with Radial Basis Function Kernel ## ## 157 samples ## 60 predictor ## 2 classes: &#39;M&#39;, &#39;R&#39; ## ## Pre-processing: centered (60), scaled (60) ## Resampling: Cross-Validated (10 fold, repeated 10 times) ## Summary of sample sizes: 142, 142, 140, 142, 142, 141, ... ## Resampling results across tuning parameters: ## ## C ROC Sens Spec ## 0.25 0.8672371 0.7413889 0.7466071 ## 0.50 0.9030134 0.8326389 0.7794643 ## 1.00 0.9221577 0.8700000 0.7748214 ## 2.00 0.9318601 0.8902778 0.7714286 ## 4.00 0.9373735 0.8881944 0.7998214 ## 8.00 0.9442411 0.9061111 0.8125000 ## 16.00 0.9445164 0.9173611 0.8126786 ## 32.00 0.9445164 0.9123611 0.8166071 ## ## Tuning parameter &#39;sigma&#39; was held constant at a value of 0.0115025 ## ROC was used to select the optimal model using the largest value. ## The final values used for the model were sigma = 0.0115025 and C = 16. Also, a regularized discriminant analysis model was fit. set.seed(825) rdaFit &lt;- train(Class ~ ., data = training, method = &quot;rda&quot;, trControl = fitControl, tuneLength = 4, metric = &quot;ROC&quot;) rdaFit ## Regularized Discriminant Analysis ## ## 157 samples ## 60 predictor ## 2 classes: &#39;M&#39;, &#39;R&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 10 times) ## Summary of sample sizes: 142, 142, 140, 142, 142, 141, ... ## Resampling results across tuning parameters: ## ## gamma lambda ROC Sens Spec ## 0.0000000 0.0000000 0.6768564 0.9244444 0.3951786 ## 0.0000000 0.3333333 0.8356200 0.8441667 0.7278571 ## 0.0000000 0.6666667 0.8578894 0.8248611 0.7798214 ## 0.0000000 1.0000000 0.8487103 0.7754167 0.7653571 ## 0.3333333 0.0000000 0.8934573 0.8688889 0.7478571 ## 0.3333333 0.3333333 0.9130853 0.8987500 0.7803571 ## 0.3333333 0.6666667 0.9079216 0.9109722 0.7692857 ## 0.3333333 1.0000000 0.8667510 0.8304167 0.7760714 ## 0.6666667 0.0000000 0.8856101 0.8704167 0.7326786 ## 0.6666667 0.3333333 0.8935640 0.8920833 0.7289286 ## 0.6666667 0.6666667 0.8869692 0.8833333 0.7416071 ## 0.6666667 1.0000000 0.8560020 0.7868056 0.7728571 ## 1.0000000 0.0000000 0.7192237 0.6590278 0.6460714 ## 1.0000000 0.3333333 0.7215253 0.6591667 0.6487500 ## 1.0000000 0.6666667 0.7226687 0.6615278 0.6487500 ## 1.0000000 1.0000000 0.7242485 0.6661111 0.6462500 ## ## ROC was used to select the optimal model using the largest value. ## The final values used for the model were gamma = 0.3333333 and lambda ## = 0.3333333. Given these models, can we make statistical statements about their performance differences? To do this, we first collect the resampling results using resamples. resamps &lt;- resamples(list(GBM = gbmFit3, SVM = svmFit, RDA = rdaFit)) resamps ## ## Call: ## resamples.default(x = list(GBM = gbmFit3, SVM = svmFit, RDA = rdaFit)) ## ## Models: GBM, SVM, RDA ## Number of resamples: 100 ## Performance metrics: ROC, Sens, Spec ## Time estimates for: everything, final model fit summary(resamps) ## ## Call: ## summary.resamples(object = resamps) ## ## Models: GBM, SVM, RDA ## Number of resamples: 100 ## ## ROC ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## GBM 0.5179 0.8571 0.9048 0.8956 0.9479 1 0 ## SVM 0.6786 0.9107 0.9557 0.9445 0.9844 1 0 ## RDA 0.6032 0.8750 0.9219 0.9131 0.9643 1 0 ## ## Sens ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## GBM 0.625 0.7778 0.8750 0.8679 1 1 0 ## SVM 0.500 0.8750 0.8889 0.9174 1 1 0 ## RDA 0.625 0.8750 0.8889 0.8988 1 1 0 ## ## Spec ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## GBM 0.2857 0.7143 0.7500 0.7664 0.8571 1 0 ## SVM 0.2857 0.7143 0.8571 0.8127 1.0000 1 0 ## RDA 0.2857 0.7143 0.7500 0.7804 0.8571 1 0 Note that, in this case, the option resamples = &quot;final&quot; should be user-defined in the control objects. There are several lattice plot methods that can be used to visualize the resampling distributions: density plots, box-whisker plots, scatterplot matrices and scatterplots of summary statistics. For example: trellis.par.set(theme1) bwplot(resamps, layout = c(3, 1)) trellis.par.set(caretTheme()) dotplot(resamps, metric = &quot;ROC&quot;) trellis.par.set(theme1) xyplot(resamps, what = &quot;BlandAltman&quot;) splom(resamps) Other visualizations are availible in densityplot.resamples and parallel.resamples Since models are fit on the same versions of the training data, it makes sense to make inferences on the differences between models. In this way we reduce the within-resample correlation that may exist. We can compute the differences, then use a simple t-test to evaluate the null hypothesis that there is no difference between models. difValues &lt;- diff(resamps) difValues ## ## Call: ## diff.resamples(x = resamps) ## ## Models: GBM, SVM, RDA ## Metrics: ROC, Sens, Spec ## Number of differences: 3 ## p-value adjustment: bonferroni summary(difValues) ## ## Call: ## summary.diff.resamples(object = difValues) ## ## p-value adjustment: bonferroni ## Upper diagonal: estimates of the difference ## Lower diagonal: p-value for H0: difference = 0 ## ## ROC ## GBM SVM RDA ## GBM -0.04896 -0.01753 ## SVM 1.168e-10 0.03143 ## RDA 0.1616 3.835e-05 ## ## Sens ## GBM SVM RDA ## GBM -0.04944 -0.03083 ## SVM 0.0002316 0.01861 ## RDA 0.1244317 0.3697745 ## ## Spec ## GBM SVM RDA ## GBM -0.04625 -0.01393 ## SVM 0.01577 0.03232 ## RDA 1.00000 0.13861 trellis.par.set(theme1) bwplot(difValues, layout = c(3, 1)) trellis.par.set(caretTheme()) dotplot(difValues) 5.9 Fitting Models Without Parameter Tuning In cases where the model tuning values are known, train can be used to fit the model to the entire training set without any resampling or parameter tuning. Using the method = &quot;none&quot; option in trainControl can be used. For example: fitControl &lt;- trainControl(method = &quot;none&quot;, classProbs = TRUE) set.seed(825) gbmFit4 &lt;- train(Class ~ ., data = training, method = &quot;gbm&quot;, trControl = fitControl, verbose = FALSE, ## Only a single model can be passed to the ## function when no resampling is used: tuneGrid = data.frame(interaction.depth = 4, n.trees = 100, shrinkage = .1, n.minobsinnode = 20), metric = &quot;ROC&quot;) gbmFit4 ## Stochastic Gradient Boosting ## ## 157 samples ## 60 predictor ## 2 classes: &#39;M&#39;, &#39;R&#39; ## ## No pre-processing ## Resampling: None Note that plot.train, resamples, confusionMatrix.train and several other functions will not work with this object but predict.train and others will: predict(gbmFit4, newdata = head(testing)) ## [1] R R R R M M ## Levels: M R predict(gbmFit4, newdata = head(testing), type = &quot;prob&quot;) ## M R ## 1 0.07043641 0.92956359 ## 2 0.02921858 0.97078142 ## 3 0.01156062 0.98843938 ## 4 0.36436834 0.63563166 ## 5 0.92596513 0.07403487 ## 6 0.82897570 0.17102430 "],
["available-models.html", "6 Available Models", " 6 Available Models The models below are available in train. The code behind these protocols can be obtained using the function getModelInfo or by going to the github repository. "],
["train-models-by-tag.html", "7 train Models By Tag", " 7 train Models By Tag The following is a basic list of model types or relevant characteristics. There entires in these lists are arguable. For example: random forests theoretically use feature selection but effectively may not, support vector machines use L2 regularization etc. Contents Accepts Case Weights Bagging Bayesian Model Binary Predictors Only Boosting Categorical Predictors Only Cost Sensitive Learning Discriminant Analysis Distance Weighted Discrimination Ensemble Model Feature Extraction Feature Selection Wrapper Gaussian Process Generalized Additive Model Generalized Linear Model Handle Missing Predictor Data Implicit Feature Selection Kernel Method L1 Regularization L2 Regularization Linear Classifier Linear Regression Logic Regression Logistic Regression Mixture Model Model Tree Multivariate Adaptive Regression Splines Neural Network Oblique Tree Ordinal Outcomes Partial Least Squares Polynomial Model Prototype Models Quantile Regression Radial Basis Function Random Forest Regularization Relevance Vector Machines Ridge Regression Robust Methods Robust Model ROC Curves Rule-Based Model Self-Organising Maps String Kernel Support Vector Machines Text Mining Tree-Based Model Two Class Only 7.0.1 Accepts Case Weights (back to contents) Adjacent Categories Probability Model for Ordinal Data method = &#39;vglmAdjCat&#39; Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM Bagged CART method = &#39;treebag&#39; Type: Regression, Classification No tuning parameters for this model Required packages: ipred, plyr, e1071 A model-specific variable importance metric is available. Bagged Flexible Discriminant Analysis method = &#39;bagFDA&#39; Type: Classification Tuning parameters: degree (Product Degree) nprune (#Terms) Required packages: earth, mda A model-specific variable importance metric is available. Bagged MARS method = &#39;bagEarth&#39; Type: Regression, Classification Tuning parameters: nprune (#Terms) degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Bagged MARS using gCV Pruning method = &#39;bagEarthGCV&#39; Type: Regression, Classification Tuning parameters: degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Bayesian Generalized Linear Model method = &#39;bayesglm&#39; Type: Regression, Classification No tuning parameters for this model Required packages: arm Boosted Generalized Additive Model method = &#39;gamboost&#39; Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: mboost, plyr Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mstop. If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. Boosted Generalized Linear Model method = &#39;glmboost&#39; Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: plyr, mboost A model-specific variable importance metric is available. Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mstop. If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. Boosted Tree method = &#39;blackboost&#39; Type: Regression, Classification Tuning parameters: mstop (#Trees) maxdepth (Max Tree Depth) Required packages: party, mboost, plyr C5.0 method = &#39;C5.0&#39; Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) Required packages: C50, plyr A model-specific variable importance metric is available. CART method = &#39;rpart&#39; Type: Regression, Classification Tuning parameters: cp (Complexity Parameter) Required packages: rpart A model-specific variable importance metric is available. CART method = &#39;rpart1SE&#39; Type: Regression, Classification No tuning parameters for this model Required packages: rpart A model-specific variable importance metric is available. Notes: This CART model replicates the same process used by the rpart function where the model complexity is determined using the one-standard error method. This procedure is replicated inside of the resampling done by train so that an external resampling estimate can be obtained. CART method = &#39;rpart2&#39; Type: Regression, Classification Tuning parameters: maxdepth (Max Tree Depth) Required packages: rpart A model-specific variable importance metric is available. CART or Ordinal Responses method = &#39;rpartScore&#39; Type: Classification Tuning parameters: cp (Complexity Parameter) split (Split Function) prune (Pruning Measure) Required packages: rpartScore, plyr A model-specific variable importance metric is available. CHi-squared Automated Interaction Detection method = &#39;chaid&#39; Type: Classification Tuning parameters: alpha2 (Merging Threshold) alpha3 (Splitting former Merged Threshold) alpha4 ( Splitting former Merged Threshold) Required packages: CHAID Conditional Inference Random Forest method = &#39;cforest&#39; Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: party A model-specific variable importance metric is available. Conditional Inference Tree method = &#39;ctree&#39; Type: Classification, Regression Tuning parameters: mincriterion (1 - P-Value Threshold) Required packages: party Conditional Inference Tree method = &#39;ctree2&#39; Type: Regression, Classification Tuning parameters: maxdepth (Max Tree Depth) mincriterion (1 - P-Value Threshold) Required packages: party Continuation Ratio Model for Ordinal Data method = &#39;vglmContRatio&#39; Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM Cost-Sensitive C5.0 method = &#39;C5.0Cost&#39; Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) cost (Cost) Required packages: C50, plyr A model-specific variable importance metric is available. Cost-Sensitive CART method = &#39;rpartCost&#39; Type: Classification Tuning parameters: cp (Complexity Parameter) Cost (Cost) Required packages: rpart Cumulative Probability Model for Ordinal Data method = &#39;vglmCumulative&#39; Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM DeepBoost method = &#39;deepboost&#39; Type: Classification Tuning parameters: num_iter (# Boosting Iterations) tree_depth (Tree Depth) beta (L1 Regularization) lambda (Tree Depth Regularization) loss_type (Loss) Required packages: deepboost Flexible Discriminant Analysis method = &#39;fda&#39; Type: Classification Tuning parameters: degree (Product Degree) nprune (#Terms) Required packages: earth, mda A model-specific variable importance metric is available. Generalized Linear Model method = &#39;glm&#39; Type: Regression, Classification No tuning parameters for this model A model-specific variable importance metric is available. Generalized Linear Model with Stepwise Feature Selection method = &#39;glmStepAIC&#39; Type: Regression, Classification No tuning parameters for this model Required packages: MASS Linear Regression method = &#39;lm&#39; Type: Regression Tuning parameters: intercept (intercept) A model-specific variable importance metric is available. Linear Regression with Stepwise Selection method = &#39;lmStepAIC&#39; Type: Regression No tuning parameters for this model Required packages: MASS Model Averaged Neural Network method = &#39;avNNet&#39; Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) bag (Bagging) Required packages: nnet Multivariate Adaptive Regression Spline method = &#39;earth&#39; Type: Regression, Classification Tuning parameters: nprune (#Terms) degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Multivariate Adaptive Regression Splines method = &#39;gcvEarth&#39; Type: Regression, Classification Tuning parameters: degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Negative Binomial Generalized Linear Model method = &#39;glm.nb&#39; Type: Regression Tuning parameters: link (Link Function) A model-specific variable importance metric is available. Neural Network method = &#39;nnet&#39; Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) Required packages: nnet A model-specific variable importance metric is available. Neural Networks with Feature Extraction method = &#39;pcaNNet&#39; Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) Required packages: nnet Ordered Logistic or Probit Regression method = &#39;polr&#39; Type: Classification Tuning parameters: method (parameter) Required packages: MASS A model-specific variable importance metric is available. Penalized Discriminant Analysis method = &#39;pda&#39; Type: Classification Tuning parameters: lambda (Shrinkage Penalty Coefficient) Required packages: mda Penalized Discriminant Analysis method = &#39;pda2&#39; Type: Classification Tuning parameters: df (Degrees of Freedom) Required packages: mda Penalized Multinomial Regression method = &#39;multinom&#39; Type: Classification Tuning parameters: decay (Weight Decay) Required packages: nnet A model-specific variable importance metric is available. Projection Pursuit Regression method = &#39;ppr&#39; Type: Regression Tuning parameters: nterms (# Terms) Random Forest method = &#39;ranger&#39; Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: e1071, ranger A model-specific variable importance metric is available. Robust Linear Model method = &#39;rlm&#39; Type: Regression Tuning parameters: intercept (intercept) psi (psi) Required packages: MASS Single C5.0 Ruleset method = &#39;C5.0Rules&#39; Type: Classification No tuning parameters for this model Required packages: C50 A model-specific variable importance metric is available. Single C5.0 Tree method = &#39;C5.0Tree&#39; Type: Classification No tuning parameters for this model Required packages: C50 A model-specific variable importance metric is available. Stochastic Gradient Boosting method = &#39;gbm&#39; Type: Regression, Classification Tuning parameters: n.trees (# Boosting Iterations) interaction.depth (Max Tree Depth) shrinkage (Shrinkage) n.minobsinnode (Min. Terminal Node Size) Required packages: gbm, plyr A model-specific variable importance metric is available. Tree Models from Genetic Algorithms method = &#39;evtree&#39; Type: Regression, Classification Tuning parameters: alpha (Complexity Parameter) Required packages: evtree 7.0.2 Bagging (back to contents) Bagged AdaBoost method = &#39;AdaBag&#39; Type: Classification Tuning parameters: mfinal (#Trees) maxdepth (Max Tree Depth) Required packages: adabag, plyr A model-specific variable importance metric is available. Bagged CART method = &#39;treebag&#39; Type: Regression, Classification No tuning parameters for this model Required packages: ipred, plyr, e1071 A model-specific variable importance metric is available. Bagged Flexible Discriminant Analysis method = &#39;bagFDA&#39; Type: Classification Tuning parameters: degree (Product Degree) nprune (#Terms) Required packages: earth, mda A model-specific variable importance metric is available. Bagged Logic Regression method = &#39;logicBag&#39; Type: Regression, Classification Tuning parameters: nleaves (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: logicFS Bagged MARS method = &#39;bagEarth&#39; Type: Regression, Classification Tuning parameters: nprune (#Terms) degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Bagged MARS using gCV Pruning method = &#39;bagEarthGCV&#39; Type: Regression, Classification Tuning parameters: degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Bagged Model method = &#39;bag&#39; Type: Regression, Classification Tuning parameters: vars (#Randomly Selected Predictors) Required packages: caret Conditional Inference Random Forest method = &#39;cforest&#39; Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: party A model-specific variable importance metric is available. Ensembles of Generalized Lienar Models method = &#39;randomGLM&#39; Type: Regression, Classification Tuning parameters: maxInteractionOrder (Interaction Order) Required packages: randomGLM Model Averaged Neural Network method = &#39;avNNet&#39; Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) bag (Bagging) Required packages: nnet Parallel Random Forest method = &#39;parRF&#39; Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: e1071, randomForest, foreach A model-specific variable importance metric is available. Quantile Random Forest method = &#39;qrf&#39; Type: Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: quantregForest Quantile Regression Neural Network method = &#39;qrnn&#39; Type: Regression Tuning parameters: n.hidden (#Hidden Units) penalty ( Weight Decay) bag (Bagged Models?) Required packages: qrnn Random Ferns method = &#39;rFerns&#39; Type: Classification Tuning parameters: depth (Fern Depth) Required packages: rFerns Random Forest method = &#39;ranger&#39; Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: e1071, ranger A model-specific variable importance metric is available. Random Forest method = &#39;Rborist&#39; Type: Classification, Regression Tuning parameters: predFixed (#Randomly Selected Predictors) Required packages: Rborist A model-specific variable importance metric is available. Random Forest method = &#39;rf&#39; Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: randomForest A model-specific variable importance metric is available. Random Forest by Randomization method = &#39;extraTrees&#39; Type: Regression, Classification Tuning parameters: mtry (# Randomly Selected Predictors) numRandomCuts (# Random Cuts) Required packages: extraTrees Random Forest Rule-Based Model method = &#39;rfRules&#39; Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) maxdepth (Maximum Rule Depth) Required packages: randomForest, inTrees, plyr A model-specific variable importance metric is available. Regularized Random Forest method = &#39;RRF&#39; Type: Regression, Classification Tuning parameters: mtry (#Randomly Selected Predictors) coefReg (Regularization Value) coefImp (Importance Coefficient) Required packages: randomForest, RRF A model-specific variable importance metric is available. Regularized Random Forest method = &#39;RRFglobal&#39; Type: Regression, Classification Tuning parameters: mtry (#Randomly Selected Predictors) coefReg (Regularization Value) Required packages: RRF A model-specific variable importance metric is available. Weighted Subspace Random Forest method = &#39;wsrf&#39; Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: wsrf 7.0.3 Bayesian Model (back to contents) Bayesian Additive Regression Trees method = &#39;bartMachine&#39; Type: Classification, Regression Tuning parameters: num_trees (#Trees) k (Prior Boundary) alpha (Base Terminal Node Hyperparameter) beta (Power Terminal Node Hyperparameter) nu (Degrees of Freedom) Required packages: bartMachine A model-specific variable importance metric is available. Bayesian Generalized Linear Model method = &#39;bayesglm&#39; Type: Regression, Classification No tuning parameters for this model Required packages: arm Bayesian Regularized Neural Networks method = &#39;brnn&#39; Type: Regression Tuning parameters: neurons (# Neurons) Required packages: brnn Bayesian Ridge Regression method = &#39;bridge&#39; Type: Regression No tuning parameters for this model Required packages: monomvn Bayesian Ridge Regression (Model Averaged) method = &#39;blassoAveraged&#39; Type: Regression No tuning parameters for this model Required packages: monomvn Notes: This model makes predictions by averaging the predictions based on the posterior estimates of the regression coefficients. While it is possible that some of these posterior estimates are zero for non-informative predictors, the final predicted value may be a function of many (or even all) predictors. Model Averaged Naive Bayes Classifier method = &#39;manb&#39; Type: Classification Tuning parameters: smooth (Smoothing Parameter) prior (Prior Probability) Required packages: bnclassify Naive Bayes method = &#39;nb&#39; Type: Classification Tuning parameters: fL (Laplace Correction) usekernel (Distribution Type) adjust (Bandwidth Adjustment) Required packages: klaR Naive Bayes Classifier method = &#39;nbDiscrete&#39; Type: Classification Tuning parameters: smooth (Smoothing Parameter) Required packages: bnclassify Naive Bayes Classifier with Attribute Weighting method = &#39;awnb&#39; Type: Classification Tuning parameters: smooth (Smoothing Parameter) Required packages: bnclassify Semi-Naive Structure Learner Wrapper method = &#39;nbSearch&#39; Type: Classification Tuning parameters: k (#Folds) epsilon (Minimum Absolute Improvement) smooth (Smoothing Parameter) final_smooth (Final Smoothing Parameter) direction (Search Direction) Required packages: bnclassify Spike and Slab Regression method = &#39;spikeslab&#39; Type: Regression Tuning parameters: vars (Variables Retained) Required packages: spikeslab, plyr The Bayesian lasso method = &#39;blasso&#39; Type: Regression Tuning parameters: sparsity (Sparsity Threshold) Required packages: monomvn Notes: This model creates predictions using the mean of the posterior distributions but sets some parameters specifically to zero based on the tuning parameter sparsity. For example, when sparsity = .5, only coefficients where at least half the posterior estimates are nonzero are used. Tree Augmented Naive Bayes Classifier method = &#39;tan&#39; Type: Classification Tuning parameters: score (Score Function) smooth (Smoothing Parameter) Required packages: bnclassify Tree Augmented Naive Bayes Classifier Structure Learner Wrapper method = &#39;tanSearch&#39; Type: Classification Tuning parameters: k (#Folds) epsilon (Minimum Absolute Improvement) smooth (Smoothing Parameter) final_smooth (Final Smoothing Parameter) sp (Super-Parent) Required packages: bnclassify Tree Augmented Naive Bayes Classifier with Attribute Weighting method = &#39;awtan&#39; Type: Classification Tuning parameters: score (Score Function) smooth (Smoothing Parameter) Required packages: bnclassify Variational Bayesian Multinomial Probit Regression method = &#39;vbmpRadial&#39; Type: Classification Tuning parameters: estimateTheta (Theta Estimated) Required packages: vbmp 7.0.4 Binary Predictors Only (back to contents) Bagged Logic Regression method = &#39;logicBag&#39; Type: Regression, Classification Tuning parameters: nleaves (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: logicFS Binary Discriminant Analysis method = &#39;binda&#39; Type: Classification Tuning parameters: lambda.freqs (Shrinkage Intensity) Required packages: binda Logic Regression method = &#39;logreg&#39; Type: Regression, Classification Tuning parameters: treesize (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: LogicReg 7.0.5 Boosting (back to contents) AdaBoost Classification Trees method = &#39;adaboost&#39; Type: Classification Tuning parameters: nIter (#Trees) method (Method) Required packages: fastAdaboost AdaBoost.M1 method = &#39;AdaBoost.M1&#39; Type: Classification Tuning parameters: mfinal (#Trees) maxdepth (Max Tree Depth) coeflearn (Coefficient Type) Required packages: adabag, plyr A model-specific variable importance metric is available. Bagged AdaBoost method = &#39;AdaBag&#39; Type: Classification Tuning parameters: mfinal (#Trees) maxdepth (Max Tree Depth) Required packages: adabag, plyr A model-specific variable importance metric is available. Boosted Classification Trees method = &#39;ada&#39; Type: Classification Tuning parameters: iter (#Trees) maxdepth (Max Tree Depth) nu (Learning Rate) Required packages: ada, plyr Boosted Generalized Additive Model method = &#39;gamboost&#39; Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: mboost, plyr Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mstop. If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. Boosted Generalized Linear Model method = &#39;glmboost&#39; Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: plyr, mboost A model-specific variable importance metric is available. Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mstop. If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. Boosted Linear Model method = &#39;BstLm&#39; Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) nu (Shrinkage) Required packages: bst, plyr Boosted Logistic Regression method = &#39;LogitBoost&#39; Type: Classification Tuning parameters: nIter (# Boosting Iterations) Required packages: caTools Boosted Smoothing Spline method = &#39;bstSm&#39; Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) nu (Shrinkage) Required packages: bst, plyr Boosted Tree method = &#39;blackboost&#39; Type: Regression, Classification Tuning parameters: mstop (#Trees) maxdepth (Max Tree Depth) Required packages: party, mboost, plyr Boosted Tree method = &#39;bstTree&#39; Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) maxdepth (Max Tree Depth) nu (Shrinkage) Required packages: bst, plyr C5.0 method = &#39;C5.0&#39; Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) Required packages: C50, plyr A model-specific variable importance metric is available. Cost-Sensitive C5.0 method = &#39;C5.0Cost&#39; Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) cost (Cost) Required packages: C50, plyr A model-specific variable importance metric is available. Cubist method = &#39;cubist&#39; Type: Regression Tuning parameters: committees (#Committees) neighbors (#Instances) Required packages: Cubist A model-specific variable importance metric is available. DeepBoost method = &#39;deepboost&#39; Type: Classification Tuning parameters: num_iter (# Boosting Iterations) tree_depth (Tree Depth) beta (L1 Regularization) lambda (Tree Depth Regularization) loss_type (Loss) Required packages: deepboost eXtreme Gradient Boosting method = &#39;xgbLinear&#39; Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) lambda (L2 Regularization) alpha (L1 Regularization) eta (Learning Rate) Required packages: xgboost A model-specific variable importance metric is available. eXtreme Gradient Boosting method = &#39;xgbTree&#39; Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) max_depth (Max Tree Depth) eta (Shrinkage) gamma (Minimum Loss Reduction) colsample_bytree (Subsample Ratio of Columns) min_child_weight (Minimum Sum of Instance Weight) subsample (Subsample Percentage) Required packages: xgboost, plyr A model-specific variable importance metric is available. glmnet method = &#39;gbm_h2o&#39; Type: Regression, Classification Tuning parameters: ntrees (# Boosting Iterations) max_depth (Max Tree Depth) min_rows (Min. Terminal Node Size) learn_rate (Shrinkage) col_sample_rate (#Randomly Selected Predictors) Required packages: h2o A model-specific variable importance metric is available. Stochastic Gradient Boosting method = &#39;gbm&#39; Type: Regression, Classification Tuning parameters: n.trees (# Boosting Iterations) interaction.depth (Max Tree Depth) shrinkage (Shrinkage) n.minobsinnode (Min. Terminal Node Size) Required packages: gbm, plyr A model-specific variable importance metric is available. 7.0.6 Categorical Predictors Only (back to contents) Model Averaged Naive Bayes Classifier method = &#39;manb&#39; Type: Classification Tuning parameters: smooth (Smoothing Parameter) prior (Prior Probability) Required packages: bnclassify Naive Bayes Classifier method = &#39;nbDiscrete&#39; Type: Classification Tuning parameters: smooth (Smoothing Parameter) Required packages: bnclassify Naive Bayes Classifier with Attribute Weighting method = &#39;awnb&#39; Type: Classification Tuning parameters: smooth (Smoothing Parameter) Required packages: bnclassify Semi-Naive Structure Learner Wrapper method = &#39;nbSearch&#39; Type: Classification Tuning parameters: k (#Folds) epsilon (Minimum Absolute Improvement) smooth (Smoothing Parameter) final_smooth (Final Smoothing Parameter) direction (Search Direction) Required packages: bnclassify Tree Augmented Naive Bayes Classifier method = &#39;tan&#39; Type: Classification Tuning parameters: score (Score Function) smooth (Smoothing Parameter) Required packages: bnclassify Tree Augmented Naive Bayes Classifier Structure Learner Wrapper method = &#39;tanSearch&#39; Type: Classification Tuning parameters: k (#Folds) epsilon (Minimum Absolute Improvement) smooth (Smoothing Parameter) final_smooth (Final Smoothing Parameter) sp (Super-Parent) Required packages: bnclassify Tree Augmented Naive Bayes Classifier with Attribute Weighting method = &#39;awtan&#39; Type: Classification Tuning parameters: score (Score Function) smooth (Smoothing Parameter) Required packages: bnclassify 7.0.7 Cost Sensitive Learning (back to contents) Cost-Sensitive C5.0 method = &#39;C5.0Cost&#39; Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) cost (Cost) Required packages: C50, plyr A model-specific variable importance metric is available. Cost-Sensitive CART method = &#39;rpartCost&#39; Type: Classification Tuning parameters: cp (Complexity Parameter) Cost (Cost) Required packages: rpart L2 Regularized Linear Support Vector Machines with Class Weights method = &#39;svmLinearWeights2&#39; Type: Classification Tuning parameters: cost (Cost) Loss (Loss Function) weight (Class Weight) Required packages: LiblineaR Linear Support Vector Machines with Class Weights method = &#39;svmLinearWeights&#39; Type: Classification Tuning parameters: cost (Cost) weight (Class Weight) Required packages: e1071 Support Vector Machines with Class Weights method = &#39;svmRadialWeights&#39; Type: Classification Tuning parameters: sigma (Sigma) C (Cost) Weight (Weight) Required packages: kernlab 7.0.8 Discriminant Analysis (back to contents) Adaptive Mixture Discriminant Analysis method = &#39;amdai&#39; Type: Classification Tuning parameters: model (Model Type) Required packages: adaptDA Binary Discriminant Analysis method = &#39;binda&#39; Type: Classification Tuning parameters: lambda.freqs (Shrinkage Intensity) Required packages: binda Diagonal Discriminant Analysis method = &#39;dda&#39; Type: Classification Tuning parameters: model (Model) shrinkage (Shrinkage Type) Required packages: sparsediscrim Distance Weighted Discrimination with Polynomial Kernel method = &#39;dwdPoly&#39; Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) degree (Polynomial Degree) scale (Scale) Required packages: kerndwd Distance Weighted Discrimination with Radial Basis Function Kernel method = &#39;dwdRadial&#39; Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) sigma (Sigma) Required packages: kernlab, kerndwd Factor-Based Linear Discriminant Analysis method = &#39;RFlda&#39; Type: Classification Tuning parameters: q (# Factors) Required packages: HiDimDA Heteroscedastic Discriminant Analysis method = &#39;hda&#39; Type: Classification Tuning parameters: gamma (Gamma) lambda (Lambda) newdim (Dimension of the Discriminative Subspace) Required packages: hda High Dimensional Discriminant Analysis method = &#39;hdda&#39; Type: Classification Tuning parameters: threshold (Threshold) model (Model Type) Required packages: HDclassif High-Dimensional Regularized Discriminant Analysis method = &#39;hdrda&#39; Type: Classification Tuning parameters: gamma (Gamma) lambda (Lambda) shrinkage_type (Shrinkage Type) Required packages: sparsediscrim Linear Discriminant Analysis method = &#39;lda&#39; Type: Classification No tuning parameters for this model Required packages: MASS Linear Discriminant Analysis method = &#39;lda2&#39; Type: Classification Tuning parameters: dimen (#Discriminant Functions) Required packages: MASS Linear Discriminant Analysis with Stepwise Feature Selection method = &#39;stepLDA&#39; Type: Classification Tuning parameters: maxvar (Maximum #Variables) direction (Search Direction) Required packages: klaR, MASS Linear Distance Weighted Discrimination method = &#39;dwdLinear&#39; Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) Required packages: kerndwd Localized Linear Discriminant Analysis method = &#39;loclda&#39; Type: Classification Tuning parameters: k (#Nearest Neighbors) Required packages: klaR Maximum Uncertainty Linear Discriminant Analysis method = &#39;Mlda&#39; Type: Classification No tuning parameters for this model Required packages: HiDimDA Mixture Discriminant Analysis method = &#39;mda&#39; Type: Classification Tuning parameters: subclasses (#Subclasses Per Class) Required packages: mda Penalized Discriminant Analysis method = &#39;pda&#39; Type: Classification Tuning parameters: lambda (Shrinkage Penalty Coefficient) Required packages: mda Penalized Discriminant Analysis method = &#39;pda2&#39; Type: Classification Tuning parameters: df (Degrees of Freedom) Required packages: mda Penalized Linear Discriminant Analysis method = &#39;PenalizedLDA&#39; Type: Classification Tuning parameters: lambda (L1 Penalty) K (#Discriminant Functions) Required packages: penalizedLDA, plyr Quadratic Discriminant Analysis method = &#39;qda&#39; Type: Classification No tuning parameters for this model Required packages: MASS Quadratic Discriminant Analysis with Stepwise Feature Selection method = &#39;stepQDA&#39; Type: Classification Tuning parameters: maxvar (Maximum #Variables) direction (Search Direction) Required packages: klaR, MASS Regularized Discriminant Analysis method = &#39;rda&#39; Type: Classification Tuning parameters: gamma (Gamma) lambda (Lambda) Required packages: klaR Regularized Linear Discriminant Analysis method = &#39;rlda&#39; Type: Classification Tuning parameters: estimator (Regularization Method) Required packages: sparsediscrim Robust Linear Discriminant Analysis method = &#39;Linda&#39; Type: Classification No tuning parameters for this model Required packages: rrcov Robust Mixture Discriminant Analysis method = &#39;rmda&#39; Type: Classification Tuning parameters: K (#Subclasses Per Class) model (Model) Required packages: robustDA Robust Quadratic Discriminant Analysis method = &#39;QdaCov&#39; Type: Classification No tuning parameters for this model Required packages: rrcov Robust Regularized Linear Discriminant Analysis method = &#39;rrlda&#39; Type: Classification Tuning parameters: lambda (Penalty Parameter) hp (Robustness Parameter) penalty (Penalty Type) Required packages: rrlda Shrinkage Discriminant Analysis method = &#39;sda&#39; Type: Classification Tuning parameters: diagonal (Diagonalize) lambda (shrinkage) Required packages: sda Sparse Linear Discriminant Analysis method = &#39;sparseLDA&#39; Type: Classification Tuning parameters: NumVars (# Predictors) lambda (Lambda) Required packages: sparseLDA Sparse Mixture Discriminant Analysis method = &#39;smda&#39; Type: Classification Tuning parameters: NumVars (# Predictors) lambda (Lambda) R (# Subclasses) Required packages: sparseLDA Stabilized Linear Discriminant Analysis method = &#39;slda&#39; Type: Classification No tuning parameters for this model Required packages: ipred Stepwise Diagonal Linear Discriminant Analysis method = &#39;sddaLDA&#39; Type: Classification No tuning parameters for this model Required packages: SDDA Stepwise Diagonal Quadratic Discriminant Analysis method = &#39;sddaQDA&#39; Type: Classification No tuning parameters for this model Required packages: SDDA 7.0.9 Distance Weighted Discrimination (back to contents) Distance Weighted Discrimination with Polynomial Kernel method = &#39;dwdPoly&#39; Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) degree (Polynomial Degree) scale (Scale) Required packages: kerndwd Distance Weighted Discrimination with Radial Basis Function Kernel method = &#39;dwdRadial&#39; Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) sigma (Sigma) Required packages: kernlab, kerndwd Linear Distance Weighted Discrimination method = &#39;dwdLinear&#39; Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) Required packages: kerndwd Sparse Distance Weighted Discrimination method = &#39;sdwd&#39; Type: Classification Tuning parameters: lambda (L1 Penalty) lambda2 (L2 Penalty) Required packages: sdwd A model-specific variable importance metric is available. 7.0.10 Ensemble Model (back to contents) AdaBoost Classification Trees method = &#39;adaboost&#39; Type: Classification Tuning parameters: nIter (#Trees) method (Method) Required packages: fastAdaboost AdaBoost.M1 method = &#39;AdaBoost.M1&#39; Type: Classification Tuning parameters: mfinal (#Trees) maxdepth (Max Tree Depth) coeflearn (Coefficient Type) Required packages: adabag, plyr A model-specific variable importance metric is available. Bagged AdaBoost method = &#39;AdaBag&#39; Type: Classification Tuning parameters: mfinal (#Trees) maxdepth (Max Tree Depth) Required packages: adabag, plyr A model-specific variable importance metric is available. Bagged CART method = &#39;treebag&#39; Type: Regression, Classification No tuning parameters for this model Required packages: ipred, plyr, e1071 A model-specific variable importance metric is available. Bagged Flexible Discriminant Analysis method = &#39;bagFDA&#39; Type: Classification Tuning parameters: degree (Product Degree) nprune (#Terms) Required packages: earth, mda A model-specific variable importance metric is available. Bagged Logic Regression method = &#39;logicBag&#39; Type: Regression, Classification Tuning parameters: nleaves (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: logicFS Bagged MARS method = &#39;bagEarth&#39; Type: Regression, Classification Tuning parameters: nprune (#Terms) degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Bagged MARS using gCV Pruning method = &#39;bagEarthGCV&#39; Type: Regression, Classification Tuning parameters: degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Bagged Model method = &#39;bag&#39; Type: Regression, Classification Tuning parameters: vars (#Randomly Selected Predictors) Required packages: caret Boosted Classification Trees method = &#39;ada&#39; Type: Classification Tuning parameters: iter (#Trees) maxdepth (Max Tree Depth) nu (Learning Rate) Required packages: ada, plyr Boosted Generalized Additive Model method = &#39;gamboost&#39; Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: mboost, plyr Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mstop. If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. Boosted Generalized Linear Model method = &#39;glmboost&#39; Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: plyr, mboost A model-specific variable importance metric is available. Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mstop. If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. Boosted Linear Model method = &#39;BstLm&#39; Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) nu (Shrinkage) Required packages: bst, plyr Boosted Logistic Regression method = &#39;LogitBoost&#39; Type: Classification Tuning parameters: nIter (# Boosting Iterations) Required packages: caTools Boosted Smoothing Spline method = &#39;bstSm&#39; Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) nu (Shrinkage) Required packages: bst, plyr Boosted Tree method = &#39;blackboost&#39; Type: Regression, Classification Tuning parameters: mstop (#Trees) maxdepth (Max Tree Depth) Required packages: party, mboost, plyr Boosted Tree method = &#39;bstTree&#39; Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) maxdepth (Max Tree Depth) nu (Shrinkage) Required packages: bst, plyr C5.0 method = &#39;C5.0&#39; Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) Required packages: C50, plyr A model-specific variable importance metric is available. Conditional Inference Random Forest method = &#39;cforest&#39; Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: party A model-specific variable importance metric is available. Cost-Sensitive C5.0 method = &#39;C5.0Cost&#39; Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) cost (Cost) Required packages: C50, plyr A model-specific variable importance metric is available. Cubist method = &#39;cubist&#39; Type: Regression Tuning parameters: committees (#Committees) neighbors (#Instances) Required packages: Cubist A model-specific variable importance metric is available. DeepBoost method = &#39;deepboost&#39; Type: Classification Tuning parameters: num_iter (# Boosting Iterations) tree_depth (Tree Depth) beta (L1 Regularization) lambda (Tree Depth Regularization) loss_type (Loss) Required packages: deepboost Ensemble Partial Least Squares Regression method = &#39;enpls&#39; Type: Regression Tuning parameters: maxcomp (Max. #Components) Required packages: enpls Ensemble Partial Least Squares Regression with Feature Selection method = &#39;enpls.fs&#39; Type: Regression Tuning parameters: maxcomp (Max. #Components) threshold (Importance Cutoff) Required packages: enpls Ensembles of Generalized Lienar Models method = &#39;randomGLM&#39; Type: Regression, Classification Tuning parameters: maxInteractionOrder (Interaction Order) Required packages: randomGLM eXtreme Gradient Boosting method = &#39;xgbLinear&#39; Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) lambda (L2 Regularization) alpha (L1 Regularization) eta (Learning Rate) Required packages: xgboost A model-specific variable importance metric is available. eXtreme Gradient Boosting method = &#39;xgbTree&#39; Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) max_depth (Max Tree Depth) eta (Shrinkage) gamma (Minimum Loss Reduction) colsample_bytree (Subsample Ratio of Columns) min_child_weight (Minimum Sum of Instance Weight) subsample (Subsample Percentage) Required packages: xgboost, plyr A model-specific variable importance metric is available. glmnet method = &#39;gbm_h2o&#39; Type: Regression, Classification Tuning parameters: ntrees (# Boosting Iterations) max_depth (Max Tree Depth) min_rows (Min. Terminal Node Size) learn_rate (Shrinkage) col_sample_rate (#Randomly Selected Predictors) Required packages: h2o A model-specific variable importance metric is available. Model Averaged Neural Network method = &#39;avNNet&#39; Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) bag (Bagging) Required packages: nnet Oblique Random Forest method = &#39;ORFlog&#39; Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Oblique Random Forest method = &#39;ORFpls&#39; Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Oblique Random Forest method = &#39;ORFridge&#39; Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Oblique Random Forest method = &#39;ORFsvm&#39; Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Parallel Random Forest method = &#39;parRF&#39; Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: e1071, randomForest, foreach A model-specific variable importance metric is available. Quantile Random Forest method = &#39;qrf&#39; Type: Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: quantregForest Quantile Regression Neural Network method = &#39;qrnn&#39; Type: Regression Tuning parameters: n.hidden (#Hidden Units) penalty ( Weight Decay) bag (Bagged Models?) Required packages: qrnn Random Ferns method = &#39;rFerns&#39; Type: Classification Tuning parameters: depth (Fern Depth) Required packages: rFerns Random Forest method = &#39;ranger&#39; Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: e1071, ranger A model-specific variable importance metric is available. Random Forest method = &#39;Rborist&#39; Type: Classification, Regression Tuning parameters: predFixed (#Randomly Selected Predictors) Required packages: Rborist A model-specific variable importance metric is available. Random Forest method = &#39;rf&#39; Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: randomForest A model-specific variable importance metric is available. Random Forest by Randomization method = &#39;extraTrees&#39; Type: Regression, Classification Tuning parameters: mtry (# Randomly Selected Predictors) numRandomCuts (# Random Cuts) Required packages: extraTrees Random Forest Rule-Based Model method = &#39;rfRules&#39; Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) maxdepth (Maximum Rule Depth) Required packages: randomForest, inTrees, plyr A model-specific variable importance metric is available. Random Forest with Additional Feature Selection method = &#39;Boruta&#39; Type: Regression, Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: Boruta, randomForest Regularized Random Forest method = &#39;RRF&#39; Type: Regression, Classification Tuning parameters: mtry (#Randomly Selected Predictors) coefReg (Regularization Value) coefImp (Importance Coefficient) Required packages: randomForest, RRF A model-specific variable importance metric is available. Regularized Random Forest method = &#39;RRFglobal&#39; Type: Regression, Classification Tuning parameters: mtry (#Randomly Selected Predictors) coefReg (Regularization Value) Required packages: RRF A model-specific variable importance metric is available. Rotation Forest method = &#39;rotationForest&#39; Type: Classification Tuning parameters: K (#Variable Subsets) L (Ensemble Size) Required packages: rotationForest A model-specific variable importance metric is available. Rotation Forest method = &#39;rotationForestCp&#39; Type: Classification Tuning parameters: K (#Variable Subsets) L (Ensemble Size) cp (Complexity Parameter) Required packages: rpart, plyr, rotationForest A model-specific variable importance metric is available. Stochastic Gradient Boosting method = &#39;gbm&#39; Type: Regression, Classification Tuning parameters: n.trees (# Boosting Iterations) interaction.depth (Max Tree Depth) shrinkage (Shrinkage) n.minobsinnode (Min. Terminal Node Size) Required packages: gbm, plyr A model-specific variable importance metric is available. Tree-Based Ensembles method = &#39;nodeHarvest&#39; Type: Regression, Classification Tuning parameters: maxinter (Maximum Interaction Depth) mode (Prediction Mode) Required packages: nodeHarvest Weighted Subspace Random Forest method = &#39;wsrf&#39; Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: wsrf 7.0.11 Feature Extraction (back to contents) Independent Component Regression method = &#39;icr&#39; Type: Regression Tuning parameters: n.comp (#Components) Required packages: fastICA Neural Networks with Feature Extraction method = &#39;pcaNNet&#39; Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) Required packages: nnet Partial Least Squares method = &#39;kernelpls&#39; Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares method = &#39;pls&#39; Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares method = &#39;simpls&#39; Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares method = &#39;widekernelpls&#39; Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Principal Component Analysis method = &#39;pcr&#39; Type: Regression Tuning parameters: ncomp (#Components) Required packages: pls Projection Pursuit Regression method = &#39;ppr&#39; Type: Regression Tuning parameters: nterms (# Terms) Sparse Partial Least Squares method = &#39;spls&#39; Type: Regression, Classification Tuning parameters: K (#Components) eta (Threshold) kappa (Kappa) Required packages: spls Supervised Principal Component Analysis method = &#39;superpc&#39; Type: Regression Tuning parameters: threshold (Threshold) n.components (#Components) Required packages: superpc 7.0.12 Feature Selection Wrapper (back to contents) Generalized Linear Model with Stepwise Feature Selection method = &#39;glmStepAIC&#39; Type: Regression, Classification No tuning parameters for this model Required packages: MASS Linear Discriminant Analysis with Stepwise Feature Selection method = &#39;stepLDA&#39; Type: Classification Tuning parameters: maxvar (Maximum #Variables) direction (Search Direction) Required packages: klaR, MASS Linear Regression with Backwards Selection method = &#39;leapBackward&#39; Type: Regression Tuning parameters: nvmax (Maximum Number of Predictors) Required packages: leaps Linear Regression with Forward Selection method = &#39;leapForward&#39; Type: Regression Tuning parameters: nvmax (Maximum Number of Predictors) Required packages: leaps Linear Regression with Stepwise Selection method = &#39;leapSeq&#39; Type: Regression Tuning parameters: nvmax (Maximum Number of Predictors) Required packages: leaps Linear Regression with Stepwise Selection method = &#39;lmStepAIC&#39; Type: Regression No tuning parameters for this model Required packages: MASS Quadratic Discriminant Analysis with Stepwise Feature Selection method = &#39;stepQDA&#39; Type: Classification Tuning parameters: maxvar (Maximum #Variables) direction (Search Direction) Required packages: klaR, MASS Random Forest with Additional Feature Selection method = &#39;Boruta&#39; Type: Regression, Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: Boruta, randomForest Ridge Regression with Variable Selection method = &#39;foba&#39; Type: Regression Tuning parameters: k (#Variables Retained) lambda (L2 Penalty) Required packages: foba Stepwise Diagonal Linear Discriminant Analysis method = &#39;sddaLDA&#39; Type: Classification No tuning parameters for this model Required packages: SDDA Stepwise Diagonal Quadratic Discriminant Analysis method = &#39;sddaQDA&#39; Type: Classification No tuning parameters for this model Required packages: SDDA 7.0.13 Gaussian Process (back to contents) Gaussian Process method = &#39;gaussprLinear&#39; Type: Regression, Classification No tuning parameters for this model Required packages: kernlab Gaussian Process with Polynomial Kernel method = &#39;gaussprPoly&#39; Type: Regression, Classification Tuning parameters: degree (Polynomial Degree) scale (Scale) Required packages: kernlab Gaussian Process with Radial Basis Function Kernel method = &#39;gaussprRadial&#39; Type: Regression, Classification Tuning parameters: sigma (Sigma) Required packages: kernlab Variational Bayesian Multinomial Probit Regression method = &#39;vbmpRadial&#39; Type: Classification Tuning parameters: estimateTheta (Theta Estimated) Required packages: vbmp 7.0.14 Generalized Additive Model (back to contents) Boosted Generalized Additive Model method = &#39;gamboost&#39; Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: mboost, plyr Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mstop. If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. Generalized Additive Model using LOESS method = &#39;gamLoess&#39; Type: Regression, Classification Tuning parameters: span (Span) degree (Degree) Required packages: gam A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Generalized Additive Model using Splines method = &#39;bam&#39; Type: Regression, Classification Tuning parameters: select (Feature Selection) method (Method) Required packages: mgcv A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Generalized Additive Model using Splines method = &#39;gam&#39; Type: Regression, Classification Tuning parameters: select (Feature Selection) method (Method) Required packages: mgcv A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Generalized Additive Model using Splines method = &#39;gamSpline&#39; Type: Regression, Classification Tuning parameters: df (Degrees of Freedom) Required packages: gam A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. 7.0.15 Generalized Linear Model (back to contents) Bayesian Generalized Linear Model method = &#39;bayesglm&#39; Type: Regression, Classification No tuning parameters for this model Required packages: arm Boosted Generalized Linear Model method = &#39;glmboost&#39; Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: plyr, mboost A model-specific variable importance metric is available. Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mstop. If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. Ensembles of Generalized Lienar Models method = &#39;randomGLM&#39; Type: Regression, Classification Tuning parameters: maxInteractionOrder (Interaction Order) Required packages: randomGLM Generalized Additive Model using LOESS method = &#39;gamLoess&#39; Type: Regression, Classification Tuning parameters: span (Span) degree (Degree) Required packages: gam A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Generalized Additive Model using Splines method = &#39;bam&#39; Type: Regression, Classification Tuning parameters: select (Feature Selection) method (Method) Required packages: mgcv A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Generalized Additive Model using Splines method = &#39;gam&#39; Type: Regression, Classification Tuning parameters: select (Feature Selection) method (Method) Required packages: mgcv A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Generalized Additive Model using Splines method = &#39;gamSpline&#39; Type: Regression, Classification Tuning parameters: df (Degrees of Freedom) Required packages: gam A model-specific variable importance metric is available. Notes: Which terms enter the model in a nonlinear manner is determined by the number of unique values for the predictor. For example, if a predictor only has four unique values, most basis expansion method will fail because there are not enough granularity in the data. By default, a predictor must have at least 10 unique values to be used in a nonlinear basis expansion. Generalized Linear Model method = &#39;glm&#39; Type: Regression, Classification No tuning parameters for this model A model-specific variable importance metric is available. Generalized Linear Model with Stepwise Feature Selection method = &#39;glmStepAIC&#39; Type: Regression, Classification No tuning parameters for this model Required packages: MASS glmnet method = &#39;glmnet_h2o&#39; Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: h2o A model-specific variable importance metric is available. glmnet method = &#39;glmnet&#39; Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: glmnet, Matrix A model-specific variable importance metric is available. Negative Binomial Generalized Linear Model method = &#39;glm.nb&#39; Type: Regression Tuning parameters: link (Link Function) A model-specific variable importance metric is available. Penalized Ordinal Regression method = &#39;ordinalNet&#39; Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) criteria (Selection Criterion) link (Link Function) Required packages: ordinalNet, plyr A model-specific variable importance metric is available. 7.0.16 Handle Missing Predictor Data (back to contents) AdaBoost.M1 method = &#39;AdaBoost.M1&#39; Type: Classification Tuning parameters: mfinal (#Trees) maxdepth (Max Tree Depth) coeflearn (Coefficient Type) Required packages: adabag, plyr A model-specific variable importance metric is available. Bagged AdaBoost method = &#39;AdaBag&#39; Type: Classification Tuning parameters: mfinal (#Trees) maxdepth (Max Tree Depth) Required packages: adabag, plyr A model-specific variable importance metric is available. Boosted Classification Trees method = &#39;ada&#39; Type: Classification Tuning parameters: iter (#Trees) maxdepth (Max Tree Depth) nu (Learning Rate) Required packages: ada, plyr C5.0 method = &#39;C5.0&#39; Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) Required packages: C50, plyr A model-specific variable importance metric is available. CART method = &#39;rpart&#39; Type: Regression, Classification Tuning parameters: cp (Complexity Parameter) Required packages: rpart A model-specific variable importance metric is available. CART method = &#39;rpart1SE&#39; Type: Regression, Classification No tuning parameters for this model Required packages: rpart A model-specific variable importance metric is available. Notes: This CART model replicates the same process used by the rpart function where the model complexity is determined using the one-standard error method. This procedure is replicated inside of the resampling done by train so that an external resampling estimate can be obtained. CART method = &#39;rpart2&#39; Type: Regression, Classification Tuning parameters: maxdepth (Max Tree Depth) Required packages: rpart A model-specific variable importance metric is available. CART or Ordinal Responses method = &#39;rpartScore&#39; Type: Classification Tuning parameters: cp (Complexity Parameter) split (Split Function) prune (Pruning Measure) Required packages: rpartScore, plyr A model-specific variable importance metric is available. Cost-Sensitive C5.0 method = &#39;C5.0Cost&#39; Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) cost (Cost) Required packages: C50, plyr A model-specific variable importance metric is available. Cost-Sensitive CART method = &#39;rpartCost&#39; Type: Classification Tuning parameters: cp (Complexity Parameter) Cost (Cost) Required packages: rpart Single C5.0 Ruleset method = &#39;C5.0Rules&#39; Type: Classification No tuning parameters for this model Required packages: C50 A model-specific variable importance metric is available. Single C5.0 Tree method = &#39;C5.0Tree&#39; Type: Classification No tuning parameters for this model Required packages: C50 A model-specific variable importance metric is available. 7.0.17 Implicit Feature Selection (back to contents) AdaBoost Classification Trees method = &#39;adaboost&#39; Type: Classification Tuning parameters: nIter (#Trees) method (Method) Required packages: fastAdaboost AdaBoost.M1 method = &#39;AdaBoost.M1&#39; Type: Classification Tuning parameters: mfinal (#Trees) maxdepth (Max Tree Depth) coeflearn (Coefficient Type) Required packages: adabag, plyr A model-specific variable importance metric is available. Bagged AdaBoost method = &#39;AdaBag&#39; Type: Classification Tuning parameters: mfinal (#Trees) maxdepth (Max Tree Depth) Required packages: adabag, plyr A model-specific variable importance metric is available. Bagged Flexible Discriminant Analysis method = &#39;bagFDA&#39; Type: Classification Tuning parameters: degree (Product Degree) nprune (#Terms) Required packages: earth, mda A model-specific variable importance metric is available. Bagged MARS method = &#39;bagEarth&#39; Type: Regression, Classification Tuning parameters: nprune (#Terms) degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Bagged MARS using gCV Pruning method = &#39;bagEarthGCV&#39; Type: Regression, Classification Tuning parameters: degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Bayesian Additive Regression Trees method = &#39;bartMachine&#39; Type: Classification, Regression Tuning parameters: num_trees (#Trees) k (Prior Boundary) alpha (Base Terminal Node Hyperparameter) beta (Power Terminal Node Hyperparameter) nu (Degrees of Freedom) Required packages: bartMachine A model-specific variable importance metric is available. Boosted Classification Trees method = &#39;ada&#39; Type: Classification Tuning parameters: iter (#Trees) maxdepth (Max Tree Depth) nu (Learning Rate) Required packages: ada, plyr Boosted Generalized Additive Model method = &#39;gamboost&#39; Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: mboost, plyr Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mstop. If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. Boosted Linear Model method = &#39;BstLm&#39; Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) nu (Shrinkage) Required packages: bst, plyr Boosted Logistic Regression method = &#39;LogitBoost&#39; Type: Classification Tuning parameters: nIter (# Boosting Iterations) Required packages: caTools Boosted Smoothing Spline method = &#39;bstSm&#39; Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) nu (Shrinkage) Required packages: bst, plyr C4.5-like Trees method = &#39;J48&#39; Type: Classification Tuning parameters: C (Confidence Threshold) M (Minimum Instances Per Leaf) Required packages: RWeka C5.0 method = &#39;C5.0&#39; Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) Required packages: C50, plyr A model-specific variable importance metric is available. CART method = &#39;rpart&#39; Type: Regression, Classification Tuning parameters: cp (Complexity Parameter) Required packages: rpart A model-specific variable importance metric is available. CART method = &#39;rpart1SE&#39; Type: Regression, Classification No tuning parameters for this model Required packages: rpart A model-specific variable importance metric is available. Notes: This CART model replicates the same process used by the rpart function where the model complexity is determined using the one-standard error method. This procedure is replicated inside of the resampling done by train so that an external resampling estimate can be obtained. CART method = &#39;rpart2&#39; Type: Regression, Classification Tuning parameters: maxdepth (Max Tree Depth) Required packages: rpart A model-specific variable importance metric is available. CART or Ordinal Responses method = &#39;rpartScore&#39; Type: Classification Tuning parameters: cp (Complexity Parameter) split (Split Function) prune (Pruning Measure) Required packages: rpartScore, plyr A model-specific variable importance metric is available. CHi-squared Automated Interaction Detection method = &#39;chaid&#39; Type: Classification Tuning parameters: alpha2 (Merging Threshold) alpha3 (Splitting former Merged Threshold) alpha4 ( Splitting former Merged Threshold) Required packages: CHAID Conditional Inference Random Forest method = &#39;cforest&#39; Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: party A model-specific variable importance metric is available. Conditional Inference Tree method = &#39;ctree&#39; Type: Classification, Regression Tuning parameters: mincriterion (1 - P-Value Threshold) Required packages: party Conditional Inference Tree method = &#39;ctree2&#39; Type: Regression, Classification Tuning parameters: maxdepth (Max Tree Depth) mincriterion (1 - P-Value Threshold) Required packages: party Cost-Sensitive C5.0 method = &#39;C5.0Cost&#39; Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) cost (Cost) Required packages: C50, plyr A model-specific variable importance metric is available. Cost-Sensitive CART method = &#39;rpartCost&#39; Type: Classification Tuning parameters: cp (Complexity Parameter) Cost (Cost) Required packages: rpart Cubist method = &#39;cubist&#39; Type: Regression Tuning parameters: committees (#Committees) neighbors (#Instances) Required packages: Cubist A model-specific variable importance metric is available. DeepBoost method = &#39;deepboost&#39; Type: Classification Tuning parameters: num_iter (# Boosting Iterations) tree_depth (Tree Depth) beta (L1 Regularization) lambda (Tree Depth Regularization) loss_type (Loss) Required packages: deepboost Elasticnet method = &#39;enet&#39; Type: Regression Tuning parameters: fraction (Fraction of Full Solution) lambda (Weight Decay) Required packages: elasticnet eXtreme Gradient Boosting method = &#39;xgbLinear&#39; Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) lambda (L2 Regularization) alpha (L1 Regularization) eta (Learning Rate) Required packages: xgboost A model-specific variable importance metric is available. eXtreme Gradient Boosting method = &#39;xgbTree&#39; Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) max_depth (Max Tree Depth) eta (Shrinkage) gamma (Minimum Loss Reduction) colsample_bytree (Subsample Ratio of Columns) min_child_weight (Minimum Sum of Instance Weight) subsample (Subsample Percentage) Required packages: xgboost, plyr A model-specific variable importance metric is available. Flexible Discriminant Analysis method = &#39;fda&#39; Type: Classification Tuning parameters: degree (Product Degree) nprune (#Terms) Required packages: earth, mda A model-specific variable importance metric is available. Generalized Linear Model with Stepwise Feature Selection method = &#39;glmStepAIC&#39; Type: Regression, Classification No tuning parameters for this model Required packages: MASS glmnet method = &#39;gbm_h2o&#39; Type: Regression, Classification Tuning parameters: ntrees (# Boosting Iterations) max_depth (Max Tree Depth) min_rows (Min. Terminal Node Size) learn_rate (Shrinkage) col_sample_rate (#Randomly Selected Predictors) Required packages: h2o A model-specific variable importance metric is available. glmnet method = &#39;glmnet_h2o&#39; Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: h2o A model-specific variable importance metric is available. glmnet method = &#39;glmnet&#39; Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: glmnet, Matrix A model-specific variable importance metric is available. Least Angle Regression method = &#39;lars&#39; Type: Regression Tuning parameters: fraction (Fraction) Required packages: lars Least Angle Regression method = &#39;lars2&#39; Type: Regression Tuning parameters: step (#Steps) Required packages: lars Logistic Model Trees method = &#39;LMT&#39; Type: Classification Tuning parameters: iter (# Iteratons) Required packages: RWeka Model Rules method = &#39;M5Rules&#39; Type: Regression Tuning parameters: pruned (Pruned) smoothed (Smoothed) Required packages: RWeka Model Tree method = &#39;M5&#39; Type: Regression Tuning parameters: pruned (Pruned) smoothed (Smoothed) rules (Rules) Required packages: RWeka Multivariate Adaptive Regression Spline method = &#39;earth&#39; Type: Regression, Classification Tuning parameters: nprune (#Terms) degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Multivariate Adaptive Regression Splines method = &#39;gcvEarth&#39; Type: Regression, Classification Tuning parameters: degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Nearest Shrunken Centroids method = &#39;pam&#39; Type: Classification Tuning parameters: threshold (Shrinkage Threshold) Required packages: pamr A model-specific variable importance metric is available. Non-Convex Penalized Quantile Regression method = &#39;rqnc&#39; Type: Regression Tuning parameters: lambda (L1 Penalty) penalty (Penalty Type) Required packages: rqPen Oblique Random Forest method = &#39;ORFlog&#39; Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Oblique Random Forest method = &#39;ORFpls&#39; Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Oblique Random Forest method = &#39;ORFridge&#39; Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Oblique Random Forest method = &#39;ORFsvm&#39; Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Oblique Trees method = &#39;oblique.tree&#39; Type: Classification Tuning parameters: oblique.splits (Oblique Splits) variable.selection (Variable Selection Method) Required packages: oblique.tree Parallel Random Forest method = &#39;parRF&#39; Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: e1071, randomForest, foreach A model-specific variable importance metric is available. Penalized Linear Discriminant Analysis method = &#39;PenalizedLDA&#39; Type: Classification Tuning parameters: lambda (L1 Penalty) K (#Discriminant Functions) Required packages: penalizedLDA, plyr Penalized Linear Regression method = &#39;penalized&#39; Type: Regression Tuning parameters: lambda1 (L1 Penalty) lambda2 (L2 Penalty) Required packages: penalized Penalized Ordinal Regression method = &#39;ordinalNet&#39; Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) criteria (Selection Criterion) link (Link Function) Required packages: ordinalNet, plyr A model-specific variable importance metric is available. Quantile Random Forest method = &#39;qrf&#39; Type: Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: quantregForest Quantile Regression with LASSO penalty method = &#39;rqlasso&#39; Type: Regression Tuning parameters: lambda (L1 Penalty) Required packages: rqPen Random Ferns method = &#39;rFerns&#39; Type: Classification Tuning parameters: depth (Fern Depth) Required packages: rFerns Random Forest method = &#39;ranger&#39; Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: e1071, ranger A model-specific variable importance metric is available. Random Forest method = &#39;Rborist&#39; Type: Classification, Regression Tuning parameters: predFixed (#Randomly Selected Predictors) Required packages: Rborist A model-specific variable importance metric is available. Random Forest method = &#39;rf&#39; Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: randomForest A model-specific variable importance metric is available. Random Forest by Randomization method = &#39;extraTrees&#39; Type: Regression, Classification Tuning parameters: mtry (# Randomly Selected Predictors) numRandomCuts (# Random Cuts) Required packages: extraTrees Random Forest Rule-Based Model method = &#39;rfRules&#39; Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) maxdepth (Maximum Rule Depth) Required packages: randomForest, inTrees, plyr A model-specific variable importance metric is available. Regularized Random Forest method = &#39;RRF&#39; Type: Regression, Classification Tuning parameters: mtry (#Randomly Selected Predictors) coefReg (Regularization Value) coefImp (Importance Coefficient) Required packages: randomForest, RRF A model-specific variable importance metric is available. Regularized Random Forest method = &#39;RRFglobal&#39; Type: Regression, Classification Tuning parameters: mtry (#Randomly Selected Predictors) coefReg (Regularization Value) Required packages: RRF A model-specific variable importance metric is available. Relaxed Lasso method = &#39;relaxo&#39; Type: Regression Tuning parameters: lambda (Penalty Parameter) phi (Relaxation Parameter) Required packages: relaxo, plyr Rotation Forest method = &#39;rotationForest&#39; Type: Classification Tuning parameters: K (#Variable Subsets) L (Ensemble Size) Required packages: rotationForest A model-specific variable importance metric is available. Rotation Forest method = &#39;rotationForestCp&#39; Type: Classification Tuning parameters: K (#Variable Subsets) L (Ensemble Size) cp (Complexity Parameter) Required packages: rpart, plyr, rotationForest A model-specific variable importance metric is available. Rule-Based Classifier method = &#39;JRip&#39; Type: Classification Tuning parameters: NumOpt (# Optimizations) NumFolds (# Folds) MinWeights (Min Weights) Required packages: RWeka A model-specific variable importance metric is available. Rule-Based Classifier method = &#39;PART&#39; Type: Classification Tuning parameters: threshold (Confidence Threshold) pruned (Pruning) Required packages: RWeka A model-specific variable importance metric is available. Single C5.0 Ruleset method = &#39;C5.0Rules&#39; Type: Classification No tuning parameters for this model Required packages: C50 A model-specific variable importance metric is available. Single C5.0 Tree method = &#39;C5.0Tree&#39; Type: Classification No tuning parameters for this model Required packages: C50 A model-specific variable importance metric is available. Single Rule Classification method = &#39;OneR&#39; Type: Classification No tuning parameters for this model Required packages: RWeka Sparse Distance Weighted Discrimination method = &#39;sdwd&#39; Type: Classification Tuning parameters: lambda (L1 Penalty) lambda2 (L2 Penalty) Required packages: sdwd A model-specific variable importance metric is available. Sparse Linear Discriminant Analysis method = &#39;sparseLDA&#39; Type: Classification Tuning parameters: NumVars (# Predictors) lambda (Lambda) Required packages: sparseLDA Sparse Mixture Discriminant Analysis method = &#39;smda&#39; Type: Classification Tuning parameters: NumVars (# Predictors) lambda (Lambda) R (# Subclasses) Required packages: sparseLDA Spike and Slab Regression method = &#39;spikeslab&#39; Type: Regression Tuning parameters: vars (Variables Retained) Required packages: spikeslab, plyr Stochastic Gradient Boosting method = &#39;gbm&#39; Type: Regression, Classification Tuning parameters: n.trees (# Boosting Iterations) interaction.depth (Max Tree Depth) shrinkage (Shrinkage) n.minobsinnode (Min. Terminal Node Size) Required packages: gbm, plyr A model-specific variable importance metric is available. The Bayesian lasso method = &#39;blasso&#39; Type: Regression Tuning parameters: sparsity (Sparsity Threshold) Required packages: monomvn Notes: This model creates predictions using the mean of the posterior distributions but sets some parameters specifically to zero based on the tuning parameter sparsity. For example, when sparsity = .5, only coefficients where at least half the posterior estimates are nonzero are used. The lasso method = &#39;lasso&#39; Type: Regression Tuning parameters: fraction (Fraction of Full Solution) Required packages: elasticnet Tree Models from Genetic Algorithms method = &#39;evtree&#39; Type: Regression, Classification Tuning parameters: alpha (Complexity Parameter) Required packages: evtree Tree-Based Ensembles method = &#39;nodeHarvest&#39; Type: Regression, Classification Tuning parameters: maxinter (Maximum Interaction Depth) mode (Prediction Mode) Required packages: nodeHarvest Weighted Subspace Random Forest method = &#39;wsrf&#39; Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: wsrf 7.0.18 Kernel Method (back to contents) Distance Weighted Discrimination with Polynomial Kernel method = &#39;dwdPoly&#39; Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) degree (Polynomial Degree) scale (Scale) Required packages: kerndwd Distance Weighted Discrimination with Radial Basis Function Kernel method = &#39;dwdRadial&#39; Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) sigma (Sigma) Required packages: kernlab, kerndwd Gaussian Process method = &#39;gaussprLinear&#39; Type: Regression, Classification No tuning parameters for this model Required packages: kernlab Gaussian Process with Polynomial Kernel method = &#39;gaussprPoly&#39; Type: Regression, Classification Tuning parameters: degree (Polynomial Degree) scale (Scale) Required packages: kernlab Gaussian Process with Radial Basis Function Kernel method = &#39;gaussprRadial&#39; Type: Regression, Classification Tuning parameters: sigma (Sigma) Required packages: kernlab L2 Regularized Linear Support Vector Machines with Class Weights method = &#39;svmLinearWeights2&#39; Type: Classification Tuning parameters: cost (Cost) Loss (Loss Function) weight (Class Weight) Required packages: LiblineaR L2 Regularized Support Vector Machine (dual) with Linear Kernel method = &#39;svmLinear3&#39; Type: Regression, Classification Tuning parameters: cost (Cost) Loss (Loss Function) Required packages: LiblineaR Least Squares Support Vector Machine method = &#39;lssvmLinear&#39; Type: Classification Tuning parameters: tau (Regularization Parameter) Required packages: kernlab Least Squares Support Vector Machine with Polynomial Kernel method = &#39;lssvmPoly&#39; Type: Classification Tuning parameters: degree (Polynomial Degree) scale (Scale) tau (Regularization Parameter) Required packages: kernlab Least Squares Support Vector Machine with Radial Basis Function Kernel method = &#39;lssvmRadial&#39; Type: Classification Tuning parameters: sigma (Sigma) tau (Regularization Parameter) Required packages: kernlab Linear Distance Weighted Discrimination method = &#39;dwdLinear&#39; Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) Required packages: kerndwd Linear Support Vector Machines with Class Weights method = &#39;svmLinearWeights&#39; Type: Classification Tuning parameters: cost (Cost) weight (Class Weight) Required packages: e1071 Oblique Random Forest method = &#39;ORFsvm&#39; Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Partial Least Squares method = &#39;kernelpls&#39; Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Polynomial Kernel Regularized Least Squares method = &#39;krlsPoly&#39; Type: Regression Tuning parameters: lambda (Regularization Parameter) degree (Polynomial Degree) Required packages: KRLS Radial Basis Function Kernel Regularized Least Squares method = &#39;krlsRadial&#39; Type: Regression Tuning parameters: lambda (Regularization Parameter) sigma (Sigma) Required packages: KRLS, kernlab Relevance Vector Machines with Linear Kernel method = &#39;rvmLinear&#39; Type: Regression No tuning parameters for this model Required packages: kernlab Relevance Vector Machines with Polynomial Kernel method = &#39;rvmPoly&#39; Type: Regression Tuning parameters: scale (Scale) degree (Polynomial Degree) Required packages: kernlab Relevance Vector Machines with Radial Basis Function Kernel method = &#39;rvmRadial&#39; Type: Regression Tuning parameters: sigma (Sigma) Required packages: kernlab Support Vector Machines with Boundrange String Kernel method = &#39;svmBoundrangeString&#39; Type: Regression, Classification Tuning parameters: length (length) C (Cost) Required packages: kernlab Support Vector Machines with Class Weights method = &#39;svmRadialWeights&#39; Type: Classification Tuning parameters: sigma (Sigma) C (Cost) Weight (Weight) Required packages: kernlab Support Vector Machines with Exponential String Kernel method = &#39;svmExpoString&#39; Type: Regression, Classification Tuning parameters: lambda (lambda) C (Cost) Required packages: kernlab Support Vector Machines with Linear Kernel method = &#39;svmLinear&#39; Type: Regression, Classification Tuning parameters: C (Cost) Required packages: kernlab Support Vector Machines with Linear Kernel method = &#39;svmLinear2&#39; Type: Regression, Classification Tuning parameters: cost (Cost) Required packages: e1071 Support Vector Machines with Polynomial Kernel method = &#39;svmPoly&#39; Type: Regression, Classification Tuning parameters: degree (Polynomial Degree) scale (Scale) C (Cost) Required packages: kernlab Support Vector Machines with Radial Basis Function Kernel method = &#39;svmRadial&#39; Type: Regression, Classification Tuning parameters: sigma (Sigma) C (Cost) Required packages: kernlab Support Vector Machines with Radial Basis Function Kernel method = &#39;svmRadialCost&#39; Type: Regression, Classification Tuning parameters: C (Cost) Required packages: kernlab Support Vector Machines with Radial Basis Function Kernel method = &#39;svmRadialSigma&#39; Type: Regression, Classification Tuning parameters: sigma (Sigma) C (Cost) Required packages: kernlab Notes: This SVM model tunes over the cost parameter and the RBF kernel parameter sigma. In the latter case, using tuneLength will, at most, evaluate six values of the kernel parameter. This enables a broad search over the cost parameter and a relatively narrow search over sigma Support Vector Machines with Spectrum String Kernel method = &#39;svmSpectrumString&#39; Type: Regression, Classification Tuning parameters: length (length) C (Cost) Required packages: kernlab 7.0.19 L1 Regularization (back to contents) Bayesian Ridge Regression (Model Averaged) method = &#39;blassoAveraged&#39; Type: Regression No tuning parameters for this model Required packages: monomvn Notes: This model makes predictions by averaging the predictions based on the posterior estimates of the regression coefficients. While it is possible that some of these posterior estimates are zero for non-informative predictors, the final predicted value may be a function of many (or even all) predictors. DeepBoost method = &#39;deepboost&#39; Type: Classification Tuning parameters: num_iter (# Boosting Iterations) tree_depth (Tree Depth) beta (L1 Regularization) lambda (Tree Depth Regularization) loss_type (Loss) Required packages: deepboost Elasticnet method = &#39;enet&#39; Type: Regression Tuning parameters: fraction (Fraction of Full Solution) lambda (Weight Decay) Required packages: elasticnet glmnet method = &#39;glmnet_h2o&#39; Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: h2o A model-specific variable importance metric is available. glmnet method = &#39;glmnet&#39; Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: glmnet, Matrix A model-specific variable importance metric is available. Least Angle Regression method = &#39;lars&#39; Type: Regression Tuning parameters: fraction (Fraction) Required packages: lars Least Angle Regression method = &#39;lars2&#39; Type: Regression Tuning parameters: step (#Steps) Required packages: lars Non-Convex Penalized Quantile Regression method = &#39;rqnc&#39; Type: Regression Tuning parameters: lambda (L1 Penalty) penalty (Penalty Type) Required packages: rqPen Penalized Linear Discriminant Analysis method = &#39;PenalizedLDA&#39; Type: Classification Tuning parameters: lambda (L1 Penalty) K (#Discriminant Functions) Required packages: penalizedLDA, plyr Penalized Linear Regression method = &#39;penalized&#39; Type: Regression Tuning parameters: lambda1 (L1 Penalty) lambda2 (L2 Penalty) Required packages: penalized Penalized Ordinal Regression method = &#39;ordinalNet&#39; Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) criteria (Selection Criterion) link (Link Function) Required packages: ordinalNet, plyr A model-specific variable importance metric is available. Quantile Regression with LASSO penalty method = &#39;rqlasso&#39; Type: Regression Tuning parameters: lambda (L1 Penalty) Required packages: rqPen Relaxed Lasso method = &#39;relaxo&#39; Type: Regression Tuning parameters: lambda (Penalty Parameter) phi (Relaxation Parameter) Required packages: relaxo, plyr Sparse Distance Weighted Discrimination method = &#39;sdwd&#39; Type: Classification Tuning parameters: lambda (L1 Penalty) lambda2 (L2 Penalty) Required packages: sdwd A model-specific variable importance metric is available. Sparse Linear Discriminant Analysis method = &#39;sparseLDA&#39; Type: Classification Tuning parameters: NumVars (# Predictors) lambda (Lambda) Required packages: sparseLDA Sparse Mixture Discriminant Analysis method = &#39;smda&#39; Type: Classification Tuning parameters: NumVars (# Predictors) lambda (Lambda) R (# Subclasses) Required packages: sparseLDA Sparse Partial Least Squares method = &#39;spls&#39; Type: Regression, Classification Tuning parameters: K (#Components) eta (Threshold) kappa (Kappa) Required packages: spls The Bayesian lasso method = &#39;blasso&#39; Type: Regression Tuning parameters: sparsity (Sparsity Threshold) Required packages: monomvn Notes: This model creates predictions using the mean of the posterior distributions but sets some parameters specifically to zero based on the tuning parameter sparsity. For example, when sparsity = .5, only coefficients where at least half the posterior estimates are nonzero are used. The lasso method = &#39;lasso&#39; Type: Regression Tuning parameters: fraction (Fraction of Full Solution) Required packages: elasticnet 7.0.20 L2 Regularization (back to contents) Bayesian Ridge Regression method = &#39;bridge&#39; Type: Regression No tuning parameters for this model Required packages: monomvn Distance Weighted Discrimination with Polynomial Kernel method = &#39;dwdPoly&#39; Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) degree (Polynomial Degree) scale (Scale) Required packages: kerndwd Distance Weighted Discrimination with Radial Basis Function Kernel method = &#39;dwdRadial&#39; Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) sigma (Sigma) Required packages: kernlab, kerndwd glmnet method = &#39;glmnet_h2o&#39; Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: h2o A model-specific variable importance metric is available. glmnet method = &#39;glmnet&#39; Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: glmnet, Matrix A model-specific variable importance metric is available. Linear Distance Weighted Discrimination method = &#39;dwdLinear&#39; Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) Required packages: kerndwd Model Averaged Neural Network method = &#39;avNNet&#39; Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) bag (Bagging) Required packages: nnet Multi-Layer Perceptron method = &#39;mlpWeightDecay&#39; Type: Regression, Classification Tuning parameters: size (#Hidden Units) decay (Weight Decay) Required packages: RSNNS Multi-Layer Perceptron, multiple layers method = &#39;mlpWeightDecayML&#39; Type: Regression, Classification Tuning parameters: layer1 (#Hidden Units layer1) layer2 (#Hidden Units layer2) layer3 (#Hidden Units layer3) decay (Weight Decay) Required packages: RSNNS Multilayer Perceptron Network by Stochastic Gradient Descent method = &#39;mlpSGD&#39; Type: Regression, Classification Tuning parameters: size (#Hidden Units) l2reg (L2 Regularization) lambda (RMSE Gradient Scaling) learn_rate (Learning Rate) momentum (Momentum) gamma (Decay) minibatchsz (Batch Size) repeats (#Models) Required packages: FCNN4R, plyr A model-specific variable importance metric is available. Neural Network method = &#39;nnet&#39; Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) Required packages: nnet A model-specific variable importance metric is available. Neural Networks with Feature Extraction method = &#39;pcaNNet&#39; Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) Required packages: nnet Oblique Random Forest method = &#39;ORFridge&#39; Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Penalized Linear Regression method = &#39;penalized&#39; Type: Regression Tuning parameters: lambda1 (L1 Penalty) lambda2 (L2 Penalty) Required packages: penalized Penalized Logistic Regression method = &#39;plr&#39; Type: Classification Tuning parameters: lambda (L2 Penalty) cp (Complexity Parameter) Required packages: stepPlr Penalized Multinomial Regression method = &#39;multinom&#39; Type: Classification Tuning parameters: decay (Weight Decay) Required packages: nnet A model-specific variable importance metric is available. Penalized Ordinal Regression method = &#39;ordinalNet&#39; Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) criteria (Selection Criterion) link (Link Function) Required packages: ordinalNet, plyr A model-specific variable importance metric is available. Polynomial Kernel Regularized Least Squares method = &#39;krlsPoly&#39; Type: Regression Tuning parameters: lambda (Regularization Parameter) degree (Polynomial Degree) Required packages: KRLS Quantile Regression Neural Network method = &#39;qrnn&#39; Type: Regression Tuning parameters: n.hidden (#Hidden Units) penalty ( Weight Decay) bag (Bagged Models?) Required packages: qrnn Radial Basis Function Kernel Regularized Least Squares method = &#39;krlsRadial&#39; Type: Regression Tuning parameters: lambda (Regularization Parameter) sigma (Sigma) Required packages: KRLS, kernlab Radial Basis Function Network method = &#39;rbf&#39; Type: Classification, Regression Tuning parameters: size (#Hidden Units) Required packages: RSNNS Radial Basis Function Network method = &#39;rbfDDA&#39; Type: Regression, Classification Tuning parameters: negativeThreshold (Activation Limit for Conflicting Classes) Required packages: RSNNS Relaxed Lasso method = &#39;relaxo&#39; Type: Regression Tuning parameters: lambda (Penalty Parameter) phi (Relaxation Parameter) Required packages: relaxo, plyr Ridge Regression method = &#39;ridge&#39; Type: Regression Tuning parameters: lambda (Weight Decay) Required packages: elasticnet Ridge Regression with Variable Selection method = &#39;foba&#39; Type: Regression Tuning parameters: k (#Variables Retained) lambda (L2 Penalty) Required packages: foba Sparse Distance Weighted Discrimination method = &#39;sdwd&#39; Type: Classification Tuning parameters: lambda (L1 Penalty) lambda2 (L2 Penalty) Required packages: sdwd A model-specific variable importance metric is available. 7.0.21 Linear Classifier (back to contents) Adjacent Categories Probability Model for Ordinal Data method = &#39;vglmAdjCat&#39; Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM Bagged Logic Regression method = &#39;logicBag&#39; Type: Regression, Classification Tuning parameters: nleaves (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: logicFS Bayesian Generalized Linear Model method = &#39;bayesglm&#39; Type: Regression, Classification No tuning parameters for this model Required packages: arm Boosted Generalized Linear Model method = &#39;glmboost&#39; Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: plyr, mboost A model-specific variable importance metric is available. Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mstop. If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. Continuation Ratio Model for Ordinal Data method = &#39;vglmContRatio&#39; Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM Cumulative Probability Model for Ordinal Data method = &#39;vglmCumulative&#39; Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM Diagonal Discriminant Analysis method = &#39;dda&#39; Type: Classification Tuning parameters: model (Model) shrinkage (Shrinkage Type) Required packages: sparsediscrim Ensembles of Generalized Lienar Models method = &#39;randomGLM&#39; Type: Regression, Classification Tuning parameters: maxInteractionOrder (Interaction Order) Required packages: randomGLM Factor-Based Linear Discriminant Analysis method = &#39;RFlda&#39; Type: Classification Tuning parameters: q (# Factors) Required packages: HiDimDA Gaussian Process method = &#39;gaussprLinear&#39; Type: Regression, Classification No tuning parameters for this model Required packages: kernlab Generalized Linear Model method = &#39;glm&#39; Type: Regression, Classification No tuning parameters for this model A model-specific variable importance metric is available. Generalized Linear Model with Stepwise Feature Selection method = &#39;glmStepAIC&#39; Type: Regression, Classification No tuning parameters for this model Required packages: MASS Generalized Partial Least Squares method = &#39;gpls&#39; Type: Classification Tuning parameters: K.prov (#Components) Required packages: gpls glmnet method = &#39;glmnet_h2o&#39; Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: h2o A model-specific variable importance metric is available. glmnet method = &#39;glmnet&#39; Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: glmnet, Matrix A model-specific variable importance metric is available. Heteroscedastic Discriminant Analysis method = &#39;hda&#39; Type: Classification Tuning parameters: gamma (Gamma) lambda (Lambda) newdim (Dimension of the Discriminative Subspace) Required packages: hda High Dimensional Discriminant Analysis method = &#39;hdda&#39; Type: Classification Tuning parameters: threshold (Threshold) model (Model Type) Required packages: HDclassif High-Dimensional Regularized Discriminant Analysis method = &#39;hdrda&#39; Type: Classification Tuning parameters: gamma (Gamma) lambda (Lambda) shrinkage_type (Shrinkage Type) Required packages: sparsediscrim L2 Regularized Linear Support Vector Machines with Class Weights method = &#39;svmLinearWeights2&#39; Type: Classification Tuning parameters: cost (Cost) Loss (Loss Function) weight (Class Weight) Required packages: LiblineaR L2 Regularized Support Vector Machine (dual) with Linear Kernel method = &#39;svmLinear3&#39; Type: Regression, Classification Tuning parameters: cost (Cost) Loss (Loss Function) Required packages: LiblineaR Least Squares Support Vector Machine method = &#39;lssvmLinear&#39; Type: Classification Tuning parameters: tau (Regularization Parameter) Required packages: kernlab Linear Discriminant Analysis method = &#39;lda&#39; Type: Classification No tuning parameters for this model Required packages: MASS Linear Discriminant Analysis method = &#39;lda2&#39; Type: Classification Tuning parameters: dimen (#Discriminant Functions) Required packages: MASS Linear Discriminant Analysis with Stepwise Feature Selection method = &#39;stepLDA&#39; Type: Classification Tuning parameters: maxvar (Maximum #Variables) direction (Search Direction) Required packages: klaR, MASS Linear Distance Weighted Discrimination method = &#39;dwdLinear&#39; Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) Required packages: kerndwd Linear Support Vector Machines with Class Weights method = &#39;svmLinearWeights&#39; Type: Classification Tuning parameters: cost (Cost) weight (Class Weight) Required packages: e1071 Localized Linear Discriminant Analysis method = &#39;loclda&#39; Type: Classification Tuning parameters: k (#Nearest Neighbors) Required packages: klaR Logic Regression method = &#39;logreg&#39; Type: Regression, Classification Tuning parameters: treesize (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: LogicReg Logistic Model Trees method = &#39;LMT&#39; Type: Classification Tuning parameters: iter (# Iteratons) Required packages: RWeka Maximum Uncertainty Linear Discriminant Analysis method = &#39;Mlda&#39; Type: Classification No tuning parameters for this model Required packages: HiDimDA Nearest Shrunken Centroids method = &#39;pam&#39; Type: Classification Tuning parameters: threshold (Shrinkage Threshold) Required packages: pamr A model-specific variable importance metric is available. Ordered Logistic or Probit Regression method = &#39;polr&#39; Type: Classification Tuning parameters: method (parameter) Required packages: MASS A model-specific variable importance metric is available. Partial Least Squares method = &#39;kernelpls&#39; Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares method = &#39;pls&#39; Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares method = &#39;simpls&#39; Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares method = &#39;widekernelpls&#39; Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Penalized Linear Discriminant Analysis method = &#39;PenalizedLDA&#39; Type: Classification Tuning parameters: lambda (L1 Penalty) K (#Discriminant Functions) Required packages: penalizedLDA, plyr Penalized Logistic Regression method = &#39;plr&#39; Type: Classification Tuning parameters: lambda (L2 Penalty) cp (Complexity Parameter) Required packages: stepPlr Penalized Multinomial Regression method = &#39;multinom&#39; Type: Classification Tuning parameters: decay (Weight Decay) Required packages: nnet A model-specific variable importance metric is available. Penalized Ordinal Regression method = &#39;ordinalNet&#39; Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) criteria (Selection Criterion) link (Link Function) Required packages: ordinalNet, plyr A model-specific variable importance metric is available. Regularized Discriminant Analysis method = &#39;rda&#39; Type: Classification Tuning parameters: gamma (Gamma) lambda (Lambda) Required packages: klaR Regularized Linear Discriminant Analysis method = &#39;rlda&#39; Type: Classification Tuning parameters: estimator (Regularization Method) Required packages: sparsediscrim Robust Linear Discriminant Analysis method = &#39;Linda&#39; Type: Classification No tuning parameters for this model Required packages: rrcov Robust Regularized Linear Discriminant Analysis method = &#39;rrlda&#39; Type: Classification Tuning parameters: lambda (Penalty Parameter) hp (Robustness Parameter) penalty (Penalty Type) Required packages: rrlda Robust SIMCA method = &#39;RSimca&#39; Type: Classification No tuning parameters for this model Required packages: rrcovHD Shrinkage Discriminant Analysis method = &#39;sda&#39; Type: Classification Tuning parameters: diagonal (Diagonalize) lambda (shrinkage) Required packages: sda Sparse Distance Weighted Discrimination method = &#39;sdwd&#39; Type: Classification Tuning parameters: lambda (L1 Penalty) lambda2 (L2 Penalty) Required packages: sdwd A model-specific variable importance metric is available. Sparse Linear Discriminant Analysis method = &#39;sparseLDA&#39; Type: Classification Tuning parameters: NumVars (# Predictors) lambda (Lambda) Required packages: sparseLDA Sparse Partial Least Squares method = &#39;spls&#39; Type: Regression, Classification Tuning parameters: K (#Components) eta (Threshold) kappa (Kappa) Required packages: spls Stabilized Linear Discriminant Analysis method = &#39;slda&#39; Type: Classification No tuning parameters for this model Required packages: ipred Stepwise Diagonal Linear Discriminant Analysis method = &#39;sddaLDA&#39; Type: Classification No tuning parameters for this model Required packages: SDDA Support Vector Machines with Linear Kernel method = &#39;svmLinear&#39; Type: Regression, Classification Tuning parameters: C (Cost) Required packages: kernlab Support Vector Machines with Linear Kernel method = &#39;svmLinear2&#39; Type: Regression, Classification Tuning parameters: cost (Cost) Required packages: e1071 7.0.22 Linear Regression (back to contents) Bagged Logic Regression method = &#39;logicBag&#39; Type: Regression, Classification Tuning parameters: nleaves (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: logicFS Bayesian Ridge Regression method = &#39;bridge&#39; Type: Regression No tuning parameters for this model Required packages: monomvn Bayesian Ridge Regression (Model Averaged) method = &#39;blassoAveraged&#39; Type: Regression No tuning parameters for this model Required packages: monomvn Notes: This model makes predictions by averaging the predictions based on the posterior estimates of the regression coefficients. While it is possible that some of these posterior estimates are zero for non-informative predictors, the final predicted value may be a function of many (or even all) predictors. Boosted Linear Model method = &#39;BstLm&#39; Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) nu (Shrinkage) Required packages: bst, plyr Cubist method = &#39;cubist&#39; Type: Regression Tuning parameters: committees (#Committees) neighbors (#Instances) Required packages: Cubist A model-specific variable importance metric is available. Elasticnet method = &#39;enet&#39; Type: Regression Tuning parameters: fraction (Fraction of Full Solution) lambda (Weight Decay) Required packages: elasticnet glmnet method = &#39;glmnet_h2o&#39; Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: h2o A model-specific variable importance metric is available. glmnet method = &#39;glmnet&#39; Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: glmnet, Matrix A model-specific variable importance metric is available. Independent Component Regression method = &#39;icr&#39; Type: Regression Tuning parameters: n.comp (#Components) Required packages: fastICA L2 Regularized Support Vector Machine (dual) with Linear Kernel method = &#39;svmLinear3&#39; Type: Regression, Classification Tuning parameters: cost (Cost) Loss (Loss Function) Required packages: LiblineaR Least Angle Regression method = &#39;lars&#39; Type: Regression Tuning parameters: fraction (Fraction) Required packages: lars Least Angle Regression method = &#39;lars2&#39; Type: Regression Tuning parameters: step (#Steps) Required packages: lars Linear Regression method = &#39;lm&#39; Type: Regression Tuning parameters: intercept (intercept) A model-specific variable importance metric is available. Linear Regression with Backwards Selection method = &#39;leapBackward&#39; Type: Regression Tuning parameters: nvmax (Maximum Number of Predictors) Required packages: leaps Linear Regression with Forward Selection method = &#39;leapForward&#39; Type: Regression Tuning parameters: nvmax (Maximum Number of Predictors) Required packages: leaps Linear Regression with Stepwise Selection method = &#39;leapSeq&#39; Type: Regression Tuning parameters: nvmax (Maximum Number of Predictors) Required packages: leaps Linear Regression with Stepwise Selection method = &#39;lmStepAIC&#39; Type: Regression No tuning parameters for this model Required packages: MASS Logic Regression method = &#39;logreg&#39; Type: Regression, Classification Tuning parameters: treesize (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: LogicReg Model Rules method = &#39;M5Rules&#39; Type: Regression Tuning parameters: pruned (Pruned) smoothed (Smoothed) Required packages: RWeka Model Tree method = &#39;M5&#39; Type: Regression Tuning parameters: pruned (Pruned) smoothed (Smoothed) rules (Rules) Required packages: RWeka Non-Convex Penalized Quantile Regression method = &#39;rqnc&#39; Type: Regression Tuning parameters: lambda (L1 Penalty) penalty (Penalty Type) Required packages: rqPen Non-Negative Least Squares method = &#39;nnls&#39; Type: Regression No tuning parameters for this model Required packages: nnls A model-specific variable importance metric is available. Partial Least Squares method = &#39;kernelpls&#39; Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares method = &#39;pls&#39; Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares method = &#39;simpls&#39; Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares method = &#39;widekernelpls&#39; Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Penalized Linear Regression method = &#39;penalized&#39; Type: Regression Tuning parameters: lambda1 (L1 Penalty) lambda2 (L2 Penalty) Required packages: penalized Penalized Ordinal Regression method = &#39;ordinalNet&#39; Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) criteria (Selection Criterion) link (Link Function) Required packages: ordinalNet, plyr A model-specific variable importance metric is available. Principal Component Analysis method = &#39;pcr&#39; Type: Regression Tuning parameters: ncomp (#Components) Required packages: pls Quantile Regression with LASSO penalty method = &#39;rqlasso&#39; Type: Regression Tuning parameters: lambda (L1 Penalty) Required packages: rqPen Relaxed Lasso method = &#39;relaxo&#39; Type: Regression Tuning parameters: lambda (Penalty Parameter) phi (Relaxation Parameter) Required packages: relaxo, plyr Relevance Vector Machines with Linear Kernel method = &#39;rvmLinear&#39; Type: Regression No tuning parameters for this model Required packages: kernlab Ridge Regression method = &#39;ridge&#39; Type: Regression Tuning parameters: lambda (Weight Decay) Required packages: elasticnet Ridge Regression with Variable Selection method = &#39;foba&#39; Type: Regression Tuning parameters: k (#Variables Retained) lambda (L2 Penalty) Required packages: foba Robust Linear Model method = &#39;rlm&#39; Type: Regression Tuning parameters: intercept (intercept) psi (psi) Required packages: MASS Sparse Partial Least Squares method = &#39;spls&#39; Type: Regression, Classification Tuning parameters: K (#Components) eta (Threshold) kappa (Kappa) Required packages: spls Spike and Slab Regression method = &#39;spikeslab&#39; Type: Regression Tuning parameters: vars (Variables Retained) Required packages: spikeslab, plyr Supervised Principal Component Analysis method = &#39;superpc&#39; Type: Regression Tuning parameters: threshold (Threshold) n.components (#Components) Required packages: superpc Support Vector Machines with Linear Kernel method = &#39;svmLinear&#39; Type: Regression, Classification Tuning parameters: C (Cost) Required packages: kernlab Support Vector Machines with Linear Kernel method = &#39;svmLinear2&#39; Type: Regression, Classification Tuning parameters: cost (Cost) Required packages: e1071 The Bayesian lasso method = &#39;blasso&#39; Type: Regression Tuning parameters: sparsity (Sparsity Threshold) Required packages: monomvn Notes: This model creates predictions using the mean of the posterior distributions but sets some parameters specifically to zero based on the tuning parameter sparsity. For example, when sparsity = .5, only coefficients where at least half the posterior estimates are nonzero are used. The lasso method = &#39;lasso&#39; Type: Regression Tuning parameters: fraction (Fraction of Full Solution) Required packages: elasticnet 7.0.23 Logic Regression (back to contents) Bagged Logic Regression method = &#39;logicBag&#39; Type: Regression, Classification Tuning parameters: nleaves (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: logicFS Logic Regression method = &#39;logreg&#39; Type: Regression, Classification Tuning parameters: treesize (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: LogicReg 7.0.24 Logistic Regression (back to contents) Adjacent Categories Probability Model for Ordinal Data method = &#39;vglmAdjCat&#39; Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM Bagged Logic Regression method = &#39;logicBag&#39; Type: Regression, Classification Tuning parameters: nleaves (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: logicFS Bayesian Generalized Linear Model method = &#39;bayesglm&#39; Type: Regression, Classification No tuning parameters for this model Required packages: arm Boosted Logistic Regression method = &#39;LogitBoost&#39; Type: Classification Tuning parameters: nIter (# Boosting Iterations) Required packages: caTools Continuation Ratio Model for Ordinal Data method = &#39;vglmContRatio&#39; Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM Cumulative Probability Model for Ordinal Data method = &#39;vglmCumulative&#39; Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM Generalized Partial Least Squares method = &#39;gpls&#39; Type: Classification Tuning parameters: K.prov (#Components) Required packages: gpls Logic Regression method = &#39;logreg&#39; Type: Regression, Classification Tuning parameters: treesize (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: LogicReg Logistic Model Trees method = &#39;LMT&#39; Type: Classification Tuning parameters: iter (# Iteratons) Required packages: RWeka Oblique Random Forest method = &#39;ORFlog&#39; Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Ordered Logistic or Probit Regression method = &#39;polr&#39; Type: Classification Tuning parameters: method (parameter) Required packages: MASS A model-specific variable importance metric is available. Penalized Logistic Regression method = &#39;plr&#39; Type: Classification Tuning parameters: lambda (L2 Penalty) cp (Complexity Parameter) Required packages: stepPlr Penalized Multinomial Regression method = &#39;multinom&#39; Type: Classification Tuning parameters: decay (Weight Decay) Required packages: nnet A model-specific variable importance metric is available. 7.0.25 Mixture Model (back to contents) Adaptive Mixture Discriminant Analysis method = &#39;amdai&#39; Type: Classification Tuning parameters: model (Model Type) Required packages: adaptDA Mixture Discriminant Analysis method = &#39;mda&#39; Type: Classification Tuning parameters: subclasses (#Subclasses Per Class) Required packages: mda Robust Mixture Discriminant Analysis method = &#39;rmda&#39; Type: Classification Tuning parameters: K (#Subclasses Per Class) model (Model) Required packages: robustDA Sparse Mixture Discriminant Analysis method = &#39;smda&#39; Type: Classification Tuning parameters: NumVars (# Predictors) lambda (Lambda) R (# Subclasses) Required packages: sparseLDA 7.0.26 Model Tree (back to contents) Cubist method = &#39;cubist&#39; Type: Regression Tuning parameters: committees (#Committees) neighbors (#Instances) Required packages: Cubist A model-specific variable importance metric is available. Logistic Model Trees method = &#39;LMT&#39; Type: Classification Tuning parameters: iter (# Iteratons) Required packages: RWeka Model Rules method = &#39;M5Rules&#39; Type: Regression Tuning parameters: pruned (Pruned) smoothed (Smoothed) Required packages: RWeka Model Tree method = &#39;M5&#39; Type: Regression Tuning parameters: pruned (Pruned) smoothed (Smoothed) rules (Rules) Required packages: RWeka 7.0.27 Multivariate Adaptive Regression Splines (back to contents) Bagged Flexible Discriminant Analysis method = &#39;bagFDA&#39; Type: Classification Tuning parameters: degree (Product Degree) nprune (#Terms) Required packages: earth, mda A model-specific variable importance metric is available. Bagged MARS method = &#39;bagEarth&#39; Type: Regression, Classification Tuning parameters: nprune (#Terms) degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Bagged MARS using gCV Pruning method = &#39;bagEarthGCV&#39; Type: Regression, Classification Tuning parameters: degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Flexible Discriminant Analysis method = &#39;fda&#39; Type: Classification Tuning parameters: degree (Product Degree) nprune (#Terms) Required packages: earth, mda A model-specific variable importance metric is available. Multivariate Adaptive Regression Spline method = &#39;earth&#39; Type: Regression, Classification Tuning parameters: nprune (#Terms) degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. Multivariate Adaptive Regression Splines method = &#39;gcvEarth&#39; Type: Regression, Classification Tuning parameters: degree (Product Degree) Required packages: earth A model-specific variable importance metric is available. 7.0.28 Neural Network (back to contents) Bayesian Regularized Neural Networks method = &#39;brnn&#39; Type: Regression Tuning parameters: neurons (# Neurons) Required packages: brnn Extreme Learning Machine method = &#39;elm&#39; Type: Classification, Regression Tuning parameters: nhid (#Hidden Units) actfun (Activation Function) Required packages: elmNN Model Averaged Neural Network method = &#39;avNNet&#39; Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) bag (Bagging) Required packages: nnet Multi-Layer Perceptron method = &#39;mlp&#39; Type: Regression, Classification Tuning parameters: size (#Hidden Units) Required packages: RSNNS Multi-Layer Perceptron method = &#39;mlpWeightDecay&#39; Type: Regression, Classification Tuning parameters: size (#Hidden Units) decay (Weight Decay) Required packages: RSNNS Multi-Layer Perceptron, multiple layers method = &#39;mlpWeightDecayML&#39; Type: Regression, Classification Tuning parameters: layer1 (#Hidden Units layer1) layer2 (#Hidden Units layer2) layer3 (#Hidden Units layer3) decay (Weight Decay) Required packages: RSNNS Multi-Layer Perceptron, with multiple layers method = &#39;mlpML&#39; Type: Regression, Classification Tuning parameters: layer1 (#Hidden Units layer1) layer2 (#Hidden Units layer2) layer3 (#Hidden Units layer3) Required packages: RSNNS Multilayer Perceptron Network by Stochastic Gradient Descent method = &#39;mlpSGD&#39; Type: Regression, Classification Tuning parameters: size (#Hidden Units) l2reg (L2 Regularization) lambda (RMSE Gradient Scaling) learn_rate (Learning Rate) momentum (Momentum) gamma (Decay) minibatchsz (Batch Size) repeats (#Models) Required packages: FCNN4R, plyr A model-specific variable importance metric is available. Neural Network method = &#39;neuralnet&#39; Type: Regression Tuning parameters: layer1 (#Hidden Units in Layer 1) layer2 (#Hidden Units in Layer 2) layer3 (#Hidden Units in Layer 3) Required packages: neuralnet Neural Network method = &#39;nnet&#39; Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) Required packages: nnet A model-specific variable importance metric is available. Neural Networks with Feature Extraction method = &#39;pcaNNet&#39; Type: Classification, Regression Tuning parameters: size (#Hidden Units) decay (Weight Decay) Required packages: nnet Penalized Multinomial Regression method = &#39;multinom&#39; Type: Classification Tuning parameters: decay (Weight Decay) Required packages: nnet A model-specific variable importance metric is available. Quantile Regression Neural Network method = &#39;qrnn&#39; Type: Regression Tuning parameters: n.hidden (#Hidden Units) penalty ( Weight Decay) bag (Bagged Models?) Required packages: qrnn Radial Basis Function Network method = &#39;rbf&#39; Type: Classification, Regression Tuning parameters: size (#Hidden Units) Required packages: RSNNS Radial Basis Function Network method = &#39;rbfDDA&#39; Type: Regression, Classification Tuning parameters: negativeThreshold (Activation Limit for Conflicting Classes) Required packages: RSNNS Stacked AutoEncoder Deep Neural Network method = &#39;dnn&#39; Type: Classification, Regression Tuning parameters: layer1 (Hidden Layer 1) layer2 (Hidden Layer 2) layer3 (Hidden Layer 3) hidden_dropout (Hidden Dropouts) visible_dropout (Visible Dropout) Required packages: deepnet 7.0.29 Oblique Tree (back to contents) Oblique Random Forest method = &#39;ORFlog&#39; Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Oblique Random Forest method = &#39;ORFpls&#39; Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Oblique Random Forest method = &#39;ORFridge&#39; Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Oblique Random Forest method = &#39;ORFsvm&#39; Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Oblique Trees method = &#39;oblique.tree&#39; Type: Classification Tuning parameters: oblique.splits (Oblique Splits) variable.selection (Variable Selection Method) Required packages: oblique.tree 7.0.30 Ordinal Outcomes (back to contents) Adjacent Categories Probability Model for Ordinal Data method = &#39;vglmAdjCat&#39; Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM CART or Ordinal Responses method = &#39;rpartScore&#39; Type: Classification Tuning parameters: cp (Complexity Parameter) split (Split Function) prune (Pruning Measure) Required packages: rpartScore, plyr A model-specific variable importance metric is available. Continuation Ratio Model for Ordinal Data method = &#39;vglmContRatio&#39; Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM Cumulative Probability Model for Ordinal Data method = &#39;vglmCumulative&#39; Type: Classification Tuning parameters: parallel (Parallel Curves) link (Link Function) Required packages: VGAM Ordered Logistic or Probit Regression method = &#39;polr&#39; Type: Classification Tuning parameters: method (parameter) Required packages: MASS A model-specific variable importance metric is available. 7.0.31 Partial Least Squares (back to contents) Ensemble Partial Least Squares Regression method = &#39;enpls&#39; Type: Regression Tuning parameters: maxcomp (Max. #Components) Required packages: enpls Ensemble Partial Least Squares Regression with Feature Selection method = &#39;enpls.fs&#39; Type: Regression Tuning parameters: maxcomp (Max. #Components) threshold (Importance Cutoff) Required packages: enpls Generalized Partial Least Squares method = &#39;gpls&#39; Type: Classification Tuning parameters: K.prov (#Components) Required packages: gpls Oblique Random Forest method = &#39;ORFpls&#39; Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Partial Least Squares method = &#39;kernelpls&#39; Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares method = &#39;pls&#39; Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares method = &#39;simpls&#39; Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares method = &#39;widekernelpls&#39; Type: Regression, Classification Tuning parameters: ncomp (#Components) Required packages: pls A model-specific variable importance metric is available. Partial Least Squares Generalized Linear Models method = &#39;plsRglm&#39; Type: Classification, Regression Tuning parameters: nt (#PLS Components) alpha.pvals.expli (p-Value threshold) Required packages: plsRglm Sparse Partial Least Squares method = &#39;spls&#39; Type: Regression, Classification Tuning parameters: K (#Components) eta (Threshold) kappa (Kappa) Required packages: spls 7.0.32 Polynomial Model (back to contents) Diagonal Discriminant Analysis method = &#39;dda&#39; Type: Classification Tuning parameters: model (Model) shrinkage (Shrinkage Type) Required packages: sparsediscrim Distance Weighted Discrimination with Polynomial Kernel method = &#39;dwdPoly&#39; Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) degree (Polynomial Degree) scale (Scale) Required packages: kerndwd Gaussian Process with Polynomial Kernel method = &#39;gaussprPoly&#39; Type: Regression, Classification Tuning parameters: degree (Polynomial Degree) scale (Scale) Required packages: kernlab High-Dimensional Regularized Discriminant Analysis method = &#39;hdrda&#39; Type: Classification Tuning parameters: gamma (Gamma) lambda (Lambda) shrinkage_type (Shrinkage Type) Required packages: sparsediscrim Least Squares Support Vector Machine with Polynomial Kernel method = &#39;lssvmPoly&#39; Type: Classification Tuning parameters: degree (Polynomial Degree) scale (Scale) tau (Regularization Parameter) Required packages: kernlab Penalized Discriminant Analysis method = &#39;pda&#39; Type: Classification Tuning parameters: lambda (Shrinkage Penalty Coefficient) Required packages: mda Penalized Discriminant Analysis method = &#39;pda2&#39; Type: Classification Tuning parameters: df (Degrees of Freedom) Required packages: mda Polynomial Kernel Regularized Least Squares method = &#39;krlsPoly&#39; Type: Regression Tuning parameters: lambda (Regularization Parameter) degree (Polynomial Degree) Required packages: KRLS Quadratic Discriminant Analysis method = &#39;qda&#39; Type: Classification No tuning parameters for this model Required packages: MASS Quadratic Discriminant Analysis with Stepwise Feature Selection method = &#39;stepQDA&#39; Type: Classification Tuning parameters: maxvar (Maximum #Variables) direction (Search Direction) Required packages: klaR, MASS Regularized Discriminant Analysis method = &#39;rda&#39; Type: Classification Tuning parameters: gamma (Gamma) lambda (Lambda) Required packages: klaR Regularized Linear Discriminant Analysis method = &#39;rlda&#39; Type: Classification Tuning parameters: estimator (Regularization Method) Required packages: sparsediscrim Relevance Vector Machines with Polynomial Kernel method = &#39;rvmPoly&#39; Type: Regression Tuning parameters: scale (Scale) degree (Polynomial Degree) Required packages: kernlab Robust Quadratic Discriminant Analysis method = &#39;QdaCov&#39; Type: Classification No tuning parameters for this model Required packages: rrcov Stepwise Diagonal Quadratic Discriminant Analysis method = &#39;sddaQDA&#39; Type: Classification No tuning parameters for this model Required packages: SDDA Support Vector Machines with Polynomial Kernel method = &#39;svmPoly&#39; Type: Regression, Classification Tuning parameters: degree (Polynomial Degree) scale (Scale) C (Cost) Required packages: kernlab 7.0.33 Prototype Models (back to contents) Cubist method = &#39;cubist&#39; Type: Regression Tuning parameters: committees (#Committees) neighbors (#Instances) Required packages: Cubist A model-specific variable importance metric is available. Greedy Prototype Selection method = &#39;protoclass&#39; Type: Classification Tuning parameters: eps (Ball Size) Minkowski (Distance Order) Required packages: proxy, protoclass k-Nearest Neighbors method = &#39;kknn&#39; Type: Regression, Classification Tuning parameters: kmax (Max. #Neighbors) distance (Distance) kernel (Kernel) Required packages: kknn k-Nearest Neighbors method = &#39;knn&#39; Type: Classification, Regression Tuning parameters: k (#Neighbors) Knn regression via sklearn.neighbors.KNeighborsRegressor method = &#39;pythonKnnReg&#39; Type: Regression Tuning parameters: n_neighbors (#Neighbors) weights (Weight Function) algorithm (Algorithm) leaf_size (Leaf Size) metric (Distance Metric) p (p) Required packages: rPython Learning Vector Quantization method = &#39;lvq&#39; Type: Classification Tuning parameters: size (Codebook Size) k (#Prototypes) Required packages: class Nearest Shrunken Centroids method = &#39;pam&#39; Type: Classification Tuning parameters: threshold (Shrinkage Threshold) Required packages: pamr A model-specific variable importance metric is available. Optimal Weighted Nearest Neighbor Classifier method = &#39;ownn&#39; Type: Classification Tuning parameters: K (#Neighbors) Required packages: snn Stabilized Nearest Neighbor Classifier method = &#39;snn&#39; Type: Classification Tuning parameters: lambda (Stabilization Parameter) Required packages: snn 7.0.34 Quantile Regression (back to contents) Non-Convex Penalized Quantile Regression method = &#39;rqnc&#39; Type: Regression Tuning parameters: lambda (L1 Penalty) penalty (Penalty Type) Required packages: rqPen Quantile Random Forest method = &#39;qrf&#39; Type: Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: quantregForest Quantile Regression Neural Network method = &#39;qrnn&#39; Type: Regression Tuning parameters: n.hidden (#Hidden Units) penalty ( Weight Decay) bag (Bagged Models?) Required packages: qrnn Quantile Regression with LASSO penalty method = &#39;rqlasso&#39; Type: Regression Tuning parameters: lambda (L1 Penalty) Required packages: rqPen 7.0.35 Radial Basis Function (back to contents) Distance Weighted Discrimination with Radial Basis Function Kernel method = &#39;dwdRadial&#39; Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) sigma (Sigma) Required packages: kernlab, kerndwd Gaussian Process with Radial Basis Function Kernel method = &#39;gaussprRadial&#39; Type: Regression, Classification Tuning parameters: sigma (Sigma) Required packages: kernlab Least Squares Support Vector Machine with Radial Basis Function Kernel method = &#39;lssvmRadial&#39; Type: Classification Tuning parameters: sigma (Sigma) tau (Regularization Parameter) Required packages: kernlab Radial Basis Function Kernel Regularized Least Squares method = &#39;krlsRadial&#39; Type: Regression Tuning parameters: lambda (Regularization Parameter) sigma (Sigma) Required packages: KRLS, kernlab Radial Basis Function Network method = &#39;rbf&#39; Type: Classification, Regression Tuning parameters: size (#Hidden Units) Required packages: RSNNS Radial Basis Function Network method = &#39;rbfDDA&#39; Type: Regression, Classification Tuning parameters: negativeThreshold (Activation Limit for Conflicting Classes) Required packages: RSNNS Relevance Vector Machines with Radial Basis Function Kernel method = &#39;rvmRadial&#39; Type: Regression Tuning parameters: sigma (Sigma) Required packages: kernlab Support Vector Machines with Class Weights method = &#39;svmRadialWeights&#39; Type: Classification Tuning parameters: sigma (Sigma) C (Cost) Weight (Weight) Required packages: kernlab Support Vector Machines with Radial Basis Function Kernel method = &#39;svmRadial&#39; Type: Regression, Classification Tuning parameters: sigma (Sigma) C (Cost) Required packages: kernlab Support Vector Machines with Radial Basis Function Kernel method = &#39;svmRadialCost&#39; Type: Regression, Classification Tuning parameters: C (Cost) Required packages: kernlab Support Vector Machines with Radial Basis Function Kernel method = &#39;svmRadialSigma&#39; Type: Regression, Classification Tuning parameters: sigma (Sigma) C (Cost) Required packages: kernlab Notes: This SVM model tunes over the cost parameter and the RBF kernel parameter sigma. In the latter case, using tuneLength will, at most, evaluate six values of the kernel parameter. This enables a broad search over the cost parameter and a relatively narrow search over sigma Variational Bayesian Multinomial Probit Regression method = &#39;vbmpRadial&#39; Type: Classification Tuning parameters: estimateTheta (Theta Estimated) Required packages: vbmp 7.0.36 Random Forest (back to contents) Conditional Inference Random Forest method = &#39;cforest&#39; Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: party A model-specific variable importance metric is available. Oblique Random Forest method = &#39;ORFlog&#39; Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Oblique Random Forest method = &#39;ORFpls&#39; Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Oblique Random Forest method = &#39;ORFridge&#39; Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Oblique Random Forest method = &#39;ORFsvm&#39; Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Parallel Random Forest method = &#39;parRF&#39; Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: e1071, randomForest, foreach A model-specific variable importance metric is available. Quantile Random Forest method = &#39;qrf&#39; Type: Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: quantregForest Random Ferns method = &#39;rFerns&#39; Type: Classification Tuning parameters: depth (Fern Depth) Required packages: rFerns Random Forest method = &#39;ranger&#39; Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: e1071, ranger A model-specific variable importance metric is available. Random Forest method = &#39;Rborist&#39; Type: Classification, Regression Tuning parameters: predFixed (#Randomly Selected Predictors) Required packages: Rborist A model-specific variable importance metric is available. Random Forest method = &#39;rf&#39; Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: randomForest A model-specific variable importance metric is available. Random Forest by Randomization method = &#39;extraTrees&#39; Type: Regression, Classification Tuning parameters: mtry (# Randomly Selected Predictors) numRandomCuts (# Random Cuts) Required packages: extraTrees Random Forest Rule-Based Model method = &#39;rfRules&#39; Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) maxdepth (Maximum Rule Depth) Required packages: randomForest, inTrees, plyr A model-specific variable importance metric is available. Random Forest with Additional Feature Selection method = &#39;Boruta&#39; Type: Regression, Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: Boruta, randomForest Regularized Random Forest method = &#39;RRF&#39; Type: Regression, Classification Tuning parameters: mtry (#Randomly Selected Predictors) coefReg (Regularization Value) coefImp (Importance Coefficient) Required packages: randomForest, RRF A model-specific variable importance metric is available. Regularized Random Forest method = &#39;RRFglobal&#39; Type: Regression, Classification Tuning parameters: mtry (#Randomly Selected Predictors) coefReg (Regularization Value) Required packages: RRF A model-specific variable importance metric is available. Weighted Subspace Random Forest method = &#39;wsrf&#39; Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: wsrf 7.0.37 Regularization (back to contents) Bayesian Regularized Neural Networks method = &#39;brnn&#39; Type: Regression Tuning parameters: neurons (# Neurons) Required packages: brnn Diagonal Discriminant Analysis method = &#39;dda&#39; Type: Classification Tuning parameters: model (Model) shrinkage (Shrinkage Type) Required packages: sparsediscrim Heteroscedastic Discriminant Analysis method = &#39;hda&#39; Type: Classification Tuning parameters: gamma (Gamma) lambda (Lambda) newdim (Dimension of the Discriminative Subspace) Required packages: hda High-Dimensional Regularized Discriminant Analysis method = &#39;hdrda&#39; Type: Classification Tuning parameters: gamma (Gamma) lambda (Lambda) shrinkage_type (Shrinkage Type) Required packages: sparsediscrim Regularized Discriminant Analysis method = &#39;rda&#39; Type: Classification Tuning parameters: gamma (Gamma) lambda (Lambda) Required packages: klaR Regularized Linear Discriminant Analysis method = &#39;rlda&#39; Type: Classification Tuning parameters: estimator (Regularization Method) Required packages: sparsediscrim Regularized Random Forest method = &#39;RRF&#39; Type: Regression, Classification Tuning parameters: mtry (#Randomly Selected Predictors) coefReg (Regularization Value) coefImp (Importance Coefficient) Required packages: randomForest, RRF A model-specific variable importance metric is available. Regularized Random Forest method = &#39;RRFglobal&#39; Type: Regression, Classification Tuning parameters: mtry (#Randomly Selected Predictors) coefReg (Regularization Value) Required packages: RRF A model-specific variable importance metric is available. Robust Regularized Linear Discriminant Analysis method = &#39;rrlda&#39; Type: Classification Tuning parameters: lambda (Penalty Parameter) hp (Robustness Parameter) penalty (Penalty Type) Required packages: rrlda Shrinkage Discriminant Analysis method = &#39;sda&#39; Type: Classification Tuning parameters: diagonal (Diagonalize) lambda (shrinkage) Required packages: sda 7.0.38 Relevance Vector Machines (back to contents) Relevance Vector Machines with Linear Kernel method = &#39;rvmLinear&#39; Type: Regression No tuning parameters for this model Required packages: kernlab Relevance Vector Machines with Polynomial Kernel method = &#39;rvmPoly&#39; Type: Regression Tuning parameters: scale (Scale) degree (Polynomial Degree) Required packages: kernlab Relevance Vector Machines with Radial Basis Function Kernel method = &#39;rvmRadial&#39; Type: Regression Tuning parameters: sigma (Sigma) Required packages: kernlab 7.0.39 Ridge Regression (back to contents) Oblique Random Forest method = &#39;ORFridge&#39; Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Ridge Regression with Variable Selection method = &#39;foba&#39; Type: Regression Tuning parameters: k (#Variables Retained) lambda (L2 Penalty) Required packages: foba 7.0.40 Robust Methods (back to contents) L2 Regularized Linear Support Vector Machines with Class Weights method = &#39;svmLinearWeights2&#39; Type: Classification Tuning parameters: cost (Cost) Loss (Loss Function) weight (Class Weight) Required packages: LiblineaR L2 Regularized Support Vector Machine (dual) with Linear Kernel method = &#39;svmLinear3&#39; Type: Regression, Classification Tuning parameters: cost (Cost) Loss (Loss Function) Required packages: LiblineaR Linear Support Vector Machines with Class Weights method = &#39;svmLinearWeights&#39; Type: Classification Tuning parameters: cost (Cost) weight (Class Weight) Required packages: e1071 Relevance Vector Machines with Linear Kernel method = &#39;rvmLinear&#39; Type: Regression No tuning parameters for this model Required packages: kernlab Relevance Vector Machines with Polynomial Kernel method = &#39;rvmPoly&#39; Type: Regression Tuning parameters: scale (Scale) degree (Polynomial Degree) Required packages: kernlab Relevance Vector Machines with Radial Basis Function Kernel method = &#39;rvmRadial&#39; Type: Regression Tuning parameters: sigma (Sigma) Required packages: kernlab Robust Mixture Discriminant Analysis method = &#39;rmda&#39; Type: Classification Tuning parameters: K (#Subclasses Per Class) model (Model) Required packages: robustDA Support Vector Machines with Boundrange String Kernel method = &#39;svmBoundrangeString&#39; Type: Regression, Classification Tuning parameters: length (length) C (Cost) Required packages: kernlab Support Vector Machines with Exponential String Kernel method = &#39;svmExpoString&#39; Type: Regression, Classification Tuning parameters: lambda (lambda) C (Cost) Required packages: kernlab Support Vector Machines with Linear Kernel method = &#39;svmLinear&#39; Type: Regression, Classification Tuning parameters: C (Cost) Required packages: kernlab Support Vector Machines with Linear Kernel method = &#39;svmLinear2&#39; Type: Regression, Classification Tuning parameters: cost (Cost) Required packages: e1071 Support Vector Machines with Polynomial Kernel method = &#39;svmPoly&#39; Type: Regression, Classification Tuning parameters: degree (Polynomial Degree) scale (Scale) C (Cost) Required packages: kernlab Support Vector Machines with Radial Basis Function Kernel method = &#39;svmRadial&#39; Type: Regression, Classification Tuning parameters: sigma (Sigma) C (Cost) Required packages: kernlab Support Vector Machines with Radial Basis Function Kernel method = &#39;svmRadialSigma&#39; Type: Regression, Classification Tuning parameters: sigma (Sigma) C (Cost) Required packages: kernlab Notes: This SVM model tunes over the cost parameter and the RBF kernel parameter sigma. In the latter case, using tuneLength will, at most, evaluate six values of the kernel parameter. This enables a broad search over the cost parameter and a relatively narrow search over sigma Support Vector Machines with Spectrum String Kernel method = &#39;svmSpectrumString&#39; Type: Regression, Classification Tuning parameters: length (length) C (Cost) Required packages: kernlab 7.0.41 Robust Model (back to contents) Quantile Random Forest method = &#39;qrf&#39; Type: Regression Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: quantregForest Quantile Regression Neural Network method = &#39;qrnn&#39; Type: Regression Tuning parameters: n.hidden (#Hidden Units) penalty ( Weight Decay) bag (Bagged Models?) Required packages: qrnn Robust Linear Discriminant Analysis method = &#39;Linda&#39; Type: Classification No tuning parameters for this model Required packages: rrcov Robust Linear Model method = &#39;rlm&#39; Type: Regression Tuning parameters: intercept (intercept) psi (psi) Required packages: MASS Robust Regularized Linear Discriminant Analysis method = &#39;rrlda&#39; Type: Classification Tuning parameters: lambda (Penalty Parameter) hp (Robustness Parameter) penalty (Penalty Type) Required packages: rrlda Robust SIMCA method = &#39;RSimca&#39; Type: Classification No tuning parameters for this model Required packages: rrcovHD SIMCA method = &#39;CSimca&#39; Type: Classification No tuning parameters for this model Required packages: rrcovHD 7.0.42 ROC Curves (back to contents) ROC-Based Classifier method = &#39;rocc&#39; Type: Classification Tuning parameters: xgenes (#Variables Retained) Required packages: rocc 7.0.43 Rule-Based Model (back to contents) Adaptive-Network-Based Fuzzy Inference System method = &#39;ANFIS&#39; Type: Regression Tuning parameters: num.labels (#Fuzzy Terms) max.iter (Max. Iterations) Required packages: frbs C5.0 method = &#39;C5.0&#39; Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) Required packages: C50, plyr A model-specific variable importance metric is available. Cost-Sensitive C5.0 method = &#39;C5.0Cost&#39; Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) cost (Cost) Required packages: C50, plyr A model-specific variable importance metric is available. Cubist method = &#39;cubist&#39; Type: Regression Tuning parameters: committees (#Committees) neighbors (#Instances) Required packages: Cubist A model-specific variable importance metric is available. Dynamic Evolving Neural-Fuzzy Inference System method = &#39;DENFIS&#39; Type: Regression Tuning parameters: Dthr (Threshold) max.iter (Max. Iterations) Required packages: frbs Fuzzy Inference Rules by Descent Method method = &#39;FIR.DM&#39; Type: Regression Tuning parameters: num.labels (#Fuzzy Terms) max.iter (Max. Iterations) Required packages: frbs Fuzzy Rules Using Chi’s Method method = &#39;FRBCS.CHI&#39; Type: Classification Tuning parameters: num.labels (#Fuzzy Terms) type.mf (Membership Function) Required packages: frbs Fuzzy Rules Using Genetic Cooperative-Competitive Learning method = &#39;GFS.GCCL&#39; Type: Classification Tuning parameters: num.labels (#Fuzzy Terms) popu.size (Population Size) max.gen (Max. Generations) Required packages: frbs Fuzzy Rules Using Genetic Cooperative-Competitive Learning and Pittsburgh method = &#39;FH.GBML&#39; Type: Classification Tuning parameters: max.num.rule (Max. #Rules) popu.size (Population Size) max.gen (Max. Generations) Required packages: frbs Fuzzy Rules Using the Structural Learning Algorithm on Vague Environment method = &#39;SLAVE&#39; Type: Classification Tuning parameters: num.labels (#Fuzzy Terms) max.iter (Max. Iterations) max.gen (Max. Generations) Required packages: frbs Fuzzy Rules via MOGUL method = &#39;GFS.FR.MOGUL&#39; Type: Regression Tuning parameters: max.gen (Max. Generations) max.iter (Max. Iterations) max.tune (Max. Tuning Iterations) Required packages: frbs Fuzzy Rules via Thrift method = &#39;GFS.THRIFT&#39; Type: Regression Tuning parameters: popu.size (Population Size) num.labels (# Fuzzy Labels) max.gen (Max. Generations) Required packages: frbs Fuzzy Rules with Weight Factor method = &#39;FRBCS.W&#39; Type: Classification Tuning parameters: num.labels (#Fuzzy Terms) type.mf (Membership Function) Required packages: frbs Genetic Lateral Tuning and Rule Selection of Linguistic Fuzzy Systems method = &#39;GFS.LT.RS&#39; Type: Regression Tuning parameters: popu.size (Population Size) num.labels (# Fuzzy Labels) max.gen (Max. Generations) Required packages: frbs Hybrid Neural Fuzzy Inference System method = &#39;HYFIS&#39; Type: Regression Tuning parameters: num.labels (#Fuzzy Terms) max.iter (Max. Iterations) Required packages: frbs Model Rules method = &#39;M5Rules&#39; Type: Regression Tuning parameters: pruned (Pruned) smoothed (Smoothed) Required packages: RWeka Model Tree method = &#39;M5&#39; Type: Regression Tuning parameters: pruned (Pruned) smoothed (Smoothed) rules (Rules) Required packages: RWeka Random Forest Rule-Based Model method = &#39;rfRules&#39; Type: Classification, Regression Tuning parameters: mtry (#Randomly Selected Predictors) maxdepth (Maximum Rule Depth) Required packages: randomForest, inTrees, plyr A model-specific variable importance metric is available. Rule-Based Classifier method = &#39;JRip&#39; Type: Classification Tuning parameters: NumOpt (# Optimizations) NumFolds (# Folds) MinWeights (Min Weights) Required packages: RWeka A model-specific variable importance metric is available. Rule-Based Classifier method = &#39;PART&#39; Type: Classification Tuning parameters: threshold (Confidence Threshold) pruned (Pruning) Required packages: RWeka A model-specific variable importance metric is available. Simplified TSK Fuzzy Rules method = &#39;FS.HGD&#39; Type: Regression Tuning parameters: num.labels (#Fuzzy Terms) max.iter (Max. Iterations) Required packages: frbs Single C5.0 Ruleset method = &#39;C5.0Rules&#39; Type: Classification No tuning parameters for this model Required packages: C50 A model-specific variable importance metric is available. Single Rule Classification method = &#39;OneR&#39; Type: Classification No tuning parameters for this model Required packages: RWeka Subtractive Clustering and Fuzzy c-Means Rules method = &#39;SBC&#39; Type: Regression Tuning parameters: r.a (Radius) eps.high (Upper Threshold) eps.low (Lower Threshold) Required packages: frbs Wang and Mendel Fuzzy Rules method = &#39;WM&#39; Type: Regression Tuning parameters: num.labels (#Fuzzy Terms) type.mf (Membership Function) Required packages: frbs 7.0.44 Self-Organising Maps (back to contents) Self-Organizing Map method = &#39;bdk&#39; Type: Classification, Regression Tuning parameters: xdim (Row) ydim (Columns) xweight (X Weight) topo (Topology) Required packages: kohonen Self-Organizing Maps method = &#39;xyf&#39; Type: Classification, Regression Tuning parameters: xdim (Row) ydim (Columns) xweight (X Weight) topo (Topology) Required packages: kohonen 7.0.45 String Kernel (back to contents) Support Vector Machines with Boundrange String Kernel method = &#39;svmBoundrangeString&#39; Type: Regression, Classification Tuning parameters: length (length) C (Cost) Required packages: kernlab Support Vector Machines with Exponential String Kernel method = &#39;svmExpoString&#39; Type: Regression, Classification Tuning parameters: lambda (lambda) C (Cost) Required packages: kernlab Support Vector Machines with Spectrum String Kernel method = &#39;svmSpectrumString&#39; Type: Regression, Classification Tuning parameters: length (length) C (Cost) Required packages: kernlab 7.0.46 Support Vector Machines (back to contents) L2 Regularized Linear Support Vector Machines with Class Weights method = &#39;svmLinearWeights2&#39; Type: Classification Tuning parameters: cost (Cost) Loss (Loss Function) weight (Class Weight) Required packages: LiblineaR L2 Regularized Support Vector Machine (dual) with Linear Kernel method = &#39;svmLinear3&#39; Type: Regression, Classification Tuning parameters: cost (Cost) Loss (Loss Function) Required packages: LiblineaR Least Squares Support Vector Machine method = &#39;lssvmLinear&#39; Type: Classification Tuning parameters: tau (Regularization Parameter) Required packages: kernlab Least Squares Support Vector Machine with Polynomial Kernel method = &#39;lssvmPoly&#39; Type: Classification Tuning parameters: degree (Polynomial Degree) scale (Scale) tau (Regularization Parameter) Required packages: kernlab Least Squares Support Vector Machine with Radial Basis Function Kernel method = &#39;lssvmRadial&#39; Type: Classification Tuning parameters: sigma (Sigma) tau (Regularization Parameter) Required packages: kernlab Linear Support Vector Machines with Class Weights method = &#39;svmLinearWeights&#39; Type: Classification Tuning parameters: cost (Cost) weight (Class Weight) Required packages: e1071 Support Vector Machines with Boundrange String Kernel method = &#39;svmBoundrangeString&#39; Type: Regression, Classification Tuning parameters: length (length) C (Cost) Required packages: kernlab Support Vector Machines with Class Weights method = &#39;svmRadialWeights&#39; Type: Classification Tuning parameters: sigma (Sigma) C (Cost) Weight (Weight) Required packages: kernlab Support Vector Machines with Exponential String Kernel method = &#39;svmExpoString&#39; Type: Regression, Classification Tuning parameters: lambda (lambda) C (Cost) Required packages: kernlab Support Vector Machines with Linear Kernel method = &#39;svmLinear&#39; Type: Regression, Classification Tuning parameters: C (Cost) Required packages: kernlab Support Vector Machines with Linear Kernel method = &#39;svmLinear2&#39; Type: Regression, Classification Tuning parameters: cost (Cost) Required packages: e1071 Support Vector Machines with Polynomial Kernel method = &#39;svmPoly&#39; Type: Regression, Classification Tuning parameters: degree (Polynomial Degree) scale (Scale) C (Cost) Required packages: kernlab Support Vector Machines with Radial Basis Function Kernel method = &#39;svmRadial&#39; Type: Regression, Classification Tuning parameters: sigma (Sigma) C (Cost) Required packages: kernlab Support Vector Machines with Radial Basis Function Kernel method = &#39;svmRadialCost&#39; Type: Regression, Classification Tuning parameters: C (Cost) Required packages: kernlab Support Vector Machines with Radial Basis Function Kernel method = &#39;svmRadialSigma&#39; Type: Regression, Classification Tuning parameters: sigma (Sigma) C (Cost) Required packages: kernlab Notes: This SVM model tunes over the cost parameter and the RBF kernel parameter sigma. In the latter case, using tuneLength will, at most, evaluate six values of the kernel parameter. This enables a broad search over the cost parameter and a relatively narrow search over sigma Support Vector Machines with Spectrum String Kernel method = &#39;svmSpectrumString&#39; Type: Regression, Classification Tuning parameters: length (length) C (Cost) Required packages: kernlab 7.0.47 Text Mining (back to contents) Support Vector Machines with Boundrange String Kernel method = &#39;svmBoundrangeString&#39; Type: Regression, Classification Tuning parameters: length (length) C (Cost) Required packages: kernlab Support Vector Machines with Exponential String Kernel method = &#39;svmExpoString&#39; Type: Regression, Classification Tuning parameters: lambda (lambda) C (Cost) Required packages: kernlab Support Vector Machines with Spectrum String Kernel method = &#39;svmSpectrumString&#39; Type: Regression, Classification Tuning parameters: length (length) C (Cost) Required packages: kernlab 7.0.48 Tree-Based Model (back to contents) AdaBoost Classification Trees method = &#39;adaboost&#39; Type: Classification Tuning parameters: nIter (#Trees) method (Method) Required packages: fastAdaboost AdaBoost.M1 method = &#39;AdaBoost.M1&#39; Type: Classification Tuning parameters: mfinal (#Trees) maxdepth (Max Tree Depth) coeflearn (Coefficient Type) Required packages: adabag, plyr A model-specific variable importance metric is available. Bagged AdaBoost method = &#39;AdaBag&#39; Type: Classification Tuning parameters: mfinal (#Trees) maxdepth (Max Tree Depth) Required packages: adabag, plyr A model-specific variable importance metric is available. Bagged CART method = &#39;treebag&#39; Type: Regression, Classification No tuning parameters for this model Required packages: ipred, plyr, e1071 A model-specific variable importance metric is available. Bayesian Additive Regression Trees method = &#39;bartMachine&#39; Type: Classification, Regression Tuning parameters: num_trees (#Trees) k (Prior Boundary) alpha (Base Terminal Node Hyperparameter) beta (Power Terminal Node Hyperparameter) nu (Degrees of Freedom) Required packages: bartMachine A model-specific variable importance metric is available. Boosted Classification Trees method = &#39;ada&#39; Type: Classification Tuning parameters: iter (#Trees) maxdepth (Max Tree Depth) nu (Learning Rate) Required packages: ada, plyr Boosted Logistic Regression method = &#39;LogitBoost&#39; Type: Classification Tuning parameters: nIter (# Boosting Iterations) Required packages: caTools Boosted Tree method = &#39;blackboost&#39; Type: Regression, Classification Tuning parameters: mstop (#Trees) maxdepth (Max Tree Depth) Required packages: party, mboost, plyr Boosted Tree method = &#39;bstTree&#39; Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) maxdepth (Max Tree Depth) nu (Shrinkage) Required packages: bst, plyr C4.5-like Trees method = &#39;J48&#39; Type: Classification Tuning parameters: C (Confidence Threshold) M (Minimum Instances Per Leaf) Required packages: RWeka C5.0 method = &#39;C5.0&#39; Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) Required packages: C50, plyr A model-specific variable importance metric is available. CART method = &#39;rpart&#39; Type: Regression, Classification Tuning parameters: cp (Complexity Parameter) Required packages: rpart A model-specific variable importance metric is available. CART method = &#39;rpart1SE&#39; Type: Regression, Classification No tuning parameters for this model Required packages: rpart A model-specific variable importance metric is available. Notes: This CART model replicates the same process used by the rpart function where the model complexity is determined using the one-standard error method. This procedure is replicated inside of the resampling done by train so that an external resampling estimate can be obtained. CART method = &#39;rpart2&#39; Type: Regression, Classification Tuning parameters: maxdepth (Max Tree Depth) Required packages: rpart A model-specific variable importance metric is available. CART or Ordinal Responses method = &#39;rpartScore&#39; Type: Classification Tuning parameters: cp (Complexity Parameter) split (Split Function) prune (Pruning Measure) Required packages: rpartScore, plyr A model-specific variable importance metric is available. CHi-squared Automated Interaction Detection method = &#39;chaid&#39; Type: Classification Tuning parameters: alpha2 (Merging Threshold) alpha3 (Splitting former Merged Threshold) alpha4 ( Splitting former Merged Threshold) Required packages: CHAID Conditional Inference Tree method = &#39;ctree&#39; Type: Classification, Regression Tuning parameters: mincriterion (1 - P-Value Threshold) Required packages: party Conditional Inference Tree method = &#39;ctree2&#39; Type: Regression, Classification Tuning parameters: maxdepth (Max Tree Depth) mincriterion (1 - P-Value Threshold) Required packages: party Cost-Sensitive C5.0 method = &#39;C5.0Cost&#39; Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) cost (Cost) Required packages: C50, plyr A model-specific variable importance metric is available. Cost-Sensitive CART method = &#39;rpartCost&#39; Type: Classification Tuning parameters: cp (Complexity Parameter) Cost (Cost) Required packages: rpart DeepBoost method = &#39;deepboost&#39; Type: Classification Tuning parameters: num_iter (# Boosting Iterations) tree_depth (Tree Depth) beta (L1 Regularization) lambda (Tree Depth Regularization) loss_type (Loss) Required packages: deepboost eXtreme Gradient Boosting method = &#39;xgbTree&#39; Type: Regression, Classification Tuning parameters: nrounds (# Boosting Iterations) max_depth (Max Tree Depth) eta (Shrinkage) gamma (Minimum Loss Reduction) colsample_bytree (Subsample Ratio of Columns) min_child_weight (Minimum Sum of Instance Weight) subsample (Subsample Percentage) Required packages: xgboost, plyr A model-specific variable importance metric is available. glmnet method = &#39;gbm_h2o&#39; Type: Regression, Classification Tuning parameters: ntrees (# Boosting Iterations) max_depth (Max Tree Depth) min_rows (Min. Terminal Node Size) learn_rate (Shrinkage) col_sample_rate (#Randomly Selected Predictors) Required packages: h2o A model-specific variable importance metric is available. Model Tree method = &#39;M5&#39; Type: Regression Tuning parameters: pruned (Pruned) smoothed (Smoothed) rules (Rules) Required packages: RWeka Oblique Trees method = &#39;oblique.tree&#39; Type: Classification Tuning parameters: oblique.splits (Oblique Splits) variable.selection (Variable Selection Method) Required packages: oblique.tree Random Forest with Additional Feature Selection method = &#39;Boruta&#39; Type: Regression, Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: Boruta, randomForest Rotation Forest method = &#39;rotationForest&#39; Type: Classification Tuning parameters: K (#Variable Subsets) L (Ensemble Size) Required packages: rotationForest A model-specific variable importance metric is available. Rotation Forest method = &#39;rotationForestCp&#39; Type: Classification Tuning parameters: K (#Variable Subsets) L (Ensemble Size) cp (Complexity Parameter) Required packages: rpart, plyr, rotationForest A model-specific variable importance metric is available. Single C5.0 Tree method = &#39;C5.0Tree&#39; Type: Classification No tuning parameters for this model Required packages: C50 A model-specific variable importance metric is available. Stochastic Gradient Boosting method = &#39;gbm&#39; Type: Regression, Classification Tuning parameters: n.trees (# Boosting Iterations) interaction.depth (Max Tree Depth) shrinkage (Shrinkage) n.minobsinnode (Min. Terminal Node Size) Required packages: gbm, plyr A model-specific variable importance metric is available. Tree Models from Genetic Algorithms method = &#39;evtree&#39; Type: Regression, Classification Tuning parameters: alpha (Complexity Parameter) Required packages: evtree Tree-Based Ensembles method = &#39;nodeHarvest&#39; Type: Regression, Classification Tuning parameters: maxinter (Maximum Interaction Depth) mode (Prediction Mode) Required packages: nodeHarvest 7.0.49 Two Class Only (back to contents) AdaBoost Classification Trees method = &#39;adaboost&#39; Type: Classification Tuning parameters: nIter (#Trees) method (Method) Required packages: fastAdaboost Bagged Logic Regression method = &#39;logicBag&#39; Type: Regression, Classification Tuning parameters: nleaves (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: logicFS Bayesian Additive Regression Trees method = &#39;bartMachine&#39; Type: Classification, Regression Tuning parameters: num_trees (#Trees) k (Prior Boundary) alpha (Base Terminal Node Hyperparameter) beta (Power Terminal Node Hyperparameter) nu (Degrees of Freedom) Required packages: bartMachine A model-specific variable importance metric is available. Binary Discriminant Analysis method = &#39;binda&#39; Type: Classification Tuning parameters: lambda.freqs (Shrinkage Intensity) Required packages: binda Boosted Classification Trees method = &#39;ada&#39; Type: Classification Tuning parameters: iter (#Trees) maxdepth (Max Tree Depth) nu (Learning Rate) Required packages: ada, plyr Boosted Generalized Additive Model method = &#39;gamboost&#39; Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: mboost, plyr Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mstop. If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. Boosted Generalized Linear Model method = &#39;glmboost&#39; Type: Regression, Classification Tuning parameters: mstop (# Boosting Iterations) prune (AIC Prune?) Required packages: plyr, mboost A model-specific variable importance metric is available. Notes: The prune option for this model enables the number of iterations to be determined by the optimal AIC value across all iterations. See the examples in ?mstop. If pruning is not used, the ensemble makes predictions using the exact value of the mstop tuning parameter value. CHi-squared Automated Interaction Detection method = &#39;chaid&#39; Type: Classification Tuning parameters: alpha2 (Merging Threshold) alpha3 (Splitting former Merged Threshold) alpha4 ( Splitting former Merged Threshold) Required packages: CHAID Cost-Sensitive C5.0 method = &#39;C5.0Cost&#39; Type: Classification Tuning parameters: trials (# Boosting Iterations) model (Model Type) winnow (Winnow) cost (Cost) Required packages: C50, plyr A model-specific variable importance metric is available. Cost-Sensitive CART method = &#39;rpartCost&#39; Type: Classification Tuning parameters: cp (Complexity Parameter) Cost (Cost) Required packages: rpart DeepBoost method = &#39;deepboost&#39; Type: Classification Tuning parameters: num_iter (# Boosting Iterations) tree_depth (Tree Depth) beta (L1 Regularization) lambda (Tree Depth Regularization) loss_type (Loss) Required packages: deepboost Distance Weighted Discrimination with Polynomial Kernel method = &#39;dwdPoly&#39; Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) degree (Polynomial Degree) scale (Scale) Required packages: kerndwd Distance Weighted Discrimination with Radial Basis Function Kernel method = &#39;dwdRadial&#39; Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) sigma (Sigma) Required packages: kernlab, kerndwd Generalized Linear Model method = &#39;glm&#39; Type: Regression, Classification No tuning parameters for this model A model-specific variable importance metric is available. Generalized Linear Model with Stepwise Feature Selection method = &#39;glmStepAIC&#39; Type: Regression, Classification No tuning parameters for this model Required packages: MASS glmnet method = &#39;glmnet_h2o&#39; Type: Regression, Classification Tuning parameters: alpha (Mixing Percentage) lambda (Regularization Parameter) Required packages: h2o A model-specific variable importance metric is available. L2 Regularized Linear Support Vector Machines with Class Weights method = &#39;svmLinearWeights2&#39; Type: Classification Tuning parameters: cost (Cost) Loss (Loss Function) weight (Class Weight) Required packages: LiblineaR Linear Distance Weighted Discrimination method = &#39;dwdLinear&#39; Type: Classification Tuning parameters: lambda (Regularization Parameter) qval (q) Required packages: kerndwd Linear Support Vector Machines with Class Weights method = &#39;svmLinearWeights&#39; Type: Classification Tuning parameters: cost (Cost) weight (Class Weight) Required packages: e1071 Logic Regression method = &#39;logreg&#39; Type: Regression, Classification Tuning parameters: treesize (Maximum Number of Leaves) ntrees (Number of Trees) Required packages: LogicReg Oblique Random Forest method = &#39;ORFlog&#39; Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Oblique Random Forest method = &#39;ORFpls&#39; Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Oblique Random Forest method = &#39;ORFridge&#39; Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Oblique Random Forest method = &#39;ORFsvm&#39; Type: Classification Tuning parameters: mtry (#Randomly Selected Predictors) Required packages: obliqueRF Partial Least Squares Generalized Linear Models method = &#39;plsRglm&#39; Type: Classification, Regression Tuning parameters: nt (#PLS Components) alpha.pvals.expli (p-Value threshold) Required packages: plsRglm Rotation Forest method = &#39;rotationForest&#39; Type: Classification Tuning parameters: K (#Variable Subsets) L (Ensemble Size) Required packages: rotationForest A model-specific variable importance metric is available. Rotation Forest method = &#39;rotationForestCp&#39; Type: Classification Tuning parameters: K (#Variable Subsets) L (Ensemble Size) cp (Complexity Parameter) Required packages: rpart, plyr, rotationForest A model-specific variable importance metric is available. Support Vector Machines with Class Weights method = &#39;svmRadialWeights&#39; Type: Classification Tuning parameters: sigma (Sigma) C (Cost) Weight (Weight) Required packages: kernlab Tree-Based Ensembles method = &#39;nodeHarvest&#39; Type: Regression, Classification Tuning parameters: maxinter (Maximum Interaction Depth) mode (Prediction Mode) Required packages: nodeHarvest "],
["models-clustered-by-tag-similarity.html", "8 Models Clustered by Tag Similarity", " 8 Models Clustered by Tag Similarity This page shows a network diagram of all the models that can be accessed by train. See the Revolutions blog for details about how this visualization was made (and this page has updated code using the networkD3 package). In summary, the package annotates each model by a set of tags (e.g. “Bagging”, “L1 Regularization” etc.). Using this information we can cluster models that are similar to each other. Green circles are models only used for regression, blue is classification only and orange is “dual use”. Hover over a circle to get the model name and the model code used by the caret package and refreshing the screen will re-configure the layout. You may need to move a node to the left to see the whole name. 21 models without connections are not shown in the graph. The data used to create this graph can be found here. The plot below shows the similarity matrix. Hover over a cell to see the pair of models and their Jaccard similarity. Darker colors indicate similar models. You can also use it along with maximum dissimilarity sampling to pick out a diverse set of models. Suppose you would like to use a SVM model with a radial basis function on some regression data. Based on these tags, what other four models would constitute the most diverse set? tag &lt;- read.csv(&quot;tag_data.csv&quot;, row.names = 1) tag &lt;- as.matrix(tag) ## Select only models for regression regModels &lt;- tag[tag[,&quot;Regression&quot;] == 1,] all &lt;- 1:nrow(regModels) ## Seed the analysis with the SVM model start &lt;- grep(&quot;(svmRadial)&quot;, rownames(regModels), fixed = TRUE) pool &lt;- all[all != start] ## Select 4 model models by maximizing the Jaccard ## dissimilarity between sets of models nextMods &lt;- maxDissim(regModels[start,,drop = FALSE], regModels[pool, ], method = &quot;Jaccard&quot;, n = 4) rownames(regModels)[c(start, nextMods)] ## [1] &quot;Support Vector Machines with Radial Basis Function Kernel (svmRadial)&quot; ## [2] &quot;Cubist (cubist)&quot; ## [3] &quot;Bayesian Regularized Neural Networks (brnn)&quot; ## [4] &quot;Negative Binomial Generalized Linear Model (glm.nb)&quot; ## [5] &quot;Logic Regression (logreg)&quot; "],
["parallel-processing.html", "9 Parallel Processing", " 9 Parallel Processing In this package, resampling is primary approach for optimizing predictive models with tuning parameters. To do this, many alternate versions of the training set are used to train the model and predict a hold-out set. This process is repeated many times to get performance estimates that generalize to new data sets. Each of the resampled data sets is independent of the others, so there is no formal requirement that the models must be run sequentially. If a computer with multiple processors or cores is available, the computations could be spread across these “workers” to increase the computational efficiency. caret leverages one of the parallel processing frameworks in R to do just this. The foreach package allows R code to be run either sequentially or in parallel using several different technologies, such as the multicore or Rmpi packages (see Schmidberger et al, 2009 for summaries and descriptions of the available options). There are several R packages that work with foreach to implement these techniques, such as doMC (for multicore) or doMPI (for Rmpi). To tune a predictive model using multiple workers, the function syntax in the caret package functions (e.g. train, rfe or sbf) do not change. A separate function is used to “register” the parallel processing technique and specify the number of workers to use. For example, to use the multicore) package (not available on Windows) with five cores on the same machine, the package is loaded and them registered: library(doMC) registerDoMC(cores = 5) ## All subsequent models are then run in parallel model &lt;- train(y ~ ., data = training, method = &quot;rf&quot;) The syntax for other packages associated with foreach is very similar. Note that as the number of workers increases, the memory required also increase. For example, using five workers would keep a total of six versions of the data in memory. If the data are large or the computational model is demanding, performance can be affected if the amount of required memory exceeds the physical amount available. Also, for rfe and sbf, these functions may call train for some models. In this case, registering M workers will actually invoke M2 total processes. Does this help reduce the time to fit models? A moderately sized data set (4331 rows and 8) was modeled multiple times with different number of workers for several models. Random forest was used with 2000 trees and tuned over 10 values of mtry. Variable importance calculations were also conducted during each model fit. Linear discriminant analysis was also run, as was a cost-sensitive radial basis function support vector machine (tuned over 15 cost values). All models were tuned using five repeats of 10-fold cross-validation. The results are shown in the figure below. The y-axis corresponds to the total execution time (encompassing model tuning and the final model fit) versus the number of workers. Random forest clearly took the longest to train and the LDA models were very computationally efficient. The total time (in minutes) decreased as the number of workers increase but stabilized around seven workers. The data for this plot were generated in a randomized fashion so that there should be no bias in the run order. The bottom right panel shows the speed-up which is the sequential time divided by the parallel time. For example, a speed-up of three indicates that the parallel version was three times faster than the sequential version. At best, parallelization can achieve linear speed-ups; that is, for M workers, the parallel time is 1/M. For these models, the speed-up is close to linear until four or five workers are used. After this, there is a small improvement in performance. Since LDA is already computationally efficient, the speed-up levels off more rapidly than the other models. While not linear, the decrease in execution time is helpful - a nearly 10 hour model fit was decreased to about 90 minutes. Note that some models, especially those using the RWeka package, may not be able to be run in parallel due to the underlying code structure. train, rfe, sbf, bag and avNNet were given an additional argument in their respective control files called allowParallel that defaults to TRUE. When TRUE, the code will be executed in parallel if a parallel backend (e.g. doMC) is registered. When allowParallel`` = FALSE, the parallel backend is always ignored. The use case is when rfe or sbf calls train. If a parallel backend with P processors is being used, the combination of these functions will create P2 processes. Since some operations benefit more from parallelization than others, the user has the ability to concentrate computing resources for specific functions. One additional “trick” that train exploits to increase computational efficiency is to use sub-models; a single model fit can produce predictions for multiple tuning parameters. For example, in most implementations of boosted models, a model trained on B boosting iterations can produce predictions for models for iterations less than B. Suppose a gbm model was tuned over the following grid: gbmGrid &lt;- expand.grid(interaction.depth = c(1, 5, 9), n.trees = (1:15)*100, shrinkage = 0.1, n.minobsinnode = 20) In reality, train only created objects for 3 models and derived the other predictions from these objects. This trick is used for the following models: ada, AdaBag, AdaBoost.M1, bagEarth, blackboost, blasso, BstLm, bstSm, bstTree, C5.0, C5.0Cost, cubist, earth, enet, foba, gamboost, gbm, glmboost, glmnet, kernelpls, lars, lars2, lasso, lda2, leapBackward, leapForward, leapSeq, LogitBoost, pam, partDSA, pcr, PenalizedLDA, pls, relaxo, rfRules, rotationForest, rotationForestCp, rpart, rpart2, rpartCost, simpls, spikeslab, superpc, widekernelpls, xgbTree. "],
["random-hyperparameter-search.html", "10 Random Hyperparameter Search", " 10 Random Hyperparameter Search The default method for optimizing tuning parameters in train is to use a grid search. This approach is usually effective but, in cases when there are many tuning parameters, it can be inefficient. An alternative is to use a combination of grid search and racing. Another is to use a random selection of tuning parameter combinations to cover the parameter space to a lesser extent. There are a number of models where this can be beneficial in finding reasonable values of the tuning parameters in a relatively short time. However, there are some models where the efficiency in a small search field can cancel out other optimizations. For example, a number of models in caret utilize the “sub-model trick” where M tuning parameter combinations are evaluated, potentially far fewer than M model fits are required. This approach is best leveraged when a simple grid search is used. For this reason, it may be inefficient to use random search for the following model codes: ada, AdaBag, AdaBoost.M1, bagEarth, blackboost, blasso, BstLm, bstSm, bstTree, C5.0, C5.0Cost, cubist, earth, enet, foba, gamboost, gbm, glmboost, glmnet, kernelpls, lars, lars2, lasso, lda2, leapBackward, leapForward, leapSeq, LogitBoost, pam, partDSA, pcr, PenalizedLDA, pls, relaxo, rfRules, rotationForest, rotationForestCp, rpart, rpart2, rpartCost, simpls, spikeslab, superpc, widekernelpls, xgbTree. Finally, many of the models wrapped by train have a small number of parameters. The average number of parameters is 1.8. To use random search, another option is available in trainControl called search. Possible values of this argument are &quot;grid&quot; and &quot;random&quot;. The built-in models contained in caret contain code to generate random tuning parameter combinations. The total number of unique combinations is specified by the tuneLength option to train. Again, we will use the sonar data from the previous training page to demonstrate the method with a regularized discriminant analysis by looking at a total of 30 tuning parameter combinations: library(mlbench) data(Sonar) library(caret) set.seed(998) inTraining &lt;- createDataPartition(Sonar$Class, p = .75, list = FALSE) training &lt;- Sonar[ inTraining,] testing &lt;- Sonar[-inTraining,] fitControl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 10, classProbs = TRUE, summaryFunction = twoClassSummary, search = &quot;random&quot;) set.seed(825) rda_fit &lt;- train(Class ~ ., data = training, method = &quot;rda&quot;, metric = &quot;ROC&quot;, tuneLength = 30, trControl = fitControl) rda_fit ## Regularized Discriminant Analysis ## ## 157 samples ## 60 predictor ## 2 classes: &#39;M&#39;, &#39;R&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 10 times) ## Summary of sample sizes: 142, 142, 140, 142, 142, 141, ... ## Resampling results across tuning parameters: ## ## gamma lambda ROC Sens Spec ## 0.03989739 0.90079034 0.8876811 0.8459722 0.7667857 ## 0.04968076 0.92570716 0.8816245 0.8425000 0.7610714 ## 0.07268784 0.24747192 0.9153125 0.8893056 0.8107143 ## 0.15727695 0.92403079 0.8842262 0.8636111 0.7696429 ## 0.18095571 0.19419752 0.9138318 0.8895833 0.7907143 ## 0.25251704 0.59742978 0.9184152 0.9254167 0.7891071 ## 0.29954232 0.63759620 0.9133681 0.9169444 0.7710714 ## 0.31520463 0.96521251 0.8743998 0.8511111 0.7773214 ## 0.33009668 0.81815445 0.8950967 0.8937500 0.7746429 ## 0.36085613 0.75063821 0.8999231 0.9056944 0.7658929 ## 0.36447104 0.04755168 0.8947321 0.8726389 0.7500000 ## 0.38014796 0.81490324 0.8939087 0.8901389 0.7648214 ## 0.42198667 0.94140042 0.8776984 0.8508333 0.7801786 ## 0.42208259 0.20124965 0.9042262 0.8863889 0.7526786 ## 0.43249714 0.41644166 0.9075818 0.8958333 0.7641071 ## 0.43342775 0.24999664 0.9047173 0.8898611 0.7598214 ## 0.49307138 0.20539733 0.8989608 0.8897222 0.7503571 ## 0.50624038 0.06571280 0.8952133 0.8794444 0.7444643 ## 0.51244277 0.52918233 0.9032813 0.9000000 0.7444643 ## 0.54605138 0.44369088 0.9007292 0.8916667 0.7428571 ## 0.56784496 0.37966361 0.8992882 0.8931944 0.7419643 ## 0.58229517 0.15112209 0.8933135 0.8776389 0.7423214 ## 0.69886586 0.18057206 0.8876314 0.8825000 0.7244643 ## 0.70747426 0.02153708 0.8836111 0.8740278 0.7232143 ## 0.70986464 0.02730106 0.8839286 0.8752778 0.7217857 ## 0.71609215 0.98811552 0.8544147 0.7781944 0.7617857 ## 0.74102544 0.88861156 0.8634003 0.8097222 0.7433929 ## 0.77714849 0.94955834 0.8504812 0.7794444 0.7433929 ## 0.89569896 0.81051218 0.8287773 0.7795833 0.7067857 ## 0.93822474 0.10278451 0.8341741 0.8397222 0.6810714 ## ## ROC was used to select the optimal model using the largest value. ## The final values used for the model were gamma = 0.252517 and lambda ## = 0.5974298. There is currently only a ggplot method (instead of a basic plot method). The results of this function with random searching depends on the number and type of tuning parameters. In this case, it produces a scatter plot of the continuous parameters. ggplot(rda_fit) + theme(legend.position = &quot;top&quot;) "],
["subsampling-for-class-imbalances.html", "11 Subsampling For Class Imbalances 11.1 Subsampling Techniques 11.2 Subsampling During Resampling 11.3 Complications 11.4 Using Custom Subsampling Techniques", " 11 Subsampling For Class Imbalances Contents Subsampling Techniques Subsampling During Resampling Complications Using Custom Subsampling Techniques In classification problems, a disparity in the frequencies of the observed classes can have a significant negative impact on model fitting. One technique for resolving such a class imbalance is to subsample the training data in a manner that mitigates the issues. Examples of sampling methods for this purpose are: down-sampling: randomly subset all the classes in the training set so that their class frequencies match the least prevalent class. For example, suppose that 80% of the training set samples are the first class and the remaining 20% are in the second class. Down-sampling would randomly sample the first class to be the same size as the second class (so that only 40% of the total training set is used to fit the model). caret contains a function (downSample) to do this. up-sampling: randomly sample (with replacement) the minority class to be the same size as the majority class. caret contains a function (upSample) to do this. hybrid methods: techniques such as SMOTE and ROSE down-sample the majority class and synthesize new data points in the minority class. There are two packages (DMwR and ROSE) that implement these procedures. Note that this type of sampling is different from splitting the data into a training and test set. You would never want to artificially balance the test set; its class frequencies should be in-line with what one would see “in the wild”. Also, the above procedures are independent of resampling methods such as cross-validation and the bootstrap. In practice, one could take the training set and, before model fitting, sample the data. There are two issues with this approach Firstly, during model tuning the holdout samples generated during resampling are also glanced and may not reflect the class imbalance that future predictions would encounter. This is likely to lead to overly optimistic estimates of performance. Secondly, the subsampling process will probably induce more model uncertainty. Would the model results differ under a different subsample? As above, the resampling statistics are more likely to make the model appear more effective than it actually is. The alternative is to include the subsampling inside of the usual resampling procedure. This is also advocated for pre-process and featur selection steps too. The two disadvantages are that it might increase computational times and that it might also complicate the analysis in other ways (see the section below about the pitfalls). 11.1 Subsampling Techniques To illustrate these methods, let’s simulate some data with a class imbalance using this method. We will simulate a training and test set where each contains 10000 samples and a minority class rate of about 5.9%: library(caret) set.seed(2969) imbal_train &lt;- twoClassSim(10000, intercept = -20, linearVars = 20) imbal_test &lt;- twoClassSim(10000, intercept = -20, linearVars = 20) table(imbal_train$Class) ## ## Class1 Class2 ## 9411 589 Let’s create different versions of the training set prior to model tuning: set.seed(9560) down_train &lt;- downSample(x = imbal_train[, -ncol(imbal_train)], y = imbal_train$Class) table(down_train$Class) ## ## Class1 Class2 ## 589 589 set.seed(9560) up_train &lt;- upSample(x = imbal_train[, -ncol(imbal_train)], y = imbal_train$Class) table(up_train$Class) ## ## Class1 Class2 ## 9411 9411 library(DMwR) set.seed(9560) smote_train &lt;- SMOTE(Class ~ ., data = imbal_train) table(smote_train$Class) ## ## Class1 Class2 ## 2356 1767 library(ROSE) set.seed(9560) rose_train &lt;- ROSE(Class ~ ., data = imbal_train)$data table(rose_train$Class) ## ## Class1 Class2 ## 4939 5061 For these data, we’ll use a bagged classification and estimate the area under the ROC curve using five repeats of 10-fold CV. ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, repeats = 5, classProbs = TRUE, summaryFunction = twoClassSummary) set.seed(5627) orig_fit &lt;- train(Class ~ ., data = imbal_train, method = &quot;treebag&quot;, nbagg = 50, metric = &quot;ROC&quot;, trControl = ctrl) set.seed(5627) down_outside &lt;- train(Class ~ ., data = down_train, method = &quot;treebag&quot;, nbagg = 50, metric = &quot;ROC&quot;, trControl = ctrl) set.seed(5627) up_outside &lt;- train(Class ~ ., data = up_train, method = &quot;treebag&quot;, nbagg = 50, metric = &quot;ROC&quot;, trControl = ctrl) set.seed(5627) rose_outside &lt;- train(Class ~ ., data = rose_train, method = &quot;treebag&quot;, nbagg = 50, metric = &quot;ROC&quot;, trControl = ctrl) set.seed(5627) smote_outside &lt;- train(Class ~ ., data = smote_train, method = &quot;treebag&quot;, nbagg = 50, metric = &quot;ROC&quot;, trControl = ctrl) We will collate the resampling results and create a wrapper to estimate the test set performance: outside_models &lt;- list(original = orig_fit, down = down_outside, up = up_outside, SMOTE = smote_outside, ROSE = rose_outside) outside_resampling &lt;- resamples(outside_models) test_roc &lt;- function(model, data) { library(pROC) roc_obj &lt;- roc(data$Class, predict(model, data, type = &quot;prob&quot;)[, &quot;Class1&quot;], levels = c(&quot;Class2&quot;, &quot;Class1&quot;)) ci(roc_obj) } outside_test &lt;- lapply(outside_models, test_roc, data = imbal_test) outside_test &lt;- lapply(outside_test, as.vector) outside_test &lt;- do.call(&quot;rbind&quot;, outside_test) colnames(outside_test) &lt;- c(&quot;lower&quot;, &quot;ROC&quot;, &quot;upper&quot;) outside_test &lt;- as.data.frame(outside_test) summary(outside_resampling, metric = &quot;ROC&quot;) ## ## Call: ## summary.resamples(object = outside_resampling, metric = &quot;ROC&quot;) ## ## Models: original, down, up, SMOTE, ROSE ## Number of resamples: 50 ## ## ROC ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## original 0.8935 0.9311 0.9407 0.9410 0.9564 0.9717 0 ## down 0.8937 0.9205 0.9369 0.9347 0.9549 0.9684 0 ## up 0.9995 1.0000 1.0000 0.9999 1.0000 1.0000 0 ## SMOTE 0.9622 0.9762 0.9808 0.9796 0.9840 0.9925 0 ## ROSE 0.8764 0.8908 0.8953 0.8956 0.8999 0.9166 0 outside_test ## lower ROC upper ## original 0.9134895 0.9251843 0.9368791 ## down 0.9238817 0.9315572 0.9392327 ## up 0.9360997 0.9437082 0.9513167 ## SMOTE 0.9390808 0.9457264 0.9523720 ## ROSE 0.9409081 0.9474742 0.9540403 The training and test set estimates for the area under the ROC curve do not appear to correlate. Based on the resampling results, one would infer that up-sampling is nearly perfect and that ROSE does relatively poorly. The reason that up-sampling appears to perform so well is that the samples in the majority class are replicated and have a large potential to be in both the model building and hold-out sets. In essence, the hold-outs here are not truly independent samples. In reality, all of the sampling methods do about the same (based on the test set). The statistics for the basic model fit with no sampling are fairly in-line with one another (0.941 via resampling and 0.925 for the test set). 11.2 Subsampling During Resampling Recent versions of caret allow the user to specify subsampling when using train so that it is conducted inside of resampling. All four methods shown above can be accessed with the basic package using simple syntax. If you want to use your own technique, or want to change some of the parameters for SMOTE or ROSE, the last section below shows how to use custom subsampling. The way to enable subsampling is to use yet another option in trainControl called sampling. The most basic syntax is to use a character string with the name of the sampling method, either &quot;down&quot;, &quot;up&quot;, &quot;smote&quot;, or &quot;rose&quot;. Note that you will need to have the DMwR and ROSE packages installed to use SMOTE and ROSE, respectively. One complication is related to pre-processing. Should the subsampling occur before or after the pre-processing? For example, if you down-sample the data and using PCA for signal extraction, should the loadings be estimated from the entire training set? The estimate is potentially better since the entire training set is being used but the subsample may happen to capture a small potion of the PCA space. There isn’t any obvious answer. The default behavior is to subsample the data prior to pre-processing. This can be easily changed and an example is given below. Now let’s re-run our bagged tree models while sampling inside of cross-validation: ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, repeats = 5, classProbs = TRUE, summaryFunction = twoClassSummary, ## new option here: sampling = &quot;down&quot;) set.seed(5627) down_inside &lt;- train(Class ~ ., data = imbal_train, method = &quot;treebag&quot;, nbagg = 50, metric = &quot;ROC&quot;, trControl = ctrl) ## now just change that option ctrl$sampling &lt;- &quot;up&quot; set.seed(5627) up_inside &lt;- train(Class ~ ., data = imbal_train, method = &quot;treebag&quot;, nbagg = 50, metric = &quot;ROC&quot;, trControl = ctrl) ctrl$sampling &lt;- &quot;rose&quot; set.seed(5627) rose_inside &lt;- train(Class ~ ., data = imbal_train, method = &quot;treebag&quot;, nbagg = 50, metric = &quot;ROC&quot;, trControl = ctrl) ctrl$sampling &lt;- &quot;smote&quot; set.seed(5627) smote_inside &lt;- train(Class ~ ., data = imbal_train, method = &quot;treebag&quot;, nbagg = 50, metric = &quot;ROC&quot;, trControl = ctrl) Here are the resampling and test set results: inside_models &lt;- list(original = orig_fit, down = down_inside, up = up_inside, SMOTE = smote_inside, ROSE = rose_inside) inside_resampling &lt;- resamples(inside_models) inside_test &lt;- lapply(inside_models, test_roc, data = imbal_test) inside_test &lt;- lapply(inside_test, as.vector) inside_test &lt;- do.call(&quot;rbind&quot;, inside_test) colnames(inside_test) &lt;- c(&quot;lower&quot;, &quot;ROC&quot;, &quot;upper&quot;) inside_test &lt;- as.data.frame(inside_test) summary(inside_resampling, metric = &quot;ROC&quot;) ## ## Call: ## summary.resamples(object = inside_resampling, metric = &quot;ROC&quot;) ## ## Models: original, down, up, SMOTE, ROSE ## Number of resamples: 50 ## ## ROC ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## original 0.8935 0.9311 0.9407 0.9410 0.9564 0.9717 0 ## down 0.8902 0.9348 0.9445 0.9421 0.9507 0.9665 0 ## up 0.8928 0.9211 0.9374 0.9352 0.9499 0.9637 0 ## SMOTE 0.9299 0.9458 0.9528 0.9527 0.9609 0.9761 0 ## ROSE 0.9189 0.9440 0.9529 0.9517 0.9597 0.9741 0 inside_test ## lower ROC upper ## original 0.9134895 0.9251843 0.9368791 ## down 0.9234409 0.9308037 0.9381665 ## up 0.9241506 0.9340749 0.9439992 ## SMOTE 0.9455682 0.9512705 0.9569727 ## ROSE 0.9413039 0.9482492 0.9551946 The figure below shows the difference in the area under the ROC curve and the test set results for the approaches shown here. Repeating the subsampling procedures for every resample produces results that are more consistent with the test set. 11.3 Complications The user should be aware that there are a few things that can happening when subsampling that can cause issues in their code. As previously mentioned, when sampling occurs in relation to pre-processing is one such issue. Others are: Sparsely represented categories in factor variables may turn into zero-variance predictors or may be completely sampled out of the model. The underlying functions that do the sampling (e.g. SMOTE, downSample, etc) operate in very different ways and this can affect your results. For example, SMOTE and ROSE will convert your predictor input argument into a data frame (even if you start with a matrix). Currently, sample weights are not supported with sub-sampling. If you use tuneLength to specify the search grid, understand that the data that is used to determine the grid has not been sampled. In most cases, this will not matter but if the grid creation process is affected by the sample size, you may end up using a sub-optimal tuning grid. For some models that require more samples than parameters, a reduction in the sample size may prevent you from being able to fit the model. 11.4 Using Custom Subsampling Techniques Users have the ability to create their own type of subsampling procedure. To do this, alternative syntax is used with the sampling argument of the trainControl. Previously, we used a simple string as the value of this argument. Another way to specify the argument is to use a list with three (named) elements: The name value is a character string used when the train object is printed. It can be any string. The func element is a function that does the subsampling. It should have arguments called x and y that will contain the predictors and outcome data, respectively. The function should return a list with elements of the same name. The first element is a single logical value that indicates whether the subsampling should occur first relative to pre-process. A value of FALSE means that the subsampling function will receive the sampled versions of x and y. For example, here is what the list version of the sampling argument looks like when simple down-sampling is used: down_inside$control$sampling ## $name ## [1] &quot;down&quot; ## ## $func ## function (x, y) ## downSample(x, y, list = TRUE) ## ## $first ## [1] TRUE As another example, suppose we want to use SMOTE but use 10 nearest neighbors instead of the default of 5. To do this, we can create a simple wrapper around the SMOTE function and call this instead: smotest &lt;- list(name = &quot;SMOTE with more neighbors!&quot;, func = function (x, y) { library(DMwR) dat &lt;- if (is.data.frame(x)) x else as.data.frame(x) dat$.y &lt;- y dat &lt;- SMOTE(.y ~ ., data = dat, k = 10) list(x = dat[, !grepl(&quot;.y&quot;, colnames(dat), fixed = TRUE)], y = dat$.y) }, first = TRUE) The control object would then be: ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, repeats = 5, classProbs = TRUE, summaryFunction = twoClassSummary, sampling = smotest) "],
["using-your-own-model-in-train.html", "12 Using Your Own Model in train 12.1 Introduction 12.2 Illustrative Example 1: SVMs with Laplacian Kernels 12.3 Model Components 12.4 The sort Element 12.5 Illustrative Example 2: Something More Complicated - LogitBoost 12.6 Illustrative Example 3: Nonstandard Formulas 12.7 Illustrative Example 4: PLS Feature Extraction Pre-Processing 12.8 Illustrative Example 5: Optimizing probability thresholds for class imbalances 12.9 Illustrative Example 6: Offsets in Generalized Linear Models", " 12 Using Your Own Model in train Contents Introduction Illustrative Example 1: SVMs with Laplacian Kernels Model Components Illustrative Example 2: Something More Complicated LogitBoost Illustrative Example 3: Nonstandard Formulas Illustrative Example 4: PLS Feature Extraction Pre-Processing Illustrative Example 5: Optimizing probability thresholds for class imbalances 12.1 Introduction The package contains a large number of predictive model interfaces. However, you may want to create your own because: you are testing out a novel model or the package doesn’t have a model that you are interested in you would like to run an existing model in the package your own way there are pre-processing or sampling steps not contained in the package or you just don’t like the way the package does things You can still get the benefits of the caret infrastructure by creating your own model. Currently, when you specify the type of model that you are interested in (e.g. type = &quot;lda&quot;), the train function runs another function called getModelInfo to retrieve the specifics of that model from the existing catalog. For example: ldaModelInfo &lt;- getModelInfo(model = &quot;lda&quot;, regex = FALSE)[[1]] ## Model components names(ldaModelInfo) ## [1] &quot;label&quot; &quot;library&quot; &quot;loop&quot; &quot;type&quot; &quot;parameters&quot; ## [6] &quot;grid&quot; &quot;fit&quot; &quot;predict&quot; &quot;prob&quot; &quot;predictors&quot; ## [11] &quot;tags&quot; &quot;levels&quot; &quot;sort&quot; To use your own model, you can pass a list of these components to type. This page will describe those components in detail. 12.2 Illustrative Example 1: SVMs with Laplacian Kernels The package currently contains support vector machine (SVM) models using linear, polynomial and radial basis function kernels. The kernlab package has other functions, including the Laplacian kernel. We will illustrate the model components for this model, which has two parameters: the standard cost parameter for SVMs and one kernel parameter (sigma) 12.3 Model Components You can pass a list of information to the method argument in train. For models that are built-in to the package, you can just pass the method name as before. There are some basic components of the list for custom models. A brief description is below for each then, after setting up and example, each will be described in detail. The list should have the following elements: library is a character vector of package names that will be needed to fit the model or calculate predictions. NULL can also be used. type is a simple character vector with values &quot;Classification&quot;, &quot;Regression&quot; or both. parameters is a data frame with three simple attributes for each tuning parameter (if any): the argument name (e.g. mtry), the type of data in the parameter grid and textual labels for the parameter. grid is a function that is used to create the tuning grid (unless the user gives the exact values of the parameters via tuneGrid) fit is a function that fits the model predict is the function that creates predictions prob is a function that can be used to create class probabilities (if applicable) sort is a function that sorts the parameter from most complex to least loop is an optional function for advanced users for models that can create multiple submodel predictions from the same object. levels is an optional function, primarily for classification models using S4 methods to return the factor levels of the outcome. tags is an optional character vector that has subjects associated with the model, such as Tree-Based Model or Embedded Feature Selection. This string is used by the package to create additional documentation pages on the package website. label is an optional character string that names the model (e.g. “Linear Discriminant Analysis”). predictors is an optional function that returns a character vector that contains the names of the predictors that we used in the prediction equation. varImp is an optional function that calculates variable importance metrics for the model (if any). oob is another optional function that calculates out-of-bag performance estimates from the model object. Most models do not have this capability but some (e.g. random forests, bagged models) do. notes is an optional character vector that can be used to document non-obvious aspects of the model. For example, there are two Bayesian lasso models (blasso and blassoAveraged) and this field is used to describe the differences between the two models. check is an optional function that can be used to check the system/install to make sure that any atypical software requirements are available to the user. The input is pkg, which is the same character string given by the library. This function is run after the checking function to see if the packages specified in library are installed. As an example, the model pythonKnnReg uses certain python libraries and the user should have python and these libraries installed. The model file demonstrates how to check for python libraries prior to running the R model. In the caret package, the subdirectory models has all the code for each model that train interfaces with and these can be used as prototypes for your model. Let’s create a new model for a classification support vector machin using the Laplacian kernel function. We will use the kernlab package’s ksvm function. The kernel has two parameters: the standard cost parameter for SVMs and one kernel parameter (sigma). To start, we’ll create a new list: lpSVM &lt;- list(type = &quot;Classification&quot;, library = &quot;kernlab&quot;, loop = NULL) This model can also be used for regression but we will constrain things here for simplicity. For other SVM models, the type value would be c(&quot;Classification&quot;, &quot;Regression&quot;). The library value checks to see if this package is installed and loads it whenever it is needed (e.g. before modeling or prediction). 12.3.1 The parameters Element We have to create some basic information for the parameters in the form of a data frame. The first column is the name of the parameter. The convention is to use the argument name in the model function (e.g. the ksvm function here). Those values are C and sigma. Each is a number and we can give them labels of &quot;Cost&quot; and &quot;Sigma&quot;, respectively. The parameters element would then be: prm &lt;- data.frame(parameter = c(&quot;C&quot;, &quot;sigma&quot;), class = rep(&quot;numeric&quot;, 2), label = c(&quot;Cost&quot;, &quot;Sigma&quot;)) Now we assign it to the model list: lpSVM$parameters &lt;- prm Values of type can indicate numeric, character or logical data types. 12.3.2 The grid Element This should be a function that takes parameters: x and y (for the predictors and outcome data), len (the number of values per tuning parameter) as well as search. len is the value of tuneLength that is potentially passed in through train. search can be either &quot;grid&quot; or &quot;random&quot;. This can be used to setup a grid for searching or random values for random search. The output should be a data frame of tuning parameter combinations with a column for each parameter. The column names should be the parameter name (e.g. the values of prm$parameter). In our case, let’s vary the cost parameter on the log 2 scale. For the sigma parameter, we can use the kernlab function sigest to pre-estimate the value. Following ksvm we take the average of the low and high estimates. Here is a function we could use: svmGrid &lt;- function(x, y, len = NULL, search = &quot;grid&quot;) { library(kernlab) ## This produces low, middle and high values for sigma ## (i.e. a vector with 3 elements). sigmas &lt;- sigest(as.matrix(x), na.action = na.omit, scaled = TRUE) ## To use grid search: if(search == &quot;grid&quot;) { out &lt;- expand.grid(sigma = mean(as.vector(sigmas[-2])), C = 2 ^((1:len) - 3)) } else { ## For random search, define ranges for the parameters then ## generate random values for them rng &lt;- extendrange(log(sigmas), f = .75) out &lt;- data.frame(sigma = exp(runif(len, min = rng[1], max = rng[2])), C = 2^runif(len, min = -5, max = 8)) } out } Again, the user can pass their own grid via train’s tuneGrid option or they can use this code to create a default grid. We assign this function to the overall model list: lpSVM$grid &lt;- svmGrid 12.3.3 The fit Element Here is where we fit the model. This fit function has several arguments: x, y: the current data used to fit the model wts: optional instance weights (not applicable for this particular model) param: the current tuning parameter values lev: the class levels of the outcome (or NULL in regression) last: a logical for whether the current fit is the final fit weights classProbs: a logical for whether class probabilities should be computed. Here is something we could use for this model: svmFit &lt;- function(x, y, wts, param, lev, last, weights, classProbs, ...) { ksvm(x = as.matrix(x), y = y, kernel = rbfdot, kpar = list(sigma = param$sigma), C = param$C, prob.model = classProbs, ...) } lpSVM$fit &lt;- svmFit A few notes about this: Notice that the package is not loaded in the code. It is loaded prior to this function being called so it won’t hurt if you load it again (but that’s not needed). The ksvm function requires a matrix or predictors. If the original data were a data frame, this would throw and error. The tuning parameters are references in the param data frame. There is always a single row in this data frame. The probability model is fit based on the value of classProbs. This value is determined by the value given in trainControl. The three dots allow the user to pass options in from train to, in this case, the ksvm function. For example, if the use wanted to set the cache size for the function, they could list cache = 80 and this argument will be pass from train to ksvm. Any pre-processing that was requested in the call to train have been done. For example, if preProc = &quot;center&quot; was originally requested, the columns of x seen within this function are mean centered. 12.3.4 The predict Element This is a function that produces a vector or predictions. In our case these are class predictions but they could be numbers for regression models. The arguments are: modelFit: the model produced by the fit code shown above. newdata: the predictor values of the instances being predicted (e.g. out-of-bag samples) preProc submodels: this an optional list of tuning parameters only used with the loop element discussed below. In most cases, it will be NULL. Our function will be very simple: svmPred &lt;- function(modelFit, newdata, preProc = NULL, submodels = NULL) predict(modelFit, newdata) lpSVM$predict &lt;- svmPred The function predict.ksvm will automatically create a factor vector as output. The function could also produce character values. Either way, the innards of train will make them factors and ensure that the same levels as the original data are used. 12.3.5 The prob Element If a regression model is being used or if the classification model does not create class probabilities a value of NULL can be used here instead of a function. Otherwise, the function arguments are the same as the pred function. The output should be a matrix or data frame of class probabilities with a column for each class. The column names should be the class levels. We can use: svmProb &lt;- function(modelFit, newdata, preProc = NULL, submodels = NULL) predict(modelFit, newdata, type=&quot;probabilities&quot;) lpSVM$prob &lt;- svmProb If you look at some of the SVM examples in the models directory, the real functions used by train are much more complicated so that they can deal with model failures, probabilities that do not sum to 1 etc. 12.4 The sort Element This is an optional function that sorts the tuning parameters from the simplest model to the most complex. There are times where this ordering is not obvious. This information is used when the performance values are tied across multiple parameters. We would probably want to choose the least complex model in those cases. Here, we will sort by the cost value. Smaller values of C produce smoother class boundaries than larger values: svmSort &lt;- function(x) x[order(x$C),] lpSVM$sort &lt;- svmSort 12.4.1 The levels Element train ensures that classification models always predict factors with the same levels. To do this at prediction time, the package needs to know the levels from the model object (specifically, the finalModels slot of the train object). For model functions using S3 methods, train automatically attaches a character vector called obsLevels to the object and the package code uses this value. However, this strategy does not work for S4 methods. In these cases, the package will use the code found in the levels slot of the model list. For example, the ksvm function uses S4 methods but, unlike most model functions, has a built–in function called lev that will extract the class levels (if any). In this case, our levels code would be: lpSVM$levels &lt;- function(x) lev(x) In most other cases, the levels will beed to be extracted from data contained in the fitted model object. As another example, objects created using the ctree function in the party package would need to use: function(x) levels(x@data@get(&quot;response&quot;)[,1]) Again, this slot is only used for classification models using S4 methods. We should now be ready to fit our model. library(mlbench) data(Sonar) library(caret) set.seed(998) inTraining &lt;- createDataPartition(Sonar$Class, p = .75, list = FALSE) training &lt;- Sonar[ inTraining,] testing &lt;- Sonar[-inTraining,] fitControl &lt;- trainControl(method = &quot;repeatedcv&quot;, ## 10-fold CV... number = 10, ## repeated ten times repeats = 10) set.seed(825) Laplacian &lt;- train(Class ~ ., data = training, method = lpSVM, preProc = c(&quot;center&quot;, &quot;scale&quot;), tuneLength = 8, trControl = fitControl) Laplacian ## 157 samples ## 60 predictor ## 2 classes: &#39;M&#39;, &#39;R&#39; ## ## Pre-processing: centered (60), scaled (60) ## Resampling: Cross-Validated (10 fold, repeated 10 times) ## Summary of sample sizes: 142, 142, 140, 142, 142, 141, ... ## Resampling results across tuning parameters: ## ## C Accuracy Kappa ## 0.25 0.7479387 0.4793327 ## 0.50 0.7971814 0.5843980 ## 1.00 0.8230613 0.6396747 ## 2.00 0.8260343 0.6456674 ## 4.00 0.8449657 0.6848766 ## 8.00 0.8666005 0.7285194 ## 16.00 0.8671887 0.7296499 ## 32.00 0.8671887 0.7296499 ## ## Tuning parameter &#39;sigma&#39; was held constant at a value of 0.0115025 ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were C = 16 and sigma = 0.0115025. A plot of the data shows that the model doesn’t change when the cost value is above 16. ggplot(Laplacian) + scale_x_log10() 12.5 Illustrative Example 2: Something More Complicated - LogitBoost 12.5.1 The loop Element This function can be used to create custom loops for models to tune over. In most cases, the function can just return the existing tuning grid. For example, a LogitBoost model can be trained over the number of boosting iterations. In the caTools package, the LogitBoost function can be used to fit this model. For example: mod &lt;- LogitBoost(as.matrix(x), y, nIter = 51) If we were to tune the model evaluating models where the number of iterations was 11, 21, 31, 41 and 51, the grid could be lbGrid &lt;- data.frame(nIter = seq(11, 51, by = 10)) During resampling, train could loop over all five rows in lbGrid and fit five models. However, the predict.LogitBoost function has an argument called nIter that can produce, in this case, predictions from mod for all five models. Instead of train fitting five models, we could fit a single model with nIter = class=“hl num”&gt;51and derive predictions for all five models using onlymod`. The terminology used here is that nIter is a sequential tuning parameter (and the other parameters would be considered fixed). The loop argument for models is used to produce two objects: loop: this is the actual loop that is used by train. submodels is a list that has as many elements as there are rows in loop. The list has all the “extra” parameter settings that can be derived for each model. Going back to the LogitBoost example, we could have: loop &lt;- data.frame(.nIter = 51) loop ## .nIter ## 1 51 submodels &lt;- list(data.frame(nIter = seq(11, 41, by = 10))) submodels ## [[1]] ## nIter ## 1 11 ## 2 21 ## 3 31 ## 4 41 For this case, train first fits the nIter = 51 model. When the model is predicted, that code has a for loop that iterates over the elements of submodel[[1]] to get the predictions for the other 4 models. In the end, predictions for all five models (for nIter = seq(11, 51, by = 10)) with a single model fit. There are other models built-in to caret that are used this way. There are a number of models that have multiple sequential tuning parameters. If the loop argument is left NULL the results of tuneGrid are used as the simple loop and is recommended for most situations. Note that the machinery that is used to “derive” the extra predictions is up to the user to create, typically in the predict and prob elements of the custom model object. For the LogitBoost model, some simple code to create these objects would be: fullGrid &lt;- data.frame(nIter = seq(11, 51, by = 10)) ## Get the largest value of nIter to fit the &quot;full&quot; model loop &lt;- fullGrid[which.max(fullGrid$nIter),,drop = FALSE] loop ## nIter ## 5 51 submodels &lt;- fullGrid[-which.max(fullGrid$nIter),,drop = FALSE] ## This needs to be encased in a list in case there are more ## than one tuning parameter submodels &lt;- list(submodels) submodels ## [[1]] ## nIter ## 1 11 ## 2 21 ## 3 31 ## 4 41 For the LogitBoost custom model object, we could use this code in the predict slot: lbPred &lt;- function(modelFit, newdata, preProc = NULL, submodels = NULL) { ## This model was fit with the maximum value of nIter out &lt;- caTools::predict.LogitBoost(modelFit, newdata, type=&quot;class&quot;) ## In this case, &#39;submodels&#39; is a data frame with the other values of ## nIter. We loop over these to get the other predictions. if(!is.null(submodels)) { ## Save _all_ the predictions in a list tmp &lt;- out out &lt;- vector(mode = &quot;list&quot;, length = nrow(submodels) + 1) out[[1]] &lt;- tmp for(j in seq(along = submodels$nIter)) { out[[j+1]] &lt;- caTools::predict.LogitBoost(modelFit, newdata, nIter = submodels$nIter[j]) } } out } A few more notes: The code in the fit element does not have to change. The prob slot works in the same way. The only difference is that the values saved in the outgoing lists are matrices or data frames of probabilities for each class. After model training (i.e. predicting new samples), the value of submodels is set to NULL and the code produces a single set of predictions. If the model had one sequential parameter and one fixed parameter, the loop data frame would have two columns (one for each parameter). If the model is tuned over more than one value of the fixed parameter, the submodels list would have more than one element. If loop had 10 rows, then length(submodels) would be 10 and loop[i,] would be linked to submodels[[i]]. Here is a slimmed down version of the logitBoost code already in the package: lbFuncs &lt;- list(library = &quot;caTools&quot;, loop = function(grid) { loop &lt;- grid[which.max(grid$nIter),,drop = FALSE] submodels &lt;- grid[-which.max(grid$nIter),,drop = FALSE] submodels &lt;- list(submodels) list(loop = loop, submodels = submodels) }, type = &quot;Classification&quot;, parameters = data.frame(parameter = &#39;nIter&#39;, class = &#39;numeric&#39;, label = &#39;# Boosting Iterations&#39;), grid = function(x, y, len = NULL, search = &quot;grid&quot;) { out &lt;- if(search == &quot;grid&quot;) data.frame(nIter = 1 + ((1:len)*10)) else data.frame(nIter = sample(1:500, size = len)) out }, fit = function(x, y, wts, param, lev, last, weights, classProbs, ...) { LogitBoost(as.matrix(x), y, nIter = param$nIter) }, predict = function(modelFit, newdata, preProc = NULL, submodels = NULL) { out &lt;- predict(modelFit, newdata, type=&quot;class&quot;) if(!is.null(submodels)) { tmp &lt;- out out &lt;- vector(mode = &quot;list&quot;, length = nrow(submodels) + 1) out[[1]] &lt;- tmp for(j in seq(along = submodels$nIter)) { out[[j+1]] &lt;- predict(modelFit, newdata, nIter = submodels$nIter[j]) } } out }, prob = NULL, sort = function(x) x) Should you care about this? Let’s tune the model over the same data set used for the SVM model above and see how long it takes: set.seed(825) lb1 &lt;- system.time(train(Class ~ ., data = training, method = lbFuncs, tuneLength = 3, trControl = fitControl)) lb1 ## user system elapsed ## 5.688 0.483 6.216 ## Now get rid of the submodel parts lbFuncs2 &lt;- lbFuncs lbFuncs2$predict &lt;- function(modelFit, newdata, preProc = NULL, submodels = NULL) predict(modelFit, newdata, type=&quot;class&quot;) lbFuncs2$loop &lt;- NULL set.seed(825) lb2 &lt;- system.time(train(Class ~ ., data = training, method = lbFuncs2, tuneLength = 3, trControl = fitControl)) lb2 ## user system elapsed ## 10.157 0.867 11.050 On a data set with 157 instances and 60 predictors and a model that is tuned over only 3 parameter values, there is a 1.78-fold speed-up. If the model were more computationally taxing or the data set were larger or the number of tune parameters that were evaluated was larger, the speed-up would increase. Here is a plot of the speed-up for a few more values of tuneLength: bigGrid &lt;- data.frame(nIter = seq(1, 151, by = 10)) results &lt;- bigGrid results$SpeedUp &lt;- NA for(i in 2:nrow(bigGrid)){ rm(lb1, lb2) set.seed(825) lb1 &lt;- system.time(train(Class ~ ., data = training, method = lbFuncs, tuneGrid = bigGrid[1:i,,drop = FALSE], trControl = fitControl)) set.seed(825) lb2 &lt;- system.time(train(Class ~ ., data = training, method = lbFuncs2, tuneGrid = bigGrid[1:i,,drop = FALSE], trControl = fitControl)) results$SpeedUp[i] &lt;- lb2[3]/lb1[3] } ggplot(results, aes(x = nIter, y = SpeedUp)) + geom_point() + geom_smooth(method = &quot;lm&quot;) + xlab(&quot;LogitBoost Iterations&quot;) + ylab(&quot;Speed-Up&quot;) The speed-ups show a significant decrease in training time using this method. 12.6 Illustrative Example 3: Nonstandard Formulas (Note: the previous third illustration (“SMOTE During Resampling”) is no longer needed due to the inclusion of subsampling via train.) One limitation of train is that it requires the use of basic model formulas. There are several functions that use special formulas or operators on predictors that won’t (and perhaps should not) work in the top level call to train. However, we can still fit these models. Here is an example using the mboost function in the mboost package from the help page. library(mboost) data(&quot;bodyfat&quot;, package = &quot;TH.data&quot;) mod &lt;- mboost(DEXfat ~ btree(age) + bols(waistcirc) + bbs(hipcirc), data = bodyfat) mod ## ## Model-based Boosting ## ## Call: ## mboost(formula = DEXfat ~ btree(age) + bols(waistcirc) + bbs(hipcirc), data = bodyfat) ## ## ## Squared Error (Regression) ## ## Loss function: (y - f)^2 ## ## ## Number of boosting iterations: mstop = 100 ## Step size: 0.1 ## Offset: 30.78282 ## Number of baselearners: 3 We can create a custom model that mimics this code so that we can obtain resampling estimates for this specific model: modelInfo &lt;- list(label = &quot;Model-based Gradient Boosting&quot;, library = &quot;mboost&quot;, type = &quot;Regression&quot;, parameters = data.frame(parameter = &quot;parameter&quot;, class = &quot;character&quot;, label = &quot;parameter&quot;), grid = function(x, y, len = NULL, search = &quot;grid&quot;) data.frame(parameter = &quot;none&quot;), loop = NULL, fit = function(x, y, wts, param, lev, last, classProbs, ...) { ## mboost requires a data frame with predictors and response dat &lt;- if(is.data.frame(x)) x else as.data.frame(x) dat$DEXfat &lt;- y mod &lt;- mboost(DEXfat ~ btree(age) + bols(waistcirc) + bbs(hipcirc), data = dat) }, predict = function(modelFit, newdata, submodels = NULL) { if(!is.data.frame(newdata)) newdata &lt;- as.data.frame(newdata) ## By default a matrix is returned; we convert it to a vector predict(modelFit, newdata)[,1] }, prob = NULL, predictors = function(x, ...) { unique(as.vector(variable.names(x))) }, tags = c(&quot;Ensemble Model&quot;, &quot;Boosting&quot;, &quot;Implicit Feature Selection&quot;), levels = NULL, sort = function(x) x) ## Just use the basic formula method so that these predictors ## are passed &#39;as-is&#39; into the model fitting and prediction ## functions. set.seed(307) mboost_resamp &lt;- train(DEXfat ~ age + waistcirc + hipcirc, data = bodyfat, method = modelInfo, trControl = trainControl(method = &quot;repeatedcv&quot;, repeats = 5)) mboost_resamp ## Model-based Gradient Boosting ## ## 71 samples ## 3 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 5 times) ## Summary of sample sizes: 63, 65, 65, 65, 63, 65, ... ## Resampling results: ## ## RMSE Rsquared ## 4.001465 0.9091613 ## ## 12.7 Illustrative Example 4: PLS Feature Extraction Pre-Processing PCA is a common tool for feature extraction prior to modeling but is unsupervised. Partial Least Squares (PLS) is essentially a supervised version of PCA. For some data sets, there may be some benefit to using PLS to generate new features from the original data (the PLS scores) then use those as an input into a different predictive model. PLS requires parameter tuning. In the example below, we use PLS on a data set with highly correlated predictors then use the PLS scores in a random forest model. The “trick” here is to save the PLS loadings along with the random forest model fit so that the loadings can be used on future samples for prediction. Also, the PLS and random forest models are jointly tuned instead of an initial modeling process that finalizes the PLS model, then builds the random forest model separately. In this was we optimize both at once. Another important point is that the resampling results reflect the variability in the random forest and PLS models. If we did PLS up-front then resampled the random forest model, we would under-estimate the noise in the modeling process. The tecator spectroscopy data are used: data(tecator) set.seed(930) colnames(absorp) &lt;- paste(&quot;x&quot;, 1:ncol(absorp)) ## We will model the protein content data trainMeats &lt;- createDataPartition(endpoints[,3], p = 3/4) absorpTrain &lt;- absorp[trainMeats[[1]], ] proteinTrain &lt;- endpoints[trainMeats[[1]], 3] absorpTest &lt;- absorp[-trainMeats[[1]], ] proteinTest &lt;- endpoints[-trainMeats[[1]], 3] Here is the model code: pls_rf &lt;- list(label = &quot;PLS-RF&quot;, library = c(&quot;pls&quot;, &quot;randomForest&quot;), type = &quot;Regression&quot;, ## Tune over both parameters at the same time parameters = data.frame(parameter = c(&#39;ncomp&#39;, &#39;mtry&#39;), class = c(&quot;numeric&quot;, &#39;numeric&#39;), label = c(&#39;#Components&#39;, &#39;#Randomly Selected Predictors&#39;)), grid = function(x, y, len = NULL, search = &quot;grid&quot;) { if(search == &quot;grid&quot;) { grid &lt;- expand.grid(ncomp = seq(1, min(ncol(x) - 1, len), by = 1), mtry = 1:len) } else { grid &lt;- expand.grid(ncomp = sample(1:ncol(x), size = len), mtry = sample(1:ncol(x), size = len)) } ## We can&#39;t have mtry &gt; ncomp grid &lt;- subset(grid, mtry &lt;= ncomp) }, loop = NULL, fit = function(x, y, wts, param, lev, last, classProbs, ...) { ## First fit the pls model, generate the training set scores, ## then attach what is needed to the random forest object to ## be used later ## plsr only has a formula interface so create one data frame dat &lt;- x dat$y &lt;- y pre &lt;- plsr(y~ ., data = dat, ncomp = param$ncomp) scores &lt;- predict(pre, x, type = &quot;scores&quot;) colnames(scores) &lt;- paste(&quot;score&quot;, 1:param$ncomp, sep = &quot;&quot;) mod &lt;- randomForest(scores, y, mtry = param$mtry, ...) mod$projection &lt;- pre$projection mod }, predict = function(modelFit, newdata, submodels = NULL) { ## Now apply the same scaling to the new samples scores &lt;- as.matrix(newdata) %*% modelFit$projection colnames(scores) &lt;- paste(&quot;score&quot;, 1:ncol(scores), sep = &quot;&quot;) scores &lt;- as.data.frame(scores) ## Predict the random forest model predict(modelFit, scores) }, prob = NULL, varImp = NULL, predictors = function(x, ...) rownames(x$projection), levels = function(x) x$obsLevels, sort = function(x) x[order(x[,1]),]) We fit the models and look at the resampling results for the joint model: meatCtrl &lt;- trainControl(method = &quot;repeatedcv&quot;, repeats = 5) ## These will take a while for these data set.seed(184) plsrf &lt;- train(x = as.data.frame(absorpTrain), y = proteinTrain, method = pls_rf, preProc = c(&quot;center&quot;, &quot;scale&quot;), tuneLength = 10, ntree = 1000, trControl = meatCtrl) ggplot(plsrf, plotType = &quot;level&quot;) ## How does random forest do on its own? set.seed(184) rfOnly &lt;- train(absorpTrain, proteinTrain, method = &quot;rf&quot;, tuneLength = 10, ntree = 1000, trControl = meatCtrl) getTrainPerf(rfOnly) ## TrainRMSE TrainRsquared method ## 1 2.181696 0.4978596 rf ## How does random forest do on its own? set.seed(184) plsOnly &lt;- train(absorpTrain, proteinTrain, method = &quot;pls&quot;, tuneLength = 20, preProc = c(&quot;center&quot;, &quot;scale&quot;), trControl = meatCtrl) getTrainPerf(plsOnly) ## TrainRMSE TrainRsquared method ## 1 0.6290416 0.9596554 pls The test set results indicate that these data like the linear model more than anything: postResample(predict(plsrf, absorpTest), proteinTest) ## RMSE Rsquared ## 1.3758908 0.8095663 postResample(predict(rfOnly, absorpTest), proteinTest) ## RMSE Rsquared ## 2.013188 0.584195 postResample(predict(plsOnly, absorpTest), proteinTest) ## RMSE Rsquared ## 0.7179949 0.9438016 12.8 Illustrative Example 5: Optimizing probability thresholds for class imbalances This description was originally posted on this blog. One of the toughest problems in predictive model occurs when the classes have a severe imbalance. In our book, we spend an entire chapter on this subject itself. One consequence of this is that the performance is generally very biased against the class with the smallest frequencies. For example, if the data have a majority of samples belonging to the first class and very few in the second class, most predictive models will maximize accuracy by predicting everything to be the first class. As a result there’s usually great sensitivity but poor specificity. As a demonstration will use a simulation system described here. By default it has about a 50-50 class frequency but we can change this by altering the function argument called intercept: library(caret) set.seed(442) trainingSet &lt;- twoClassSim(n = 500, intercept = -16) testingSet &lt;- twoClassSim(n = 500, intercept = -16) ## Class frequencies table(trainingSet$Class) ## ## Class1 Class2 ## 450 50 There is almost a 9:1 imbalance in these data. Let’s use a standard random forest model with these data using the default value of mtry. We’ll also use repeated 10-fold cross validation to get a sense of performance: set.seed(949) mod0 &lt;- train(Class ~ ., data = trainingSet, method = &quot;rf&quot;, metric = &quot;ROC&quot;, tuneGrid = data.frame(mtry = 3), ntree = 1000, trControl = trainControl(method = &quot;repeatedcv&quot;, repeats = 5, classProbs = TRUE, summaryFunction = twoClassSummary)) getTrainPerf(mod0) ## TrainROC TrainSens TrainSpec method ## 1 0.9577333 0.9986667 0.312 rf ## Get the ROC curve roc0 &lt;- roc(testingSet$Class, predict(mod0, testingSet, type = &quot;prob&quot;)[,1], levels = rev(levels(testingSet$Class))) roc0 ## ## Call: ## roc.default(response = testingSet$Class, predictor = predict(mod0, testingSet, type = &quot;prob&quot;)[, 1], levels = rev(levels(testingSet$Class))) ## ## Data: predict(mod0, testingSet, type = &quot;prob&quot;)[, 1] in 34 controls (testingSet$Class Class2) &lt; 466 cases (testingSet$Class Class1). ## Area under the curve: 0.9309 ## Now plot plot(roc0, print.thres = c(.5), type = &quot;S&quot;, print.thres.pattern = &quot;%.3f (Spec = %.2f, Sens = %.2f)&quot;, print.thres.cex = .8, legacy.axes = TRUE) ## ## Call: ## roc.default(response = testingSet$Class, predictor = predict(mod0, testingSet, type = &quot;prob&quot;)[, 1], levels = rev(levels(testingSet$Class))) ## ## Data: predict(mod0, testingSet, type = &quot;prob&quot;)[, 1] in 34 controls (testingSet$Class Class2) &lt; 466 cases (testingSet$Class Class1). ## Area under the curve: 0.9309 The area under the ROC curve is very high, indicating that the model has very good predictive power for these data. The plot shows the default probability cut off value of 50%. The sensitivity and specificity values associated with this point indicate that performance is not that good when an actual call needs to be made on a sample. One of the most common ways to deal with this is to determine an alternate probability cut off using the ROC curve. But to do this well, another set of data (not the test set) is needed to set the cut off and the test set is used to validate it. We don’t have a lot of data this is difficult since we will be spending some of our data just to get a single cut off value. Alternatively the model can be tuned, using resampling, to determine any model tuning parameters as well as an appropriate cut off for the probabilities. Suppose the model has one tuning parameter and we want to look at four candidate values for tuning. Suppose we also want to tune the probability cut off over 20 different thresholds. Now we have to look at 20×4=80 different models (and that is for each resample). One other feature that has been opened up his ability to use sequential parameters: these are tuning parameters that don’t require a completely new model fit to produce predictions. In this case, we can fit one random forest model and get it’s predicted class probabilities and evaluate the candidate probability cutoffs using these same hold-out samples. Here is what the model code looks like: ## Get the model code for the original random forest method: thresh_code &lt;- getModelInfo(&quot;rf&quot;, regex = FALSE)[[1]] thresh_code$type &lt;- c(&quot;Classification&quot;) ## Add the threshold as another tuning parameter thresh_code$parameters &lt;- data.frame(parameter = c(&quot;mtry&quot;, &quot;threshold&quot;), class = c(&quot;numeric&quot;, &quot;numeric&quot;), label = c(&quot;#Randomly Selected Predictors&quot;, &quot;Probability Cutoff&quot;)) ## The default tuning grid code: thresh_code$grid &lt;- function(x, y, len = NULL, search = &quot;grid&quot;) { p &lt;- ncol(x) if(search == &quot;grid&quot;) { grid &lt;- expand.grid(mtry = floor(sqrt(p)), threshold = seq(.01, .99, length = len)) } else { grid &lt;- expand.grid(mtry = sample(1:p, size = len), threshold = runif(1, 0, size = len)) } grid } ## Here we fit a single random forest model (with a fixed mtry) ## and loop over the threshold values to get predictions from the same ## randomForest model. thresh_code$loop = function(grid) { library(plyr) loop &lt;- ddply(grid, c(&quot;mtry&quot;), function(x) c(threshold = max(x$threshold))) submodels &lt;- vector(mode = &quot;list&quot;, length = nrow(loop)) for(i in seq(along = loop$threshold)) { index &lt;- which(grid$mtry == loop$mtry[i]) cuts &lt;- grid[index, &quot;threshold&quot;] submodels[[i]] &lt;- data.frame(threshold = cuts[cuts != loop$threshold[i]]) } list(loop = loop, submodels = submodels) } ## Fit the model independent of the threshold parameter thresh_code$fit = function(x, y, wts, param, lev, last, classProbs, ...) { if(length(levels(y)) != 2) stop(&quot;This works only for 2-class problems&quot;) randomForest(x, y, mtry = param$mtry, ...) } ## Now get a probability prediction and use different thresholds to ## get the predicted class thresh_code$predict = function(modelFit, newdata, submodels = NULL) { class1Prob &lt;- predict(modelFit, newdata, type = &quot;prob&quot;)[, modelFit$obsLevels[1]] ## Raise the threshold for class #1 and a higher level of ## evidence is needed to call it class 1 so it should ## decrease sensitivity and increase specificity out &lt;- ifelse(class1Prob &gt;= modelFit$tuneValue$threshold, modelFit$obsLevels[1], modelFit$obsLevels[2]) if(!is.null(submodels)) { tmp2 &lt;- out out &lt;- vector(mode = &quot;list&quot;, length = length(submodels$threshold)) out[[1]] &lt;- tmp2 for(i in seq(along = submodels$threshold)) { out[[i+1]] &lt;- ifelse(class1Prob &gt;= submodels$threshold[[i]], modelFit$obsLevels[1], modelFit$obsLevels[2]) } } out } ## The probabilities are always the same but we have to create ## mulitple versions of the probs to evaluate the data across ## thresholds thresh_code$prob = function(modelFit, newdata, submodels = NULL) { out &lt;- as.data.frame(predict(modelFit, newdata, type = &quot;prob&quot;)) if(!is.null(submodels)) { probs &lt;- out out &lt;- vector(mode = &quot;list&quot;, length = length(submodels$threshold)+1) out &lt;- lapply(out, function(x) probs) } out } Basically, we define a list of model components (such as the fitting code, the prediction code, etc.) and feed this into the train function instead of using a pre-listed model string (such as method = &quot;rf&quot;). For this model and these data, there was an 8% increase in training time to evaluate 20 additional values of the probability cut off. How do we optimize this model? Normally we might look at the area under the ROC curve as a metric to choose our final values. In this case the ROC curve is independent of the probability threshold so we have to use something else. A common technique to evaluate a candidate threshold is see how close it is to the perfect model where sensitivity and specificity are one. Our code will use the distance between the current model’s performance and the best possible performance and then have train minimize this distance when choosing it’s parameters. Here is the code that we use to calculate this: fourStats &lt;- function (data, lev = levels(data$obs), model = NULL) { ## This code will get use the area under the ROC curve and the ## sensitivity and specificity values using the current candidate ## value of the probability threshold. out &lt;- c(twoClassSummary(data, lev = levels(data$obs), model = NULL)) ## The best possible model has sensitivity of 1 and specificity of 1. ## How far are we from that value? coords &lt;- matrix(c(1, 1, out[&quot;Spec&quot;], out[&quot;Sens&quot;]), ncol = 2, byrow = TRUE) colnames(coords) &lt;- c(&quot;Spec&quot;, &quot;Sens&quot;) rownames(coords) &lt;- c(&quot;Best&quot;, &quot;Current&quot;) c(out, Dist = dist(coords)[1]) } set.seed(949) mod1 &lt;- train(Class ~ ., data = trainingSet, method = thresh_code, ## Minimize the distance to the perfect model metric = &quot;Dist&quot;, maximize = FALSE, tuneLength = 20, ntree = 1000, trControl = trainControl(method = &quot;repeatedcv&quot;, repeats = 5, classProbs = TRUE, summaryFunction = fourStats)) mod1 ## Random Forest ## ## 500 samples ## 15 predictor ## 2 classes: &#39;Class1&#39;, &#39;Class2&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 5 times) ## Summary of sample sizes: 450, 450, 450, 450, 450, 450, ... ## Resampling results across tuning parameters: ## ## threshold ROC Sens Spec Dist ## 0.01000000 0.9577333 1.0000000 0.000 1.0000000 ## 0.06157895 0.9577333 1.0000000 0.000 1.0000000 ## 0.11315789 0.9577333 1.0000000 0.000 1.0000000 ## 0.16473684 0.9577333 1.0000000 0.000 1.0000000 ## 0.21631579 0.9577333 1.0000000 0.000 1.0000000 ## 0.26789474 0.9577333 1.0000000 0.000 1.0000000 ## 0.31947368 0.9577333 1.0000000 0.024 0.9760000 ## 0.37105263 0.9577333 1.0000000 0.072 0.9280000 ## 0.42263158 0.9577333 1.0000000 0.128 0.8720000 ## 0.47421053 0.9577333 0.9995556 0.240 0.7600062 ## 0.52578947 0.9577333 0.9964444 0.408 0.5921028 ## 0.57736842 0.9577333 0.9888889 0.560 0.4410562 ## 0.62894737 0.9577333 0.9742222 0.608 0.3968219 ## 0.68052632 0.9577333 0.9631111 0.652 0.3547207 ## 0.73210526 0.9577333 0.9520000 0.692 0.3186794 ## 0.78368421 0.9577333 0.9320000 0.752 0.2669135 ## 0.83526316 0.9577333 0.8977778 0.824 0.2257126 ## 0.88684211 0.9577333 0.8368889 0.940 0.1963473 ## 0.93842105 0.9577333 0.6751111 0.996 0.3264238 ## 0.99000000 0.9577333 0.1777778 1.000 0.8222222 ## ## Tuning parameter &#39;mtry&#39; was held constant at a value of 3 ## Dist was used to select the optimal model using the smallest value. ## The final values used for the model were mtry = 3 and threshold ## = 0.8868421. Using ggplot(mod1) will show the performance profile. Instead here is a plot of the sensitivity, specificity, and distance to the perfect model: library(reshape2) metrics &lt;- mod1$results[, c(2, 4:6)] metrics &lt;- melt(metrics, id.vars = &quot;threshold&quot;, variable.name = &quot;Resampled&quot;, value.name = &quot;Data&quot;) ggplot(metrics, aes(x = threshold, y = Data, color = Resampled)) + geom_line() + ylab(&quot;&quot;) + xlab(&quot;Probability Cutoff&quot;) + theme(legend.position = &quot;top&quot;) You can see that as we increase the probability cut off for the first class it takes more and more evidence for a sample to be predicted as the first class. As a result the sensitivity goes down when the threshold becomes very large. The upside is that we can increase specificity in the same way. The blue curve shows the distance to the perfect model. The value of 0.89 was found to be optimal. Now we can use the test set ROC curve to validate the cut off we chose by resampling. Here the cut off closest to the perfect model is 0.89. We were able to find a good probability cut off value without setting aside another set of data for tuning the cut off. One great thing about this code is that it will automatically apply the optimized probability threshold when predicting new samples. 12.9 Illustrative Example 6: Offsets in Generalized Linear Models Like the mboost example above, a custom method is required since a formula element is used to set the offset variable. Here is an example from ?glm: ## (Intercept) Prewt TreatCont TreatFT ## 49.7711090 -0.5655388 -4.0970655 4.5630627 We can write a small custom method to duplicate this model. Two details of note: If we have factors in the data and do not want train to convert them to dummy variables, the formula method for train should be avoided. We can let glm do that inside the custom method. This would help glm understand that the dummy variable columns came from the same original factor. This will avoid errors in other functions used with glm (e.g. anova). The slot for x should include any variables that are on the right-hand side of the model formula, including the offset column. Here is the custom model: offset_mod &lt;- getModelInfo(&quot;glm&quot;, regex = FALSE)[[1]] offset_mod$fit &lt;- function(x, y, wts, param, lev, last, classProbs, ...) { dat &lt;- if(is.data.frame(x)) x else as.data.frame(x) dat$Postwt &lt;- y glm(Postwt ~ Prewt + Treat + offset(Prewt), family = gaussian, data = dat) } mod &lt;- train(x = anorexia[, 1:2], y = anorexia$Postwt, method = offset_mod) coef(mod$finalModel) ## (Intercept) Prewt TreatCont TreatFT ## 49.7711090 -0.5655388 -4.0970655 4.5630627 "],
["adaptive-resampling.html", "13 Adaptive Resampling", " 13 Adaptive Resampling Models can benefit significantly from tuning but the optimal values are rarely known beforehand. train can be used to define a grid of possible points and resampling can be used to generate good estimates of performance for each tuning parameter combination. However, in the nominal resampling process, all the tuning parameter combinations are computed for all the resamples before a choice is made about which parameters are good and which are poor. caret contains the ability to adaptively resample the tuning parameter grid in a way that concentrates on values that are the in the neighborhood of the optimal settings. See this paper for the details. To illustrate, we will use the Sonar data from one of the previous pages. chemical mutagenicity data from Kazius et al (2005): library(mlbench) data(Sonar) library(caret) set.seed(998) inTraining &lt;- createDataPartition(Sonar$Class, p = .75, list = FALSE) training &lt;- Sonar[ inTraining,] testing &lt;- Sonar[-inTraining,] We will tune a support vector machine model using the same tuning strategy as before but with random search: svmControl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 10, classProbs = TRUE, summaryFunction = twoClassSummary, search = &quot;random&quot;) set.seed(825) svmFit &lt;- train(Class ~ ., data = training, method = &quot;svmRadial&quot;, trControl = svmControl, preProc = c(&quot;center&quot;, &quot;scale&quot;), metric = &quot;ROC&quot;, tuneLength = 15) Using this method, the optimal tuning parameters were a RBF kernel parameter of 0.0205 and a cost value of 44.8174899. To use the adaptive procedure, the trainControl option needs some additional arguments: min is the minimum number of resamples that will be used for each tuning parameter. The default value is 5 and increasing it will decrease the speed-up generated by adaptive resampling but should also increase the likelihood of finding a good model. alpha is a confidence level that is used to remove parameter settings. To date, this value has not shown much of an effect. method is either &quot;gls&quot; for a linear model or &quot;BT&quot; for a Bradley-Terry model. The latter may be more useful when you expect the model to do very well (e.g. an area under the ROC curve near 1) or when there are a large number of tuning parameter settings. complete is a logical value that specifies whether train should generate the full resampling set if it finds an optimal solution before the end of resampling. If you want to know the optimal parameter settings and don’t care much for the estimated performance value, a value of FALSE would be appropriate here. The new code is below. Recall that setting the random number seed just prior to the model fit will ensure the same resamples as well as the same random grid. adaptControl &lt;- trainControl(method = &quot;adaptive_cv&quot;, number = 10, repeats = 10, adaptive = list(min = 5, alpha = 0.05, method = &quot;gls&quot;, complete = TRUE), classProbs = TRUE, summaryFunction = twoClassSummary, search = &quot;random&quot;) set.seed(825) svmAdapt &lt;- train(Class ~ ., data = training, method = &quot;svmRadial&quot;, trControl = adaptControl, preProc = c(&quot;center&quot;, &quot;scale&quot;), metric = &quot;ROC&quot;, tuneLength = 15) The search finalized the tuning parameters on the 19th iteration of resampling and was 5.5-fold faster than the original analysis. Here, the optimal tuning parameters were a RBF kernel parameter of 0.0205 and a cost value of 44.8174899. These match the previous settings even though 1293 fewer models were fit to the data. Remember that this methodology is experimental, so please send any questions or bug reports to the package maintainer. "],
["variable-importance.html", "14 Variable Importance 14.1 Model Specific Metrics 14.2 Model Independent Metrics 14.3 An Example", " 14 Variable Importance Variable importance evaluation functions can be separated into two groups: those that use the model information and those that do not. The advantage of using a model-based approach is that is more closely tied to the model performance and that it may be able to incorporate the correlation structure between the predictors into the importance calculation. Regardless of how the importance is calculated: For most classification models, each predictor will have a separate variable importance for each class (the exceptions are classification trees, bagged trees and boosted trees). All measures of importance are scaled to have a maximum value of 100, unless the scale argument of varImp.train is set to FALSE. 14.1 Model Specific Metrics The following methods for estimating the contribution of each variable to the model are available: Linear Models: the absolute value of the t-statistic for each model parameter is used. Random Forest: from the R package: “For each tree, the prediction accuracy on the out-of-bag portion of the data is recorded. Then the same is done after permuting each predictor variable. The difference between the two accuracies are then averaged over all trees, and normalized by the standard error. For regression, the MSE is computed on the out-of-bag data for each tree, and then the same computed after permuting a variable. The differences are averaged and normalized by the standard error. If the standard error is equal to 0 for a variable, the division is not done.” Partial Least Squares: the variable importance measure here is based on weighted sums of the absolute regression coefficients. The weights are a function of the reduction of the sums of squares across the number of PLS components and are computed separately for each outcome. Therefore, the contribution of the coefficients are weighted proportionally to the reduction in the sums of squares. Recursive Partitioning: The reduction in the loss function (e.g. mean squared error) attributed to each variable at each split is tabulated and the sum is returned. Also, since there may be candidate variables that are important but are not used in a split, the top competing variables are also tabulated at each split. This can be turned off using the maxcompete argument in rpart.control. This method does not currently provide class-specific measures of importance when the response is a factor. Bagged Trees: The same methodology as a single tree is applied to all bootstrapped trees and the total importance is returned Boosted Trees: This method uses the same approach as a single tree, but sums the importances over each boosting iteration (see the gbm package vignette). Multivariate Adaptive Regression Splines: MARS models include a backwards elimination feature selection routine that looks at reductions in the generalized cross-validation (GCV) estimate of error. The varImp function tracks the changes in model statistics, such as the GCV, for each predictor and accumulates the reduction in the statistic when each predictor’s feature is added to the model. This total reduction is used as the variable importance measure. If a predictor was never used in any MARS basis function, it has an importance value of zero. There are three statistics that can be used to estimate variable importance in MARS models. Using varImp(object, value = &quot;gcv&quot;) tracks the reduction in the generalized cross-validation statistic as terms are added. However, there are some cases when terms are retained in the model that result in an increase in GCV. Negative variable importance values for MARS are set to zero. Terms with non-zero importance that were not included in the final, pruned model are also listed as zero. Alternatively, using varImp(object, value = &quot;rss&quot;) monitors the change in the residual sums of squares (RSS) as terms are added, which will never be negative. Also, the option varImp(object, value = &quot;nsubsets&quot;) returns the number of times that each variable is involved in a subset (in the final, pruned model). Prior to June 2008, varImp used an internal function to estimate importance for MARS models. Currently, it is a wrapper around the evimp function in the earth package. Nearest shrunken centroids: The difference between the class centroids and the overall centroid is used to measure the variable influence (see pamr.predict). The larger the difference between the class centroid and the overall center of the data, the larger the separation between the classes. The training set predictions must be supplied when an object of class pamrtrained is given to varImp. Cubist: The Cubist output contains variable usage statistics. It gives the percentage of times where each variable was used in a condition and/or a linear model. Note that this output will probably be inconsistent with the rules shown in the output from summary.cubist. At each split of the tree, Cubist saves a linear model (after feature selection) that is allowed to have terms for each variable used in the current split or any split above it. Quinlan (1992) discusses a smoothing algorithm where each model prediction is a linear combination of the parent and child model along the tree. As such, the final prediction is a function of all the linear models from the initial node to the terminal node. The percentages shown in the Cubist output reflects all the models involved in prediction (as opposed to the terminal models shown in the output). The variable importance used here is a linear combination of the usage in the rule conditions and the model. 14.2 Model Independent Metrics If there is no model-specific way to estimate importance (or the argument useModel = FALSE is used in varImp) the importance of each predictor is evaluated individually using a “filter” approach. For classification, ROC curve analysis is conducted on each predictor. For two class problems, a series of cutoffs is applied to the predictor data to predict the class. The sensitivity and specificity are computed for each cutoff and the ROC curve is computed. The trapezoidal rule is used to compute the area under the ROC curve. This area is used as the measure of variable importance. For multi-class outcomes, the problem is decomposed into all pair-wise problems and the area under the curve is calculated for each class pair (i.e. class 1 vs. class 2, class 2 vs. class 3 etc.). For a specific class, the maximum area under the curve across the relevant pair-wise AUC’s is used as the variable importance measure. For regression, the relationship between each predictor and the outcome is evaluated. An argument, nonpara, is used to pick the model fitting technique. When nonpara = FALSE, a linear model is fit and the absolute value of the t-value for the slope of the predictor is used. Otherwise, a loess smoother is fit between the outcome and the predictor. The R2 statistic is calculated for this model against the intercept only null model. This number is returned as a relative measure of variable importance. 14.3 An Example On the model training web, several models were fit to the example data. The boosted tree model has a built-in variable importance score but neither the support vector machine or the regularized discriminant analysis model do. gbmImp &lt;- varImp(gbmFit3, scale = FALSE) gbmImp ## gbm variable importance ## ## only 20 most important variables shown (out of 60) ## ## Overall ## V12 19.706 ## V51 11.633 ## V36 11.162 ## V21 9.203 ## V11 8.163 ## V9 7.775 ## V45 7.546 ## V23 6.806 ## V31 6.508 ## V46 6.390 ## V4 6.088 ## V48 5.989 ## V37 4.668 ## V27 3.685 ## V39 3.149 ## V34 3.087 ## V16 2.634 ## V29 2.539 ## V43 2.346 ## V2 2.338 The function automatically scales the importance scores to be between 0 and 100. Using scale = FALSE avoids this normalization step. To get the area under the ROC curve for each predictor, the filterVarImp function can be used. The area under the ROC curve is computed for each class. roc_imp &lt;- filterVarImp(x = training[, -ncol(training)], y = training$Class) head(roc_imp) ## M R ## V1 0.3726354 0.3726354 ## V2 0.4234344 0.4234344 ## V3 0.4000326 0.4000326 ## V4 0.3385519 0.3385519 ## V5 0.3479289 0.3479289 ## V6 0.4076158 0.4076158 Alternatively, for models where no built-in importance score is implemented (or exists), the varImp can still be used to get scores. For SVM classification models, the default behavior is to compute the area under the ROC curve. roc_imp2 &lt;- varImp(svmFit, scale = FALSE) roc_imp2 ## ROC curve variable importance ## ## only 20 most important variables shown (out of 60) ## ## Importance ## V36 0.6885 ## V37 0.6508 ## V35 0.6494 ## V34 0.6032 ## V31 0.5957 ## V33 0.5814 ## V32 0.5675 ## V57 0.5398 ## V38 0.5258 ## V60 0.5148 ## V55 0.5099 ## V40 0.5093 ## V30 0.4997 ## V59 0.4926 ## V41 0.4886 ## V16 0.4862 ## V25 0.4817 ## V17 0.4816 ## V26 0.4787 ## V18 0.4742 For importance scores generated from varImp.train, a plot method can be used to visualize the results. In the plot below, the top option is used to make the image more readable. plot(gbmImp, top = 20) "],
["miscellaneous-model-functions.html", "15 Miscellaneous Model Functions 15.1 Yet Another k-Nearest Neighbor Function 15.2 Partial Least Squares Discriminant Analysis 15.3 Bagged MARS and FDA 15.4 Bagging 15.5 Model Averaged Neural Networks 15.6 Neural Networks with a Principal Component Step 15.7 Independent Component Regression", " 15 Miscellaneous Model Functions Contents Yet Another k-Nearest Neighbor Function Partial Least Squares Discriminant Analysis Bagged MARS and FDA General Purpose Bagging Model Averaged Neural Networks Neural Networks with a Principal Component Step Independent Component Regression 15.1 Yet Another k-Nearest Neighbor Function knn3 is a function for k-nearest neighbor classification. This particular implementation is a modification of the knn C code and returns the vote information for all of the classes ( knn only returns the probability for the winning class). There is a formula interface via knn3(formula, data) ## or by passing the training data directly ## x is a matrix or data frame, y is a factor vector knn3(x, y) There are also print and predict methods. For the Sonar data in the mlbench package, we can fit an 11-nearest neighbor model: library(caret) library(mlbench) data(Sonar) set.seed(808) inTrain &lt;- createDataPartition(Sonar$Class, p = 2/3, list = FALSE) ## Save the predictors and class in different objects sonarTrain &lt;- Sonar[ inTrain, -ncol(Sonar)] sonarTest &lt;- Sonar[-inTrain, -ncol(Sonar)] trainClass &lt;- Sonar[ inTrain, &quot;Class&quot;] testClass &lt;- Sonar[-inTrain, &quot;Class&quot;] centerScale &lt;- preProcess(sonarTrain) centerScale ## Created from 139 samples and 60 variables ## ## Pre-processing: ## - centered (60) ## - ignored (0) ## - scaled (60) training &lt;- predict(centerScale, sonarTrain) testing &lt;- predict(centerScale, sonarTest) knnFit &lt;- knn3(training, trainClass, k = 11) knnFit ## 11-nearest neighbor classification model ## Training set class distribution: ## ## M R ## 74 65 predict(knnFit, head(testing), type = &quot;prob&quot;) ## M R ## [1,] 0.2727273 0.7272727 ## [2,] 0.2727273 0.7272727 ## [3,] 0.1818182 0.8181818 ## [4,] 0.1818182 0.8181818 ## [5,] 0.5454545 0.4545455 ## [6,] 0.0000000 1.0000000 Similarly, caret contains a k-nearest neighbor regression function, knnreg. It returns the average outcome for the neighbor. 15.2 Partial Least Squares Discriminant Analysis The plsda function is a wrapper for the plsr function in the pls package that does not require a formula interface and can take factor outcomes as arguments. The classes are broken down into dummy variables (one for each class). These 0/1 dummy variables are modeled by partial least squares. From this model, there are two approaches to computing the class predictions and probabilities: the softmax technique can be used on a per-sample basis to normalize the scores so that they are more “probability like”&quot; (i.e. they sum to one and are between zero and one). For a vector of model predictions for each class X, the softmax class probabilities are computed as. The predicted class is simply the class with the largest model prediction, or equivalently, the largest class probability. This is the default behavior for plsda. Bayes rule can be applied to the model predictions to form posterior probabilities. Here, the model predictions for the training set are used along with the training set outcomes to create conditional distributions for each class. When new samples are predicted, the raw model predictions are run through these conditional distributions to produce a posterior probability for each class (along with the prior). Bayes rule can be used by specifying probModel = &quot;Bayes&quot;. An additional parameter, prior, can be used to set prior probabilities for the classes. The advantage to using Bayes rule is that the full training set is used to directly compute the class probabilities (unlike the softmax function which only uses the current sample’s scores). This creates more realistic probability estimates but the disadvantage is that a separate Bayesian model must be created for each value of ncomp, which is more time consuming. For the sonar data set, we can fit two PLS models using each technique and predict the class probabilities for the test set. plsFit &lt;- plsda(training, trainClass, ncomp = 20) plsFit ## Partial least squares classification, fitted with the kernel algorithm. ## The softmax function was used to compute class probabilities. plsBayesFit &lt;- plsda(training, trainClass, ncomp = 20, probMethod = &quot;Bayes&quot;) plsBayesFit ## Partial least squares classification, fitted with the kernel algorithm. ## Bayes rule was used to compute class probabilities. predict(plsFit, head(testing), type = &quot;prob&quot;) ## , , 20 comps ## ## M R ## 4 0.6227621 0.3772379 ## 6 0.5240553 0.4759447 ## 12 0.3883679 0.6116321 ## 16 0.1925378 0.8074622 ## 17 0.1800970 0.8199030 ## 19 0.1336795 0.8663205 predict(plsBayesFit, head(testing), type = &quot;prob&quot;) ## , , ncomp20 ## ## M R ## 4 0.950775270 0.04922473 ## 6 0.585468690 0.41453131 ## 12 0.076098620 0.92390138 ## 16 0.002768591 0.99723141 ## 17 0.003715369 0.99628463 ## 19 0.023820018 0.97617998 Similar to plsda, caret also contains a function splsda that allows for classification using sparse PLS. A dummy matrix is created for each class and used with the spls function in the spls package. The same approach to estimating class probabilities is used for plsda and splsda. 15.3 Bagged MARS and FDA Multivariate adaptive regression splines (MARS) models, like classification/regression trees, are unstable predictors (Breiman, 1996). This means that small perturbations in the training data might lead to significantly different models. Bagged trees and random forests are effective ways of improving tree models by exploiting these instabilities. caret contains a function, bagEarth, that fits MARS models via the earth function. There are formula and non-formula interfaces. Also, flexible discriminant analysis is a generalization of linear discriminant analysis that can use non-linear features as inputs. One way of doing this is the use MARS-type features to classify samples. The function bagFDA fits FDA models of a set of bootstrap samples and aggregates the predictions to reduce noise. This function is deprecated in favor of the bag function. 15.4 Bagging The bag function offers a general platform for bagging classification and regression models. Like rfe and sbf, it is open and models are specified by declaring functions for the model fitting and prediction code (and several built-in sets of functions exist in the package). The function bagControl has options to specify the functions (more details below). The function also has a few non-standard features: The argument var can enable random sampling of the predictors at each bagging iteration. This is to de-correlate the bagged models in the same spirit of random forests (although here the sampling is done once for the whole model). The default is to use all the predictors for each model. The bagControl function has a logical argument called downSample that is useful for classification models with severe class imbalance. The bootstrapped data set is reduced so that the sample sizes for the classes with larger frequencies are the same as the sample size for the minority class. If a parallel backend for the foreach package has been loaded and registered, the bagged models can be trained in parallel. The function’s control function requires the following arguments: 15.4.1 The fit Function Inputs: x: a data frame of the training set predictor data. y: the training set outcomes. ... arguments passed from train to this function The output is the object corresponding to the trained model and any other objects required for prediction. A simple example for a linear discriminant analysis model from the MASS package is: function(x, y, ...) { library(MASS) lda(x, y, ...) } 15.4.2 The pred Function This should be a function that produces predictors for new samples. Inputs: object: the object generated by the fit module. x: a matrix or data frame of predictor data. The output is either a number vector (for regression), a factor (or character) vector for classification or a matrix/data frame of class probabilities. For classification, it is probably better to average class probabilities instead of using the votes of the class predictions. Using the lda example again: ## predict.lda returns the class and the class probabilities ## We will average the probabilities, so these are saved function(object, x) predict(object, x)$posterior ## function(object, x) predict(object, x)$posterior 15.4.3 The aggregate Function This should be a function that takes the predictions from the constituent models and converts them to a single prediction per sample. Inputs: x: a list of objects returned by the pred module. type: an optional string that describes the type of output (e.g. “class”, “prob” etc.). The output is either a number vector (for regression), a factor (or character) vector for classification or a matrix/data frame of class probabilities. For the linear discriminant model above, we saved the matrix of class probabilities. To average them and generate a class prediction, we could use: function(x, type = &quot;class&quot;) { ## The class probabilities come in as a list of matrices ## For each class, we can pool them then average over them ## Pre-allocate space for the results pooled &lt;- x[[1]] * NA n &lt;- nrow(pooled) classes &lt;- colnames(pooled) ## For each class probability, take the median across ## all the bagged model predictions for(i in 1:ncol(pooled)) { tmp &lt;- lapply(x, function(y, col) y[,col], col = i) tmp &lt;- do.call(&quot;rbind&quot;, tmp) pooled[,i] &lt;- apply(tmp, 2, median) } ## Re-normalize to make sure they add to 1 pooled &lt;- apply(pooled, 1, function(x) x/sum(x)) if(n != nrow(pooled)) pooled &lt;- t(pooled) if(type == &quot;class&quot;) { out &lt;- factor(classes[apply(pooled, 1, which.max)], levels = classes) } else out &lt;- as.data.frame(pooled) out } For example, to bag a conditional inference tree (from the party package): library(caret) set.seed(998) inTraining &lt;- createDataPartition(Sonar$Class, p = .75, list = FALSE) training &lt;- Sonar[ inTraining,] testing &lt;- Sonar[-inTraining,] set.seed(825) baggedCT &lt;- bag(x = training[, names(training) != &quot;Class&quot;], y = training$Class, B = 50, bagControl = bagControl(fit = ctreeBag$fit, predict = ctreeBag$pred, aggregate = ctreeBag$aggregate)) summary(baggedCT) ## ## Call: ## bag.default(x = training[, names(training) != &quot;Class&quot;], y ## = training$Class, B = 50, bagControl = bagControl(fit = ## ctreeBag$fit, predict = ctreeBag$pred, aggregate = ctreeBag$aggregate)) ## ## Out of bag statistics (B = 50): ## ## Accuracy Kappa ## 0.0% 0.5000 -0.004065 ## 2.5% 0.5357 0.014293 ## 25.0% 0.6667 0.333794 ## 50.0% 0.7027 0.397853 ## 75.0% 0.7531 0.489808 ## 97.5% 0.8009 0.573063 ## 100.0% 0.8226 0.641430 15.5 Model Averaged Neural Networks The avNNet fits multiple neural network models to the same data set and predicts using the average of the predictions coming from each constituent model. The models can be different either due to different random number seeds to initialize the network or by fitting the models on bootstrap samples of the original training set (i.e. bagging the neural network). For classification models, the class probabilities are averaged to produce the final class prediction (as opposed to voting from the individual class predictions. As an example, the model can be fit via train: set.seed(825) avNnetFit &lt;- train(x = training, y = trainClass, method = &quot;avNNet&quot;, repeats = 15, trace = FALSE) 15.6 Neural Networks with a Principal Component Step Neural networks can be affected by severe amounts of multicollinearity in the predictors. The function pcaNNet is a wrapper around the preProcess and nnet functions that will run principal component analysis on the predictors before using them as inputs into a neural network. The function will keep enough components that will capture some pre-defined threshold on the cumulative proportion of variance (see the thresh argument). For new samples, the same transformation is applied to the new predictor values (based on the loadings from the training set). The function is available for both regression and classification. This function is deprecated in favor of the train function using method = &quot;nnet&quot; and preProc = &quot;pca&quot;. 15.7 Independent Component Regression The icr function can be used to fit a model analogous to principal component regression (PCR), but using independent component analysis (ICA). The predictor data are centered and projected to the ICA components. These components are then regressed against the outcome. The user needed to specify the number of components to keep. The model uses the preProcess function to compute the latent variables using the fastICA package. Like PCR, there is no guarantee that there will be a correlation between the new latent variable and the outcomes. "],
["measuring-performance.html", "16 Measuring Performance 16.1 Measures for Regression 16.2 Measures for Predicted Classes 16.3 Measures for Class Probabilities 16.4 Lift Curves 16.5 Calibration Curves", " 16 Measuring Performance Measures for Regression Measures for Predicted Classes Measures for Class Probabilities Lift Curves Calibration Curves 16.1 Measures for Regression The function postResample can be used to estimate the root mean squared error (RMSE) and simple R2 for numeric outcomes. For example: library(mlbench) data(BostonHousing) set.seed(280) bh_index &lt;- createDataPartition(BostonHousing$medv, p = .75, list = FALSE) bh_tr &lt;- BostonHousing[ bh_index, ] bh_te &lt;- BostonHousing[-bh_index, ] set.seed(7279) lm_fit &lt;- train(medv ~ . + rm:lstat, data = bh_tr, method = &quot;lm&quot;) bh_pred &lt;- predict(lm_fit, bh_te) lm_fit ## Linear Regression ## ## 381 samples ## 13 predictor ## ## No pre-processing ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 381, 381, 381, 381, 381, 381, ... ## Resampling results: ## ## RMSE Rsquared ## 4.283482 0.7894951 ## ## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE ## postResample(pred = bh_pred, obs = bh_te$medv) ## RMSE Rsquared ## 4.4412987 0.7781877 16.2 Measures for Predicted Classes Before proceeding, let’s make up some test set data: set.seed(144) true_class &lt;- factor(sample(paste0(&quot;Class&quot;, 1:2), size = 1000, prob = c(.2, .8), replace = TRUE)) true_class &lt;- sort(true_class) class1_probs &lt;- rbeta(sum(true_class == &quot;Class1&quot;), 4, 1) class2_probs &lt;- rbeta(sum(true_class == &quot;Class2&quot;), 1, 2.5) test_set &lt;- data.frame(obs = true_class, Class1 = c(class1_probs, class2_probs)) test_set$Class2 &lt;- 1 - test_set$Class1 test_set$pred &lt;- factor(ifelse(test_set$Class1 &gt;= .5, &quot;Class1&quot;, &quot;Class2&quot;)) We would expect that this model will do well on these data: ggplot(test_set, aes(x = Class1)) + geom_histogram(binwidth = .05) + facet_wrap(~obs) + xlab(&quot;Probability of Class #1&quot;) Generating the predicted classes based on the typical 50% cutoff for the probabilities, we can compute the confusion matrix, which shows a cross-tabulation of the observed and predicted classes. The confusionMatrix function can be used to generate these results: confusionMatrix(data = test_set$pred, reference = test_set$obs) ## Confusion Matrix and Statistics ## ## Reference ## Prediction Class1 Class2 ## Class1 183 141 ## Class2 13 663 ## ## Accuracy : 0.846 ## 95% CI : (0.8221, 0.8678) ## No Information Rate : 0.804 ## P-Value [Acc &gt; NIR] : 0.0003424 ## ## Kappa : 0.6081 ## Mcnemar&#39;s Test P-Value : &lt; 2.2e-16 ## ## Sensitivity : 0.9337 ## Specificity : 0.8246 ## Pos Pred Value : 0.5648 ## Neg Pred Value : 0.9808 ## Prevalence : 0.1960 ## Detection Rate : 0.1830 ## Detection Prevalence : 0.3240 ## Balanced Accuracy : 0.8792 ## ## &#39;Positive&#39; Class : Class1 ## For two classes, this function assumes that the class corresponding to an event is the first class level (but this can be changed using the positive argument. Note that there are a number of statistics shown here. The “no-information rate” is the largest proportion of the observed classes (there were more class 2 data than class 1 in this test set). A hypothesis test is also computed to evaluate whether the overall accuracy rate is greater than the rate of the largest class. Also, the prevalence of the “positive event” is computed from the data (unless passed in as an argument), the detection rate (the rate of true events also predicted to be events) and the detection prevalence (the prevalence of predicted events). If the prevalence of the event is different than those seen in the test set, the prevalence option can be used to adjust this. Suppose a 2x2 table: When there are three or more classes, confusionMatrix will show the confusion matrix and a set of “one-versus-all” results. For example, in a three class problem, the sensitivity of the first class is calculated against all the samples in the second and third classes (and so on). The confusionMatrix matrix frames the errors in terms of sensitivity and specificity. In the case of information retrieval, the precision and recall might be more appropriate. In this case, the option mode can be used to get those statistics: confusionMatrix(data = test_set$pred, reference = test_set$obs, mode = &quot;prec_recall&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction Class1 Class2 ## Class1 183 141 ## Class2 13 663 ## ## Accuracy : 0.846 ## 95% CI : (0.8221, 0.8678) ## No Information Rate : 0.804 ## P-Value [Acc &gt; NIR] : 0.0003424 ## ## Kappa : 0.6081 ## Mcnemar&#39;s Test P-Value : &lt; 2.2e-16 ## ## Precision : 0.5648 ## Recall : 0.9337 ## F1 : 0.7038 ## Prevalence : 0.1960 ## Detection Rate : 0.1830 ## Detection Prevalence : 0.3240 ## Balanced Accuracy : 0.8792 ## ## &#39;Positive&#39; Class : Class1 ## Again, the positive argument can be used to control which factor level is associated with a “found” or “important” document or sample. There are individual functions called sensitivity, specificity, posPredValue, negPredValue, precision, recall, and F_meas. Also, a resampled estimate of the training set can also be obtained using confusionMatrix.train. For each resampling iteration, a confusion matrix is created from the hold-out samples and these values can be aggregated to diagnose issues with the model fit. These values are the percentages that hold-out samples landed in the confusion matrix during resampling. There are several methods for normalizing these values. See ?confusionMatrix.train for details. The default performance function used by train is postResample, which generates the accuracy and Kappa statistics: postResample(pred = test_set$pred, obs = test_set$obs) ## Accuracy Kappa ## 0.8460000 0.6081345 As shown below, another function called twoClassSummary can be used to get the sensitivity and specificity using the default probability cutoff. Another function, multiClassSummary, can do similar calculations when there are three or more classes but both require class probabilities for each class. 16.3 Measures for Class Probabilities For data with two classes, there are specialized functions for measuring model performance. First, the twoClassSummary function computes the area under the ROC curve and the specificity and sensitivity under the 50% cutoff. Note that: this function uses the first class level to define the “event” of interest. To change this, use the lev option to the function there must be columns in the data for each of the class probabilities (named the same as the outcome’s class levels) twoClassSummary(test_set, lev = levels(test_set$obs)) ## ROC Sens Spec ## 0.9560044 0.9336735 0.8246269 A similar function can be used to get the analugous precision-recall values and the area under the precision-recall curve: prSummary(test_set, lev = levels(test_set$obs)) ## AUC Precision Recall F ## 0.8582695 0.5648148 0.9336735 0.7038462 This function requires that the MLmetrics package is installed. For multi-class problems, there are additional functions that can be used to calculate performance. One, mnLogLoss computes the negative of the multinomial log-likelihood (smaller is better) based on the class probabilities. This can be used to optimize tuning parameters but can lead to results that are inconsistent with other measures (e.g. accuracy or the area under the ROC curve), especially when the other measures are near their best possible values. The function has similar arguments to the other functions described above. Here is the two-class data from above: mnLogLoss(test_set, lev = levels(test_set$obs)) ## logLoss ## 0.370626 Additionally, the function multiClassSummary computes a number of relevant metrics: the overall accuracy and Kappa statistics using the predicted classes the negative of the multinomial log loss (if class probabilities are available) averages of the “one versus all” statistics such as sensitivity, specificity, the area under the ROC curve, etc. 16.4 Lift Curves The lift function can be used to evaluate probabilities thresholds that can capture a certain percentage of hits. The function requires a set of sample probability predictions (not from the training set) and the true class labels. For example, we can simulate two-class samples using the twoClassSim function and fit a set of models to the training set: set.seed(2) lift_training &lt;- twoClassSim(1000) lift_testing &lt;- twoClassSim(1000) ctrl &lt;- trainControl(method = &quot;cv&quot;, classProbs = TRUE, summaryFunction = twoClassSummary) set.seed(1045) fda_lift &lt;- train(Class ~ ., data = lift_training, method = &quot;fda&quot;, metric = &quot;ROC&quot;, tuneLength = 20, trControl = ctrl) set.seed(1045) lda_lift &lt;- train(Class ~ ., data = lift_training, method = &quot;lda&quot;, metric = &quot;ROC&quot;, trControl = ctrl) set.seed(1045) c5_lift &lt;- train(Class ~ ., data = lift_training, method = &quot;C5.0&quot;, metric = &quot;ROC&quot;, tuneLength = 10, trControl = ctrl, control = C5.0Control(earlyStopping = FALSE)) ## Generate the test set results lift_results &lt;- data.frame(Class = lift_testing$Class) lift_results$FDA &lt;- predict(fda_lift, lift_testing, type = &quot;prob&quot;)[,&quot;Class1&quot;] lift_results$LDA &lt;- predict(lda_lift, lift_testing, type = &quot;prob&quot;)[,&quot;Class1&quot;] lift_results$C5.0 &lt;- predict(c5_lift, lift_testing, type = &quot;prob&quot;)[,&quot;Class1&quot;] head(lift_results) ## Class FDA LDA C5.0 ## 1 Class1 0.99244077 0.8838205 0.8445830 ## 2 Class1 0.99128497 0.7572450 0.8882418 ## 3 Class1 0.82142101 0.8883830 0.5732098 ## 4 Class2 0.04336463 0.0140480 0.1690251 ## 5 Class1 0.77494981 0.9320695 0.4824400 ## 6 Class2 0.11532541 0.0524154 0.3310495 The lift function does the calculations and the corresponding plot function is used to plot the lift curve (although some call this the gain curve). The value argument creates reference lines: trellis.par.set(caretTheme()) lift_obj &lt;- lift(Class ~ FDA + LDA + C5.0, data = lift_results) plot(lift_obj, values = 60, auto.key = list(columns = 3, lines = TRUE, points = FALSE)) From this we can see that, to find 60 percent of the hits, a little more than 30 percent of the data can be sampled (when ordered by the probability predictions). The LDA model does somewhat worse than the other two models. 16.5 Calibration Curves Calibration curves can be used to characterisze how consistent the predicted class probabilities are with the observed event rates. Other functions in the gbm package, the rms package (and others) can also produce calibrartion curves. The format for the function is very similar to the lift function: trellis.par.set(caretTheme()) cal_obj &lt;- calibration(Class ~ FDA + LDA + C5.0, data = lift_results, cuts = 13) plot(cal_obj, type = &quot;l&quot;, auto.key = list(columns = 3, lines = TRUE, points = FALSE)) There is also a ggplot method that shows the confidence intervals for the proportions inside of the subsets: ggplot(cal_obj) "],
["feature-selection-overview.html", "17 Feature Selection Overview 17.1 Models with Built-In Feature Selection 17.2 Feature Selection Methods 17.3 External Validation", " 17 Feature Selection Overview Contents Models with Built-In Feature Selection Feature Selection Methods External Validation 17.1 Models with Built-In Feature Selection Many models that can be accessed using caret’s train function produce prediction equations that do not necessarily use all the predictors. These models are thought to have built-in feature selection: ada, AdaBag, AdaBoost.M1, adaboost, bagEarth, bagEarthGCV, bagFDA, bagFDAGCV, bartMachine, blasso, BstLm, bstSm, C5.0, C5.0Cost, C5.0Rules, C5.0Tree, cforest, chaid, ctree, ctree2, cubist, deepboost, earth, enet, evtree, extraTrees, fda, gamboost, gbm_h2o, gbm, gcvEarth, glmnet_h2o, glmnet, glmStepAIC, J48, JRip, lars, lars2, lasso, LMT, LogitBoost, M5, M5Rules, nodeHarvest, oblique.tree, OneR, ordinalNet, ORFlog, ORFpls, ORFridge, ORFsvm, pam, parRF, PART, penalized, PenalizedLDA, qrf, ranger, Rborist, relaxo, rf, rFerns, rfRules, rotationForest, rotationForestCp, rpart, rpart1SE, rpart2, rpartCost, rpartScore, rqlasso, rqnc, RRF, RRFglobal, sdwd, smda, sparseLDA, spikeslab, wsrf, xgbLinear, xgbTree. Many of the functions have an ancillary method called predictors that returns a vector indicating which predictors were used in the final model. In many cases, using these models with built-in feature selection will be more efficient than algorithms where the search routine for the right predictors is external to the model. Built-in feature selection typically couples the predictor search algorithm with the parameter estimation and are usually optimized with a single objective function (e.g. error rates or likelihood). 17.2 Feature Selection Methods Apart from models with built-in feature selection, most approaches for reducing the number of predictors can be placed into two main categories. Using the terminology of John, Kohavi, and Pfleger (1994): Wrapper methods evaluate multiple models using procedures that add and/or remove predictors to find the optimal combination that maximizes model performance. In essence, wrapper methods are search algorithms that treat the predictors as the inputs and utilize model performance as the output to be optimized. caret has wrapper methods based on recursive feature elimination, genetic algorithms, and simulated annealing. Filter methods evaluate the relevance of the predictors outside of the predictive models and subsequently model only the predictors that pass some criterion. For example, for classification problems, each predictor could be individually evaluated to check if there is a plausible relationship between it and the observed classes. Only predictors with important relationships would then be included in a classification model. Saeys, Inza, and Larranaga (2007) surveys filter methods. caret has a general framework for using univariate filters. Both approaches have advantages and drawbacks. Filter methods are usually more computationally efficient than wrapper methods, but the selection criterion is not directly related to the effectiveness of the model. Also, most filter methods evaluate each predictor separately and, consequently, redundant (i.e. highly-correlated) predictors may be selected and important interactions between variables will not be able to be quantified. The downside of the wrapper method is that many models are evaluated (which may also require parameter tuning) and thus an increase in computation time. There is also an increased risk of over-fitting with wrappers. 17.3 External Validation It is important to realize that feature selection is part of the model building process and, as such, should be externally validated. Just as parameter tuning can result in over-fitting, feature selection can over-fit to the predictors (especially when search wrappers are used). In each of the caret functions for feature selection, the selection process is included in any resampling loops. See See Ambroise and McLachlan (2002) for a demonstration of this issue. ## ## This data.table install has not detected OpenMP support. It will work but slower in single threaded mode. "],
["recursive-feature-elimination.html", "18 Recursive Feature Elimination 18.1 Backwards Selection 18.2 Resampling and External Validation 18.3 Recursive Feature Elimination via caret 18.4 An Example 18.5 Helper Functions 18.6 The Example", " 18 Recursive Feature Elimination Contents Feature Selection Using Search Algorithms Resampling and External Validation Recursive Feature Elimination via caret 18.1 Backwards Selection First, the algorithm fits the model to all predictors. Each predictor is ranked using it’s importance to the model. Let S be a sequence of ordered numbers which are candidate values for the number of predictors to retain (S1 &gt; S2, …). At each iteration of feature selection, the Si top ranked predictors are retained, the model is refit and performance is assessed. The value of Si with the best performance is determined and the top Si predictors are used to fit the final model. Algorithm 1 has a more complete definition. The algorithm has an optional step (line 1.9) where the predictor rankings are recomputed on the model on the reduced feature set. Svetnik et al (2004) showed that, for random forest models, there was a decrease in performance when the rankings were re-computed at every step. However, in other cases when the initial rankings are not good (e.g. linear models with highly collinear predictors), re-calculation can slightly improve performance. One potential issue over-fitting to the predictor set such that the wrapper procedure could focus on nuances of the training data that are not found in future samples (i.e. over-fitting to predictors and samples). For example, suppose a very large number of uninformative predictors were collected and one such predictor randomly correlated with the outcome. The RFE algorithm would give a good rank to this variable and the prediction error (on the same data set) would be lowered. It would take a different test/validation to find out that this predictor was uninformative. The was referred to as “selection bias” by Ambroise and McLachlan (2002). In the current RFE algorithm, the training data is being used for at least three purposes: predictor selection, model fitting and performance evaluation. Unless the number of samples is large, especially in relation to the number of variables, one static training set may not be able to fulfill these needs. 18.2 Resampling and External Validation Since feature selection is part of the model building process, resampling methods (e.g. cross-validation, the bootstrap) should factor in the variability caused by feature selection when calculating performance. For example, the RFE procedure in Algorithm 1 can estimate the model performance on line 1.7, which during the selection process. Ambroise and McLachlan (2002) and Svetnik et al (2004) showed that improper use of resampling to measure performance will result in models that perform poorly on new samples. To get performance estimates that incorporate the variation due to feature selection, it is suggested that the steps in Algorithm 1 be encapsulated inside an outer layer of resampling (e.g. 10-fold cross-validation). Algorithm 2 shows a version of the algorithm that uses resampling. While this will provide better estimates of performance, it is more computationally burdensome. For users with access to machines with multiple processors, the first For loop in Algorithm 2 (line 2.1) can be easily parallelized. Another complication to using resampling is that multiple lists of the “best” predictors are generated at each iteration. At first this may seem like a disadvantage, but it does provide a more probabilistic assessment of predictor importance than a ranking based on a single fixed data set. At the end of the algorithm, a consensus ranking can be used to determine the best predictors to retain. 18.3 Recursive Feature Elimination via caret In caret, Algorithm 1 is implemented by the function rfeIter. The resampling-based Algorithm 2 is in the rfe function. Given the potential selection bias issues, this document focuses on rfe. There are several arguments: x, a matrix or data frame of predictor variables y, a vector (numeric or factor) of outcomes sizes, a integer vector for the specific subset sizes that should be tested (which need not to include ncol(x)) rfeControl, a list of options that can be used to specify the model and the methods for prediction, ranking etc. For a specific model, a set of functions must be specified in rfeControl$functions. Sections below has descriptions of these sub-functions. There are a number of pre-defined sets of functions for several models, including: linear regression (in the object lmFuncs), random forests (rfFuncs), naive Bayes (nbFuncs), bagged trees (treebagFuncs) and functions that can be used with caret’s train function (caretFuncs). The latter is useful if the model has tuning parameters that must be determined at each iteration. 18.4 An Example library(caret) library(mlbench) library(Hmisc) library(randomForest) To test the algorithm, the “Friedman 1” benchmark (Friedman, 1991) was used. There are five informative variables generated by the equation In the simulation used here: n &lt;- 100 p &lt;- 40 sigma &lt;- 1 set.seed(1) sim &lt;- mlbench.friedman1(n, sd = sigma) colnames(sim$x) &lt;- c(paste(&quot;real&quot;, 1:5, sep = &quot;&quot;), paste(&quot;bogus&quot;, 1:5, sep = &quot;&quot;)) bogus &lt;- matrix(rnorm(n * p), nrow = n) colnames(bogus) &lt;- paste(&quot;bogus&quot;, 5+(1:ncol(bogus)), sep = &quot;&quot;) x &lt;- cbind(sim$x, bogus) y &lt;- sim$y Of the 50 predictors, there are 45 pure noise variables: 5 are uniform on \\[0, 1\\] and 40 are random univariate standard normals. The predictors are centered and scaled: normalization &lt;- preProcess(x) x &lt;- predict(normalization, x) x &lt;- as.data.frame(x) subsets &lt;- c(1:5, 10, 15, 20, 25) The simulation will fit models with subset sizes of 25, 20, 15, 10, 5, 4, 3, 2, 1. As previously mentioned, to fit linear models, the lmFuncs set of functions can be used. To do this, a control object is created with the rfeControl function. We also specify that repeated 10-fold cross-validation should be used in line 2.1 of Algorithm 2. The number of folds can be changed via the number argument to rfeControl (defaults to 10). The verbose option prevents copious amounts of output from being produced. set.seed(10) ctrl &lt;- rfeControl(functions = lmFuncs, method = &quot;repeatedcv&quot;, repeats = 5, verbose = FALSE) lmProfile &lt;- rfe(x, y, sizes = subsets, rfeControl = ctrl) lmProfile ## ## Recursive feature selection ## ## Outer resampling method: Cross-Validated (10 fold, repeated 5 times) ## ## Resampling performance over subset size: ## ## Variables RMSE Rsquared RMSESD RsquaredSD Selected ## 1 3.932 0.3812 0.7272 0.2116 ## 2 3.566 0.4894 0.5791 0.2029 ## 3 3.153 0.5953 0.6439 0.1905 ## 4 2.904 0.6562 0.7357 0.2107 * ## 5 2.993 0.6438 0.7627 0.2066 ## 10 3.221 0.5972 0.8173 0.2094 ## 15 3.394 0.5612 0.8189 0.2144 ## 20 3.517 0.5418 0.9081 0.2337 ## 25 3.719 0.5042 0.9147 0.2365 ## 50 4.088 0.4502 0.9538 0.2302 ## ## The top 4 variables (out of 4): ## real4, real5, real2, real1 The output shows that the best subset size was estimated to be 4 predictors. This set includes informative variables but did not include them all. The predictors function can be used to get a text string of variable names that were picked in the final model. The lmProfile is a list of class &quot;rfe&quot; that contains an object fit that is the final linear model with the remaining terms. The model can be used to get predictions for future or test samples. predictors(lmProfile) ## [1] &quot;real4&quot; &quot;real5&quot; &quot;real2&quot; &quot;real1&quot; lmProfile$fit ## ## Call: ## lm(formula = y ~ ., data = tmp) ## ## Coefficients: ## (Intercept) real4 real5 real2 real1 ## 14.613 2.857 1.965 1.625 1.359 head(lmProfile$resample) ## Variables RMSE Rsquared Resample ## 4 4 2.940756 0.5542831 Fold01.Rep1 ## 14 4 2.704529 0.6583788 Fold02.Rep1 ## 24 4 4.599872 0.2181731 Fold03.Rep1 ## 34 4 2.601659 0.8478823 Fold04.Rep1 ## 44 4 1.979220 0.8571701 Fold05.Rep1 ## 54 4 2.978896 0.8301785 Fold06.Rep1 There are also several plot methods to visualize the results. plot(lmProfile) produces the performance profile across different subset sizes, as shown in the figure below. trellis.par.set(caretTheme()) plot(lmProfile, type = c(&quot;g&quot;, &quot;o&quot;)) Also the resampling results are stored in the sub-object lmProfile$resample and can be used with several lattice functions. Univariate lattice functions (densityplot, histogram) can be used to plot the resampling distribution while bivariate functions (xyplot, stripplot) can be used to plot the distributions for different subset sizes. In the latter case, the option returnResamp`` = &quot;all&quot; in rfeControl can be used to save all the resampling results. Example images are shown below for the random forest model. 18.5 Helper Functions To use feature elimination for an arbitrary model, a set of functions must be passed to rfe for each of the steps in Algorithm 2. This section defines those functions and uses the existing random forest functions as an illustrative example. caret contains a list called rfFuncs, but this document will use a more simple version that will be better for illustrating the ideas. A set of simplified functions used here and called rfRFE. rfRFE &lt;- list(summary = defaultSummary, fit = function(x, y, first, last, ...){ library(randomForest) randomForest(x, y, importance = first, ...) }, pred = function(object, x) predict(object, x), rank = function(object, x, y) { vimp &lt;- varImp(object) vimp &lt;- vimp[order(vimp$Overall,decreasing = TRUE),,drop = FALSE] vimp$var &lt;- rownames(vimp) vimp }, selectSize = pickSizeBest, selectVar = pickVars) 18.5.1 The summary Function The summary function takes the observed and predicted values and computes one or more performance metrics (see line 2.14). The input is a data frame with columns obs and pred. The output should be a named vector of numeric variables. Note that the metric argument of the rfe function should reference one of the names of the output of summary. The example function is: rfRFE$summary ## function (data, lev = NULL, model = NULL) ## { ## if (is.character(data$obs)) ## data$obs &lt;- factor(data$obs, levels = lev) ## postResample(data[, &quot;pred&quot;], data[, &quot;obs&quot;]) ## } ## &lt;environment: namespace:caret&gt; Two functions in caret that can be used as the summary funciton are defaultSummary and twoClassSummary (for classification problems with two classes). 18.5.2 The fit Function This function builds the model based on the current data set (lines 2.3, 2.9 and 2.17). The arguments for the function must be: x: the current training set of predictor data with the appropriate subset of variables y: the current outcome data (either a numeric or factor vector) first: a single logical value for whether the current predictor set has all possible variables (e.g. line 2.3) last: similar to first, but TRUE when the last model is fit with the final subset size and predictors. (line 2.17) ...: optional arguments to pass to the fit function in the call to rfe The function should return a model object that can be used to generate predictions. For random forest, the fit function is simple: rfRFE$fit ## function(x, y, first, last, ...){ ## library(randomForest) ## randomForest(x, y, importance = first, ...) ## } For feature selection without re-ranking at each iteration, the random forest variable importances only need to be computed on the first iterations when all of the predictors are in the model. This can be accomplished using importance`` = first. 18.5.3 The pred Function This function returns a vector of predictions (numeric or factors) from the current model (lines 2.4 and 2.10). The input arguments must be object: the model generated by the fit function x: the current set of predictor set for the held-back samples For random forests, the function is a simple wrapper for the predict function: rfRFE$pred ## function(object, x) predict(object, x) For classification, it is probably a good idea to ensure that the resulting factor variables of predictions has the same levels as the input data. 18.5.4 The rank Function This function is used to return the predictors in the order of the most important to the least important (lines 2.5 and 2.11). Inputs are: object: the model generated by the fit function x: the current set of predictor set for the training samples y: the current training outcomes The function should return a data frame with a column called var that has the current variable names. The first row should be the most important predictor etc. Other columns can be included in the output and will be returned in the final rfe object. For random forests, the function below uses caret’s varImp function to extract the random forest importances and orders them. For classification, randomForest will produce a column of importances for each class. In this case, the default ranking function orders the predictors by the averages importance across the classes. rfRFE$rank ## function(object, x, y) { ## vimp &lt;- varImp(object) ## vimp &lt;- vimp[order(vimp$Overall,decreasing = TRUE),,drop = FALSE] ## vimp$var &lt;- rownames(vimp) ## vimp ## } 18.5.5 The selectSize Function This function determines the optimal number of predictors based on the resampling output (line 2.15). Inputs for the function are: x: a matrix with columns for the performance metrics and the number of variables, called Variables metric: a character string of the performance measure to optimize (e.g. RMSE, Accuracy) maximize: a single logical for whether the metric should be maximized This function should return an integer corresponding to the optimal subset size. caret comes with two examples functions for this purpose: pickSizeBest and pickSizeTolerance. The former simply selects the subset size that has the best value. The latter takes into account the whole profile and tries to pick a subset size that is small without sacrificing too much performance. For example, suppose we have computed the RMSE over a series of variables sizes: example &lt;- data.frame(RMSE = c(3.215, 2.819, 2.414, 2.144, 2.014, 1.997, 2.025, 1.987, 1.971, 2.055, 1.935, 1.999, 2.047, 2.002, 1.895, 2.018), Variables = 1:16) These are depicted in the figure below. The solid circle identifies the subset size with the absolute smallest RMSE. However, there are many smaller subsets that produce approximately the same performance but with fewer predictors. In this case, we might be able to accept a slightly larger error for less predictors. The pickSizeTolerance determines the absolute best value then the percent difference of the other points to this value. In the case of RMSE, this would be where RMSE{opt} is the absolute best error rate. These “tolerance” values are plotted in the bottom panel. The solid triangle is the smallest subset size that is within 10% of the optimal value. This approach can produce good results for many of the tree based models, such as random forest, where there is a plateau of good performance for larger subset sizes. For trees, this is usually because unimportant variables are infrequently used in splits and do not significantly affect performance. ## Find the row with the absolute smallest RMSE smallest &lt;- pickSizeBest(example, metric = &quot;RMSE&quot;, maximize = FALSE) smallest ## [1] 15 ## Now one that is within 10% of the smallest within10Pct &lt;- pickSizeTolerance(example, metric = &quot;RMSE&quot;, tol = 10, maximize = FALSE) within10Pct ## [1] 5 minRMSE &lt;- min(example$RMSE) example$Tolerance &lt;- (example$RMSE - minRMSE)/minRMSE * 100 ## Plot the profile and the subsets selected using the ## two different criteria par(mfrow = c(2, 1), mar = c(3, 4, 1, 2)) plot(example$Variables[-c(smallest, within10Pct)], example$RMSE[-c(smallest, within10Pct)], ylim = extendrange(example$RMSE), ylab = &quot;RMSE&quot;, xlab = &quot;Variables&quot;) points(example$Variables[smallest], example$RMSE[smallest], pch = 16, cex= 1.3) points(example$Variables[within10Pct], example$RMSE[within10Pct], pch = 17, cex= 1.3) with(example, plot(Variables, Tolerance)) abline(h = 10, lty = 2, col = &quot;darkgrey&quot;) 18.5.6 The selectVar Function After the optimal subset size is determined, this function will be used to calculate the best rankings for each variable across all the resampling iterations (line 2.16). Inputs for the function are: y: a list of variables importance for each resampling iteration and each subset size (generated by the user-defined rank function). In the example, each each of the cross-validation groups the output of the rank function is saved for each of the 10 subset sizes (including the original subset). If the rankings are not recomputed at each iteration, the values will be the same within each cross-validation iteration. size: the integer returned by the selectSize function This function should return a character string of predictor names (of length size) in the order of most important to least important For random forests, only the first importance calculation (line 2.5) is used since these are the rankings on the full set of predictors. These importances are averaged and the top predictors are returned. rfRFE$selectVar ## function (y, size) ## { ## finalImp &lt;- ddply(y[, c(&quot;Overall&quot;, &quot;var&quot;)], .(var), function(x) mean(x$Overall, ## na.rm = TRUE)) ## names(finalImp)[2] &lt;- &quot;Overall&quot; ## finalImp &lt;- finalImp[order(finalImp$Overall, decreasing = TRUE), ## ] ## as.character(finalImp$var[1:size]) ## } ## &lt;environment: namespace:caret&gt; Note that if the predictor rankings are recomputed at each iteration (line 2.11) the user will need to write their own selection function to use the other ranks. 18.6 The Example For random forest, we fit the same series of model sizes as the linear model. The option to save all the resampling results across subset sizes was changed for this model and are used to show the lattice plot function capabilities in the figures below. ctrl$functions &lt;- rfRFE ctrl$returnResamp &lt;- &quot;all&quot; set.seed(10) rfProfile &lt;- rfe(x, y, sizes = subsets, rfeControl = ctrl) rfProfile ## ## Recursive feature selection ## ## Outer resampling method: Cross-Validated (10 fold, repeated 5 times) ## ## Resampling performance over subset size: ## ## Variables RMSE Rsquared RMSESD RsquaredSD Selected ## 1 4.731 0.2068 0.9741 0.1817 ## 2 3.924 0.3801 0.6817 0.2231 ## 3 3.201 0.5981 0.6212 0.1939 ## 4 2.678 0.7713 0.5115 0.1080 * ## 5 2.846 0.7585 0.5935 0.1282 ## 10 3.089 0.7009 0.5807 0.1646 ## 15 3.167 0.6917 0.5622 0.1661 ## 20 3.306 0.6815 0.5393 0.1764 ## 25 3.385 0.6668 0.5551 0.1818 ## 50 3.537 0.6464 0.5635 0.1960 ## ## The top 4 variables (out of 4): ## real4, real5, real2, real1 The resampling profile can be visualized along with plots of the individual resampling results: trellis.par.set(caretTheme()) plot1 &lt;- plot(rfProfile, type = c(&quot;g&quot;, &quot;o&quot;)) plot2 &lt;- plot(rfProfile, type = c(&quot;g&quot;, &quot;o&quot;), metric = &quot;Rsquared&quot;) print(plot1, split=c(1,1,1,2), more=TRUE) print(plot2, split=c(1,2,1,2)) plot1 &lt;- xyplot(rfProfile, type = c(&quot;g&quot;, &quot;p&quot;, &quot;smooth&quot;), ylab = &quot;RMSE CV Estimates&quot;) plot2 &lt;- densityplot(rfProfile, subset = Variables &lt; 5, adjust = 1.25, as.table = TRUE, xlab = &quot;RMSE CV Estimates&quot;, pch = &quot;|&quot;) print(plot1, split=c(1,1,1,2), more=TRUE) print(plot2, split=c(1,2,1,2)) "],
["feature-selection-using-univariate-filters.html", "19 Feature Selection using Univariate Filters 19.1 Univariate Filters 19.2 Basic Syntax 19.3 The Example", " 19 Feature Selection using Univariate Filters Contents Univariate Filters Basic Syntax The Example 19.1 Univariate Filters Another approach to feature selection is to pre-screen the predictors using simple univariate statistical methods then only use those that pass some criterion in the subsequent model steps. Similar to recursive selection, cross-validation of the subsequent models will be biased as the remaining predictors have already been evaluate on the data set. Proper performance estimates via resampling should include the feature selection step. As an example, it has been suggested for classification models, that predictors can be filtered by conducting some sort of k-sample test (where k is the number of classes) to see if the mean of the predictor is different between the classes. Wilcoxon tests, t-tests and ANOVA models are sometimes used. Predictors that have statistically significant differences between the classes are then used for modeling. The caret function sbf (for selection by filter) can be used to cross-validate such feature selection schemes. Similar to rfe, functions can be passed into sbf for the computational components: univariate filtering, model fitting, prediction and performance summaries (details are given below). The function is applied to the entire training set and also to different resampled versions of the data set. From this, generalizable estimates of performance can be computed that properly take into account the feature selection step. Also, the results of the predictor filters can be tracked over resamples to understand the uncertainty in the filtering. 19.2 Basic Syntax Similar to the rfe function, the syntax for sbf is: sbf(predictors, outcome, sbfControl = sbfControl(), ...) ## or sbf(formula, data, sbfControl = sbfControl(), ...) In this case, the details are specificed using the sbfControl function. Here, the argument functions dictates what the different components should do. This argument should have elements called filter, fit, pred and summary. 19.2.1 The score Function This function takes as inputs the predictors and the outcome in objects called x and y, respectively. By default, each predictor in x is passed to the score function individually. In this case, the function should return a single score. Alternatively, all the predictors can be exposed to the function using the multivariate argument to sbfControl. In this case, the output should be a named vector of scores where the names correspond to the column names of x. There are two built-in functions called anovaScores and gamScores. anovaScores treats the outcome as the independent variable and the predictor as the outcome. In this way, the null hypothesis is that the mean predictor values are equal across the different classes. For regression, gamScores fits a smoothing spline in the predictor to the outcome using a generalized additive model and tests to see if there is any functional relationship between the two. In each function the p-value is used as the score. 19.2.2 The filter Function This function takes as inputs the scores coming out of the score function (in an argument called score). The function also has the training set data as inputs (arguments are called x and y). The output should be a named logical vector where the names correspond to the column names of x. Columns with values of TRUE will be used in the subsequent model. 19.2.3 The fit Function The component is very similar to the rfe-specific function described above. For sbf, there are no first or last arguments. The function should have arguments x, y and .... The data within x have been filtered using the filter function described above. The output of the fit function should be a fitted model. With some data sets, no predictors will survive the filter. In these cases, a model with predictors cannot be computed, but the lack of viable predictors should not be ignored in the final results. To account for this issue, caret contains a model function called nullModel that fits a simple model that is independent of any of the predictors. For problems where the outcome is numeric, the function predicts every sample using the simple mean of the training set outcomes. For classification, the model predicts all samples using the most prevalent class in the training data. This function can be used in the fit component function to “error-trap” cases where no predictors are selected. For example, there are several built-in functions for some models. The object rfSBF is a set of functions that may be useful for fitting random forest models with filtering. The fit function here uses nullModel to check for cases with no predictors: rfSBF$fit ## function (x, y, ...) ## { ## if (ncol(x) &gt; 0) { ## loadNamespace(&quot;randomForest&quot;) ## randomForest::randomForest(x, y, ...) ## } ## else nullModel(y = y) ## } ## &lt;environment: namespace:caret&gt; 19.2.4 The summary and pred Functions The summary function is used to calculate model performance on held-out samples. The pred function is used to predict new samples using the current predictor set. The arguments and outputs for these two functions are identical to the previously discussed summary and pred functions in previously described sections. 19.3 The Example Returning to the example from (Friedman, 1991), we can fit another random forest model with the predictors pre-filtered using the generalized additive model approach described previously. filterCtrl &lt;- sbfControl(functions = rfSBF, method = &quot;repeatedcv&quot;, repeats = 5) set.seed(10) rfWithFilter &lt;- sbf(x, y, sbfControl = filterCtrl) rfWithFilter ## ## Selection By Filter ## ## Outer resampling method: Cross-Validated (10 fold, repeated 5 times) ## ## Resampling performance: ## ## RMSE Rsquared RMSESD RsquaredSD ## 3.451 0.5506 0.5943 0.2146 ## ## Using the training set, 6 variables were selected: ## real2, real4, real5, bogus2, bogus17... ## ## During resampling, the top 5 selected variables (out of a possible 12): ## real2 (100%), real4 (100%), real5 (100%), bogus44 (72%), bogus2 (52%) ## ## On average, 5.6 variables were selected (min = 3, max = 8) In this case, the training set indicated that 6 should be used in the random forest model, but the resampling results indicate that there is some variation in this number. Some of the informative predictors are used, but a few others are erroneous retained. Similar to rfe, there are methods for predictors, densityplot, histogram and varImp. "],
["feature-selection-using-genetic-algorithms.html", "20 Feature Selection using Genetic Algorithms 20.1 Genetic Algorithms 20.2 Internal and External Performance Estimates 20.3 Basic Syntax 20.4 Example 20.5 Customizing the Search 20.6 The Example Revisited", " 20 Feature Selection using Genetic Algorithms Contents Genetic Algorithms Internal and External Performance Estimates Basic Syntax Example Customizing the Search The Example Revisited 20.1 Genetic Algorithms Genetic algorithms (GAs) mimic Darwinian forces of natural selection to find optimal values of some function (Mitchell, 1998). An initial set of candidate solutions are created and their corresponding fitness values are calculated (where larger values are better). This set of solutions is referred to as a population and each solution as an individual. The individuals with the best fitness values are combined randomly to produce offsprings which make up the next population. To do so, individual are selected and undergo cross-over (mimicking genetic reproduction) and also are subject to random mutations. This process is repeated again and again and many generations are produced (i.e. iterations of the search procedure) that should create better and better solutions. For feature selection, the individuals are subsets of predictors that are encoded as binary; a feature is either included or not in the subset. The fitness values are some measure of model performance, such as the RMSE or classification accuracy. One issue with using GAs for feature selection is that the optimization process can be very aggressive and their is potential for the GA to overfit to the predictors (much like the previous discussion for RFE). 20.2 Internal and External Performance Estimates The genetic algorithm code in caret conducts the search of the feature space repeatedly within resampling iterations. First, the training data are split be whatever resampling method was specified in the control function. For example, if 10-fold cross-validation is selected, the entire genetic algorithm is conducted 10 separate times. For the first fold, nine tenths of the data are used in the search while the remaining tenth is used to estimate the external performance since these data points were not used in the search. During the genetic algorithm, a measure of fitness is needed to guide the search. This is the internal measure of performance. During the search, the data that are available are the instances selected by the top-level resampling (e.g. the nine tenths mentioned above). A common approach is to conduct another resampling procedure. Another option is to use a holdout set of samples to determine the internal estimate of performance (see the holdout argument of the control function). While this is faster, it is more likely to cause overfitting of the features and should only be used when a large amount of training data are available. Yet another idea is to use a penalized metric (such as the AIC statistic) but this may not exist for some metrics (e.g. the area under the ROC curve). The internal estimates of performance will eventually overfit the subsets to the data. However, since the external estimate is not used by the search, it is able to make better assessments of overfitting. After resampling, this function determines the optimal number of generations for the GA. Finally, the entire data set is used in the last execution of the genetic algorithm search and the final model is built on the predictor subset that is associated with the optimal number of generations determined by resampling (although the update function can be used to manually set the number of generations). 20.3 Basic Syntax The most basic usage of the function is: obj &lt;- gafs(x = predictors, y = outcome, iters = 100) where x: a data frame or matrix of predictor values y: a factor or numeric vector of outcomes iters: the number of generations for the GA This isn’t very specific. All of the action is in the control function. That can be used to specify the model to be fit, how predictions are made and summarized as well as the genetic operations. Suppose that we want to fit a linear regression model. To do this, we can use train as an interface and pass arguments to that function through gafs: ctrl &lt;- gafsControl(functions = caretGA) obj &lt;- gafs(x = predictors, y = outcome, iters = 100, gafsControl = ctrl, ## Now pass options to `train` method = &quot;lm&quot;) Other options, such as preProcess, can be passed in as well. Some important options to gafsControl are: method, number, repeats, index, indexOut, etc: options similar to those for train top control resampling. metric: this is similar to train’s option but, in this case, the value should be a named vector with values for the internal and external metrics. If none are specified, the first value returned by the summary functions (see details below) are used and a warning is issued. A similar two-element vector for the option maximize is also required. See the last example here for an illustration. holdout: this is a number between [0, 1) that can be used to hold out samples for computing the internal fitness value. Note that this is independent of the external resampling step. Suppose 10-fold CV is being used. Within a resampling iteration, holdout can be used to sample an additional proportion of the 90% resampled data to use for estimating fitness. This may not be a good idea unless you have a very large training set and want to avoid an internal resampling procedure to estimate fitness. allowParallel and genParallel: these are logicals to control where parallel processing should be used (if at all). The former will parallelize the external resampling while the latter parallelizes the fitness calculations within a generation. allowParallel will almost always be more advantageous. There are a few built-in sets of functions to use with gafs: caretGA, rfGA, and treebagGA. The first is a simple interface to train. When using this, as shown above, arguments can be passed to train using the ... structure and the resampling estimates of performance can be used as the internal fitness value. The functions provided by rfGA and treebagGA avoid using train and their internal estimates of fitness come from using the out-of-bag estimates generated from the model. The GA implementation in caret uses the underlying code from the GA package (Scrucca, 2013). 20.4 Example Using the example from the previous page where there are five real predictors and 40 noise predictors: library(mlbench) n &lt;- 100 p &lt;- 40 sigma &lt;- 1 set.seed(1) sim &lt;- mlbench.friedman1(n, sd = sigma) colnames(sim$x) &lt;- c(paste(&quot;real&quot;, 1:5, sep = &quot;&quot;), paste(&quot;bogus&quot;, 1:5, sep = &quot;&quot;)) bogus &lt;- matrix(rnorm(n * p), nrow = n) colnames(bogus) &lt;- paste(&quot;bogus&quot;, 5+(1:ncol(bogus)), sep = &quot;&quot;) x &lt;- cbind(sim$x, bogus) y &lt;- sim$y normalization &lt;- preProcess(x) x &lt;- predict(normalization, x) x &lt;- as.data.frame(x) We’ll fit a random forest model and use the out-of-bag RMSE estimate as the internal performance metric and use the same repeated 10-fold cross-validation process used with the search. To do this, we’ll use the built-in rfGA object for this purpose. The default GA operators will be used and conduct 200 generations of the algorithm. ga_ctrl &lt;- gafsControl(functions = rfGA, method = &quot;repeatedcv&quot;, repeats = 5) ## Use the same random number seed as the RFE process ## so that the same CV folds are used for the external ## resampling. set.seed(10) rf_ga &lt;- gafs(x = x, y = y, iters = 200, gafsControl = ga_ctrl) rf_ga ## ## Genetic Algorithm Feature Selection ## ## 100 samples ## 50 predictors ## ## Maximum generations: 200 ## Population per generation: 50 ## Crossover probability: 0.8 ## Mutation probability: 0.1 ## Elitism: 0 ## ## Internal performance values: RMSE, Rsquared ## Subset selection driven to minimize internal RMSE ## ## External performance values: RMSE, Rsquared ## Best iteration chose by minimizing external RMSE ## External resampling method: Cross-Validated (10 fold, repeated 5 times) ## ## During resampling: ## * the top 5 selected variables (out of a possible 50): ## real1 (100%), real2 (100%), real4 (100%), real5 (100%), bogus17 (88%) ## * on average, 8.3 variables were selected (min = 6, max = 12) ## ## In the final search using the entire training set: ## * 12 features selected at iteration 185 including: ## real1, real2, real3, real4, real5 ... ## * external performance at this iteration is ## ## RMSE Rsquared ## 2.8037 0.7564 With 5 repeats of 10-fold cross-validation, the GA was executed 50 times. The average external performance is calculated across resamples and these results are used to determine the optimal number of iterations for the final GA to avoid over-fitting. Across the resamples, an average of 8.3 predictors were selected at the end of each of the algorithms. The plot function is used to monitor the average of the internal out-of-bag RMSE estimates as well as the average of the external performance estimates calculated from the 50 out-of-sample predictions. By default, this function uses ggplot2 package. A black and white theme can be “added” to the output object: plot(rf_ga) + theme_bw() Based on these results, the generation associated with the best external RMSE estimate was 2.8. Using the entire training set, the final GA is conducted and, at generation 185, there were 12 that were selected: real1, real2, real3, real4, real5, bogus6, bogus7, bogus13, bogus14, bogus17, bogus32, bogus43. The random forest model with these predictors is created using the entire training set is trained and this is the model that is used when predict.gafs is executed. Note: the correlation between the internal and external fitness values is somewhat atypical for most real-world problems. This is a function of the nature of the simulations (a small number of uncorrelated informative predictors) and that the OOB error estimate from random forest is a product of hundreds of trees. Your mileage may vary. 20.5 Customizing the Search 20.5.1 The fit Function This function builds the model based on a proposed current subset. The arguments for the function must be: x: the current training set of predictor data with the appropriate subset of variables y: the current outcome data (either a numeric or factor vector) lev: a character vector with the class levels (or NULL for regression problems) last: a logical that is TRUE when the final GA search is conducted on the entire data set ...: optional arguments to pass to the fit function in the call to gafs The function should return a model object that can be used to generate predictions. For random forest, the fit function is simple: rfGA$fit ## function (x, y, lev = NULL, last = FALSE, ...) ## { ## loadNamespace(&quot;randomForest&quot;) ## randomForest::randomForest(x, y, ...) ## } ## &lt;environment: namespace:caret&gt; 20.5.2 The pred Function This function returns a vector of predictions (numeric or factors) from the current model . The input arguments must be object: the model generated by the fit function x: the current set of predictor set for the held-back samples For random forests, the function is a simple wrapper for the predict function: rfGA$pred ## function (object, x) ## { ## tmp &lt;- predict(object, x) ## if (is.factor(object$y)) { ## out &lt;- cbind(data.frame(pred = tmp), as.data.frame(predict(object, ## x, type = &quot;prob&quot;))) ## } ## else out &lt;- tmp ## out ## } ## &lt;environment: namespace:caret&gt; For classification, it is probably a good idea to ensure that the resulting factor variables of predictions has the same levels as the input data. 20.5.3 The fitness_intern Function The fitness_intern function takes the fitted model and computes one or more performance metrics. The inputs to this function are: object: the model generated by the fit function x: the current set of predictor set. If the option gafsControl$holdout is zero, these values will be from the current resample (i.e. the same data used to fit the model). Otherwise, the predictor values are from the hold-out set created by gafsControl$holdout. y: outcome values. See the note for the x argument to understand which data are presented to the function. maximize: a logical from gafsControl that indicates whether the metric should be maximized or minimized p: the total number of possible predictors The output should be a named numeric vector of performance values. In many cases, some resampled measure of performance is used. In the example above using random forest, the OOB error was used. In other cases, the resampled performance from train can be used and, if gafsControl$holdout is not zero, a static hold-out set can be used. This depends on the data and problem at hand. The example function for random forest is: rfGA$fitness_intern ## function (object, x, y, maximize, p) ## rfStats(object) ## &lt;environment: namespace:caret&gt; 20.5.4 The fitness_extern Function The fitness_extern function takes the observed and predicted values form the external resampling process and computes one or more performance metrics. The input arguments are: data: a data frame or predictions generated by the fit function. For regression, the predicted values in a column called pred. For classification, pred is a factor vector. Class probabilities are usually attached as columns whose names are the class levels (see the random forest example for the fit function above) lev: a character vector with the class levels (or NULL for regression problems) The output should be a named numeric vector of performance values. The example function for random forest is: rfGA$fitness_extern ## function (data, lev = NULL, model = NULL) ## { ## if (is.character(data$obs)) ## data$obs &lt;- factor(data$obs, levels = lev) ## postResample(data[, &quot;pred&quot;], data[, &quot;obs&quot;]) ## } ## &lt;environment: namespace:caret&gt; Two functions in caret that can be used as the summary function are defaultSummary and twoClassSummary (for classification problems with two classes). 20.5.5 The initial Function This function creates an initial generation. Inputs are: vars: the number of possible predictors popSize: the population size for each generation ...: not currently used The output should be a binary 0/1 matrix where there are vars columns corresponding to the predictors and popSize rows for the individuals in the population. The default function populates the rows randomly with subset sizes varying between 10% and 90% of number of possible predictors. For example: set.seed(128) starting &lt;- rfGA$initial(vars = 12, popSize = 8) starting ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] ## [1,] 0 1 0 0 1 0 0 0 0 0 0 0 ## [2,] 0 0 0 0 0 0 1 0 0 0 1 0 ## [3,] 0 0 1 1 1 0 0 0 1 0 1 0 ## [4,] 0 1 0 0 1 0 0 1 0 1 0 1 ## [5,] 0 1 1 1 0 0 1 0 0 1 0 0 ## [6,] 1 1 1 1 1 1 0 1 1 1 1 1 ## [7,] 1 1 1 1 1 0 0 1 0 1 1 1 ## [8,] 1 0 1 1 0 1 1 1 0 1 1 1 apply(starting, 1, mean) ## [1] 0.1666667 0.1666667 0.4166667 0.4166667 0.4166667 0.9166667 0.7500000 ## [8] 0.7500000 gafs has an argument called suggestions that is similar to the one in the ga function where the initial population can be seeded with specific subsets. 20.5.6 The selection Function This function conducts the genetic selection. Inputs are: population: the indicators for the current population fitness: the corresponding fitness values for the population. Note that if the internal performance value is to be minimized, these are the negatives of the actual values r, q: tuning parameters for specific selection functions. See gafs_lrSelection and gafs_nlrSelection ...: not currently used The output should be a list with named elements. population: the indicators for the selected individuals fitness: the fitness values for the selected individuals The default function is a version of the GA package’s ga_lrSelection function. 20.5.7 The crossover Function This function conducts the genetic crossover. Inputs are: population: the indicators for the current population fitness: the corresponding fitness values for the population. Note that if the internal performance value is to be minimized, these are the negatives of the actual values parents: a matrix with two rows containing indicators for the parent individuals. ...: not currently used The default function is a version of the GA package’s ga_spCrossover function. Another function that is a version of that package’s uniform cross-over function is also available. . The output should be a list with named elements. children: from ?ga_spCrossover: “a matrix of dimension 2 times the number of decision variables containing the generated offsprings”&quot; fitness: “a vector of length 2 containing the fitness values for the offsprings. A value NA is returned if an offspring is different (which is usually the case) from the two parents.”&quot; 20.5.8 The mutation Function This function conducts the genetic mutation. Inputs are: population: the indicators for the current population parents: a vector of indices for where the mutation should occur. ...: not currently used The default function is a version of the GA package’s gabin_raMutation function. . The output should the mutated population. 20.5.9 The selectIter Function This function determines the optimal number of generations based on the resampling output. Inputs for the function are: x: a matrix with columns for the performance metrics averaged over resamples metric: a character string of the performance measure to optimize (e.g. RMSE, Accuracy) maximize: a single logical for whether the metric should be maximized This function should return an integer corresponding to the optimal subset size. 20.6 The Example Revisited The previous GA included some of the non-informative predictors. We can cheat a little and try to bias the search to get the right solution. We can try to encourage the algorithm to choose fewer predictors, we can penalize the the RMSE estimate. Normally, a metric like the Akaike information criterion (AIC) statistic would be used. However, with a random forest model, there is no real notion of model degrees of freedom. As an alternative, we can use desirability functions to penalize the RMSE. To do this, two functions are created that translate the number of predictors and the RMSE values to a measure of “desirability”. For the number of predictors, the most desirable property would be a single predictor and the worst situation would be if the model required all 50 predictors. That desirability function is visualized as: For the RMSE, the best case would be zero. Many poor models have values around four. To give the RMSE value more weight in the overall desirability calculation, we use a scale parameter value of 2. This desirability function is: To use the overall desirability to drive the feature selection, the internal function requires replacement. We make a copy of rfGA and add code using the desirability package and the function returns the estimated RMSE and the overall desirability. The gafsControl function also need changes. The metric argument needs to reflect that the overall desirability score should be maximized internally but the RMSE estimate should be minimized externally. library(desirability) rfGA2 &lt;- rfGA rfGA2$fitness_intern &lt;- function (object, x, y, maximize, p) { RMSE &lt;- rfStats(object)[1] d_RMSE &lt;- dMin(0, 4) d_Size &lt;- dMin(1, p, 2) overall &lt;- dOverall(d_RMSE, d_Size) D &lt;- predict(overall, data.frame(RMSE, ncol(x))) c(D = D, RMSE = as.vector(RMSE)) } ga_ctrl_d &lt;- gafsControl(functions = rfGA2, method = &quot;repeatedcv&quot;, repeats = 5, metric = c(internal = &quot;D&quot;, external = &quot;RMSE&quot;), maximize = c(internal = TRUE, external = FALSE)) set.seed(10) rf_ga_d &lt;- gafs(x = x, y = y, iters = 200, gafsControl = ga_ctrl_d) rf_ga_d ## ## Genetic Algorithm Feature Selection ## ## 100 samples ## 50 predictors ## ## Maximum generations: 200 ## Population per generation: 50 ## Crossover probability: 0.8 ## Mutation probability: 0.1 ## Elitism: 0 ## ## Internal performance values: D, RMSE ## Subset selection driven to maximize internal D ## ## External performance values: RMSE, Rsquared ## Best iteration chose by minimizing external RMSE ## External resampling method: Cross-Validated (10 fold, repeated 5 times) ## ## During resampling: ## * the top 5 selected variables (out of a possible 50): ## real1 (100%), real2 (100%), real4 (100%), real5 (100%), real3 (36%) ## * on average, 5.2 variables were selected (min = 4, max = 6) ## ## In the final search using the entire training set: ## * 6 features selected at iteration 177 including: ## real1, real2, real3, real4, real5 ... ## * external performance at this iteration is ## ## RMSE Rsquared ## 2.6891 0.7682 Here are the RMSE values for this search: plot(rf_ga_d) + theme_bw() The final GA found 6 that were selected: real1, real2, real3, real4, real5, bogus26. During resampling, the average number of predictors selected was 5.2, indicating that the penalty on the number of predictors was effective. "],
["feature-selection-using-simulated-annealing.html", "21 Feature Selection using Simulated Annealing 21.1 Simulated Annealing 21.2 Internal and External Performance Estimates 21.3 Basic Syntax 21.4 Example 21.5 Customizing the Search", " 21 Feature Selection using Simulated Annealing Contents Simulated Annealing Internal and External Performance Estimates Basic Syntax Example Customizing the Search 21.1 Simulated Annealing Simulated annealing (SA) is a global search method that makes small random changes (i.e. perturbations) to an initial candidate solution. If the performance value for the perturbed value is better than the previous solution, the new solution is accepted. If not, an acceptance probability is determined based on the difference between the two performance values and the current iteration of the search. From this, a sub-optimal solution can be accepted on the off-change that it may eventually produce a better solution in subsequent iterations. See Kirkpatrick (1984) or Rutenbar (1989) for better descriptions. In the context of feature selection, a solution is a binary vector that describes the current subset. The subset is perturbed by randomly changing a small number of members in the subset. 21.2 Internal and External Performance Estimates Much of the discussion on this subject in the genetic algorithm page is relevant here, although SA search is less aggressive than GA search. In any case, the implementation here conducts the SA search inside the resampling loops and uses an external performance estimate to choose how many iterations of the search are appropriate. 21.3 Basic Syntax The syntax of this function is very similar to the previous information for genetic algorithm searches. The most basic usage of the function is: obj &lt;- safs(x = predictors, y = outcome, iters = 100) where x: a data frame or matrix of predictor values y: a factor or numeric vector of outcomes iters: the number of iterations for the SA This isn’t very specific. All of the action is in the control function. That can be used to specify the model to be fit, how predictions are made and summarized as well as the genetic operations. Suppose that we want to fit a linear regression model. To do this, we can use train as an interface and pass arguments to that function through safs: ctrl &lt;- safsControl(functions = caretSA) obj &lt;- safs(x = predictors, y = outcome, iters = 100, safsControl = ctrl, ## Now pass options to `train` method = &quot;lm&quot;) Other options, such as preProcess, can be passed in as well. Some important options to safsControl are: method, number, repeats, index, indexOut, etc: options similar to those for train top control resampling. metric: this is similar to train’s option but, in this case, the value should be a named vector with values for the internal and external metrics. If none are specified, the first value returned by the summary functions (see details below) are used and a warning is issued. A similar two-element vector for the option maximize is also required. See the last example here for an illustration. holdout: this is a number between [0, 1) that can be used to hold out samples for computing the internal fitness value. Note that this is independent of the external resampling step. Suppose 10-fold CV is being used. Within a resampling iteration, holdout can be used to sample an additional proportion of the 90% resampled data to use for estimating fitness. This may not be a good idea unless you have a very large training set and want to avoid an internal resampling procedure to estimate fitness. improve: an integer (or infinity) defining how many iterations should pass without an improvement in fitness before the current subset is reset to the last known improvement. allowParallel: should the external resampling loop be run in parallel?. There are a few built-in sets of functions to use with safs: caretSA, rfSA, and treebagSA. The first is a simple interface to train. When using this, as shown above, arguments can be passed to train using the ... structure and the resampling estimates of performance can be used as the internal fitness value. The functions provided by rfSA and treebagSA avoid using train and their internal estimates of fitness come from using the out-of-bag estimates generated from the model. 21.4 Example Using the example from the previous page where there are five real predictors and 40 noise predictors. We’ll fit a random forest model and use the out-of-bag RMSE estimate as the internal performance metric and use the same repeated 10-fold cross-validation process used with the search. To do this, we’ll use the built-in rfSA object for this purpose. The default SA operators will be used with 1000 iterations of the algorithm. sa_ctrl &lt;- safsControl(functions = rfSA, method = &quot;repeatedcv&quot;, repeats = 5, improve = 50) set.seed(10) rf_sa &lt;- safs(x = x, y = y, iters = 500, safsControl = sa_ctrl) rf_sa ## ## Simulated Annealing Feature Selection ## ## 100 samples ## 50 predictors ## ## Maximum search iterations: 500 ## Restart after 50 iterations without improvement (6.3 restarts on average) ## ## Internal performance values: RMSE, Rsquared ## Subset selection driven to minimize internal RMSE ## ## External performance values: RMSE, Rsquared ## Best iteration chose by minimizing external RMSE ## External resampling method: Cross-Validated (10 fold, repeated 5 times) ## ## During resampling: ## * the top 5 selected variables (out of a possible 50): ## real1 (100%), real2 (100%), real4 (100%), real5 (100%), bogus17 (88%) ## * on average, 19.2 variables were selected (min = 13, max = 30) ## ## In the final search using the entire training set: ## * 30 features selected at iteration 318 including: ## real1, real2, real4, real5, bogus1 ... ## * external performance at this iteration is ## ## RMSE Rsquared ## 3.2674 0.6785 As with the GA, we can plot the internal and external performance over iterations. plot(rf_sa) + theme_bw() The performance here isn’t as good as the previous GA or RFE solutions. Based on these results, the iteration associated with the best external RMSE estimate was 318 with a corresponding RMSE estimate of 3.27. Using the entire training set, the final SA is conducted and, at iteration 318, there were 30 selected: real1, real2, real4, real5, bogus1, bogus2, bogus6, bogus10, bogus12, bogus13, bogus16, bogus18, bogus19, bogus22, bogus24, bogus25, bogus26, bogus27, bogus29, bogus30, bogus31, bogus32, bogus34, bogus36, bogus37, bogus39, bogus42, bogus43, bogus44, bogus45. The random forest model with these predictors is created using the entire training set is trained and this is the model that is used when predict.safs is executed. 21.5 Customizing the Search 21.5.1 The fit Function This function builds the model based on a proposed current subset. The arguments for the function must be: x: the current training set of predictor data with the appropriate subset of variables y: the current outcome data (either a numeric or factor vector) lev: a character vector with the class levels (or NULL for regression problems) last: a logical that is TRUE when the final SA search is conducted on the entire data set ...: optional arguments to pass to the fit function in the call to safs The function should return a model object that can be used to generate predictions. For random forest, the fit function is simple: rfSA$fit ## function (x, y, lev = NULL, last = FALSE, ...) ## { ## loadNamespace(&quot;randomForest&quot;) ## randomForest::randomForest(x, y, ...) ## } ## &lt;environment: namespace:caret&gt; 21.5.2 The pred Function This function returns a vector of predictions (numeric or factors) from the current model. The input arguments must be object: the model generated by the fit function x: the current set of predictor set for the held-back samples For random forests, the function is a simple wrapper for the predict function: rfSA$pred ## function (object, x) ## { ## tmp &lt;- predict(object, x) ## if (is.factor(object$y)) { ## out &lt;- cbind(data.frame(pred = tmp), as.data.frame(predict(object, ## x, type = &quot;prob&quot;))) ## } ## else out &lt;- tmp ## out ## } ## &lt;environment: namespace:caret&gt; For classification, it is probably a good idea to ensure that the resulting factor variables of predictions has the same levels as the input data. 21.5.3 The fitness_intern Function The fitness_intern function takes the fitted model and computes one or more performance metrics. The inputs to this function are: object: the model generated by the fit function x: the current set of predictor set. If the option safsControl$holdout is zero, these values will be from the current resample (i.e. the same data used to fit the model). Otherwise, the predictor values are from the hold-out set created by safsControl$holdout. y: outcome values. See the note for the x argument to understand which data are presented to the function. maximize: a logical from safsControl that indicates whether the metric should be maximized or minimized p: the total number of possible predictors The output should be a named numeric vector of performance values. In many cases, some resampled measure of performance is used. In the example above using random forest, the OOB error was used. In other cases, the resampled performance from train can be used and, if safsControl$holdout is not zero, a static hold-out set can be used. This depends on the data and problem at hand. If left The example function for random forest is: rfSA$fitness_intern ## function (object, x, y, maximize, p) ## rfStats(object) ## &lt;environment: namespace:caret&gt; 21.5.4 The fitness_extern Function The fitness_extern function takes the observed and predicted values form the external resampling process and computes one or more performance metrics. The input arguments are: data: a data frame or predictions generated by the fit function. For regression, the predicted values in a column called pred. For classification, pred is a factor vector. Class probabilities are usually attached as columns whose names are the class levels (see the random forest example for the fit function above) lev: a character vector with the class levels (or NULL for regression problems) The output should be a named numeric vector of performance values. The example function for random forest is: rfSA$fitness_extern ## function (data, lev = NULL, model = NULL) ## { ## if (is.character(data$obs)) ## data$obs &lt;- factor(data$obs, levels = lev) ## postResample(data[, &quot;pred&quot;], data[, &quot;obs&quot;]) ## } ## &lt;environment: namespace:caret&gt; Two functions in caret that can be used as the summary function are defaultSummary and twoClassSummary (for classification problems with two classes). 21.5.5 The initial Function This function creates an initial subset. Inputs are: vars: the number of possible predictors prob: the probability that a feature is in the subset ...: not currently used The output should be a vector of integers indicating which predictors are in the initial subset. Alternatively, instead of a function, a vector of integers can be used in this slot. 21.5.6 The perturb Function This function perturbs the subset. Inputs are: x: the integers defining the current subset vars: the number of possible predictors number: the number of predictors to randomly change ...: not currently used The output should be a vector of integers indicating which predictors are in the new subset. 21.5.7 The prob Function This function computes the acceptance probability. Inputs are: old: the fitness value for the current subset new: the fitness value for the new subset iteration: the current iteration number or, if the improveargument of safsControl is used, the number of iterations since the last restart ...: not currently used The output should be a numeric value between zero and one. One of the biggest difficulties in using simulated annealing is the specification of the acceptance probability calculation. There are many references on different methods for doing this but the general consensus is that 1) the probability should decrease as the difference between the current and new solution increases and 2) the probability should decrease over iterations. One issue is that the difference in fitness values can be scale-dependent. In this package, the default probability calculations uses the percent difference, i.e. (current - new)/current to normalize the difference. The basic form of the probability simply takes the difference, multiplies by the iteration number and exponentiates this product: prob = exp[(current - new)/current*iteration] To demonstrate this, the plot below shows the probability profile for different fitness values of the current subset and different (absolute) differences. For the example data that were simulated, the RMSE values ranged between values greater than 4 to just under 3. In the plot below, the red curve in the right-hand panel shows how the probability changes over time when comparing a current value of 4 with a new values of 4.5 (smaller values being better). While this difference would likely be accepted in the first few iterations, it is unlikely to be accepted after 30 or 40. Also, larger differences are uniformly disfavored relative to smaller differences. grid &lt;- expand.grid(old = c(4, 3.5), new = c(4.5, 4, 3.5) + 1, iter = 1:40) grid &lt;- subset(grid, old &lt; new) grid$prob &lt;- apply(grid, 1, function(x) safs_prob(new = x[&quot;new&quot;], old= x[&quot;old&quot;], iteration = x[&quot;iter&quot;])) grid$Difference &lt;- factor(grid$new - grid$old) grid$Group &lt;- factor(paste(&quot;Current Value&quot;, grid$old)) ggplot(grid, aes(x = iter, y = prob, color = Difference)) + geom_line() + facet_wrap(~Group) + theme_bw() + ylab(&quot;Probability&quot;) + xlab(&quot;Iteration&quot;) While this is the default, any user-written function can be used to assign probabilities. "],
["data-sets.html", "22 Data Sets 22.1 Blood-Brain Barrier Data 22.2 COX-2 Activity Data 22.3 DHFR Inhibition 22.4 Tecator NIR Data 22.5 Fatty Acid Composition Data 22.6 German Credit Data 22.7 Kelly Blue Book 22.8 Cell Body Segmentation Data 22.9 Sacramento House Price Data 22.10 Animal Scat Data", " 22 Data Sets There are a few data sets included in caret. The first four are computational chemistry problems where the object is to relate the molecular structure of compounds (via molecular descriptors) to some property of interest (Clark and Pickett (2000)). Similar data sets can be found in the QSARdata R pacakge. Other R packages with data are: mlbench, SMCRM and AppliedPredictiveModeling. 22.1 Blood-Brain Barrier Data Mente and Lombardo (2005) developed models to predict the log of the ratio of the concentration of a compound in the brain and the concentration in blood. For each compound, they computed three sets of molecular descriptors: MOE 2D, rule-of-five and Charge Polar Surface Area (CPSA). In all, 134 descriptors were calculated. Included in this package are 208 non-proprietary literature compounds. The vector logBBB contains the log concentration ratio and the data fame bbbDescr contains the descriptor values. 22.2 COX-2 Activity Data From Sutherland, O’Brien, and Weaver (2003): A set of 467 cyclooxygenase-2 (COX-2) inhibitors has been assembled from the published work of a single research group, with in vitro activities against human recombinant enzyme expressed as IC50 values ranging from 1 nM to &gt;100 uM (53 compounds have indeterminate IC50 values). A set of 255 descriptors (MOE2D and QikProp) were generated. To classify the data, we used a cutoff of 2^{2.5} to determine activity. Using data(cox2) exposes three R objects: cox2Descr is a data frame with the descriptor data, cox2IC50 is a numeric vector of IC50 assay values and cox2Class is a factor vector with the activity results. 22.3 DHFR Inhibition Sutherland and Weaver (2004) discuss QSAR models for dihydrofolate reductase (DHFR) inhibition. This data set contains values for 325 compounds. For each compound, 228 molecular descriptors have been calculated. Additionally, each samples is designated as “active” or “inactive”. The data frame dhfr contains a column called Y with the outcome classification. The remainder of the columns are molecular descriptor values. 22.4 Tecator NIR Data These data can be found in the datasets section of StatLib. The data consist of 100 near infrared absorbance spectra used to predict the moisture, fat and protein values of chopped meat. From StatLib: These data are recorded on a Tecator Infratec Food and Feed Analyzer working in the wavelength range 850 - 1050 nm by the Near Infrared Transmission (NIT) principle. Each sample contains finely chopped pure meat with different moisture, fat and protein contents. If results from these data are used in a publication we want you to mention the instrument and company name (Tecator) in the publication. In addition, please send a preprint of your article to: Karin Thente, Tecator AB, Box 70, S-263 21 Hoganas, Sweden. One reference for these data is Borggaard and Thodberg (1992). Using data(tecator) loads a 215 x 100 matrix of absorbance spectra and a 215 x 3 matrix of outcomes. 22.5 Fatty Acid Composition Data Brodnjak-Voncina et al. (2005) describe a set of data where seven fatty acid compositions were used to classify commercial oils as either pumpkin (labeled A), sunflower (B), peanut (C), olive (D), soybean (E), rapeseed (F) and corn (G). There were 96 data points contained in their Table 1 with known results. The breakdown of the classes is given in below: data(oil) dim(fattyAcids) ## [1] 96 7 table(oilType) ## oilType ## A B C D E F G ## 37 26 3 7 11 10 2 As a note, the paper states on page 32 that there are 37 unknown samples while the table on pages 33 and 34 shows that there are 34 unknowns. 22.6 German Credit Data Data from Dr. Hans Hofmann of the University of Hamburg and stored at the UC Irvine Machine Learning Repository. These data have two classes for the credit worthiness: good or bad. There are predictors related to attributes, such as: checking account status, duration, credit history, purpose of the loan, amount of the loan, savings accounts or bonds, employment duration, Installment rate in percentage of disposable income, personal information, other debtors/guarantors, residence duration, property, age, other installment plans, housing, number of existing credits, job information, Number of people being liable to provide maintenance for, telephone, and foreign worker status. Many of these predictors are discrete and have been expanded into several 0/1 indicator variables library(caret) data(GermanCredit) ## Show the first 10 columns str(GermanCredit[, 1:10]) ## &#39;data.frame&#39;: 1000 obs. of 10 variables: ## $ status : Factor w/ 4 levels &quot;... &lt; 100 DM&quot;,..: 1 2 4 1 1 4 4 2 4 2 ... ## $ duration : num 6 48 12 42 24 36 24 36 12 30 ... ## $ credit_history : Factor w/ 5 levels &quot;no credits taken/all credits paid back duly&quot;,..: 5 3 5 3 4 3 3 3 3 5 ... ## $ purpose : Factor w/ 10 levels &quot;car (new)&quot;,&quot;car (used)&quot;,..: 5 5 8 4 1 8 4 2 5 1 ... ## $ amount : num 1169 5951 2096 7882 4870 ... ## $ savings : Factor w/ 5 levels &quot;... &lt; 100 DM&quot;,..: 5 1 1 1 1 5 3 1 4 1 ... ## $ employment_duration: Ord.factor w/ 5 levels &quot;unemployed&quot;&lt;&quot;... &lt; 1 year&quot;&lt;..: 5 3 4 4 3 3 5 3 4 1 ... ## $ installment_rate : num 4 2 2 2 3 2 3 2 2 4 ... ## $ personal_status_sex: Factor w/ 5 levels &quot;male : divorced/separated&quot;,..: 3 2 3 3 3 3 3 3 1 4 ... ## $ other_debtors : Factor w/ 3 levels &quot;none&quot;,&quot;co-applicant&quot;,..: 1 1 1 3 1 1 1 1 1 1 ... 22.7 Kelly Blue Book Resale data for 2005 model year GM cars Kuiper (2008) collected data on Kelly Blue Book resale data for 804 GM cars (2005 model year). cars is data frame of the suggested retail price (column Price) and various characteristics of each car (columns Mileage, Cylinder, Doors, Cruise, Sound, Leather, Buick, Cadillac, Chevy, Pontiac, Saab, Saturn, convertible, coupe, hatchback, sedan and wagon) data(cars) str(cars) ## &#39;data.frame&#39;: 804 obs. of 18 variables: ## $ Price : num 22661 21725 29143 30732 33359 ... ## $ Mileage : int 20105 13457 31655 22479 17590 23635 17381 27558 25049 17319 ... ## $ Cylinder : int 6 6 4 4 4 4 4 4 4 4 ... ## $ Doors : int 4 2 2 2 2 2 2 2 2 4 ... ## $ Cruise : int 1 1 1 1 1 1 1 1 1 1 ... ## $ Sound : int 0 1 1 0 1 0 1 0 0 0 ... ## $ Leather : int 0 0 1 0 1 0 1 1 0 1 ... ## $ Buick : int 1 0 0 0 0 0 0 0 0 0 ... ## $ Cadillac : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Chevy : int 0 1 0 0 0 0 0 0 0 0 ... ## $ Pontiac : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Saab : int 0 0 1 1 1 1 1 1 1 1 ... ## $ Saturn : int 0 0 0 0 0 0 0 0 0 0 ... ## $ convertible: int 0 0 1 1 1 1 1 1 1 0 ... ## $ coupe : int 0 1 0 0 0 0 0 0 0 0 ... ## $ hatchback : int 0 0 0 0 0 0 0 0 0 0 ... ## $ sedan : int 1 0 0 0 0 0 0 0 0 1 ... ## $ wagon : int 0 0 0 0 0 0 0 0 0 0 ... 22.8 Cell Body Segmentation Data Hill, LaPan, Li and Haney (2007) develop models to predict which cells in a high content screen were well segmented. The data consists of 119 imaging measurements on 2019. The original analysis used 1009 for training and 1010 as a test set (see the column called Case). The outcome class is contained in a factor variable called Class with levels PS for poorly segmented and WS for well segmented. data(segmentationData) str(segmentationData[,1:10]) ## &#39;data.frame&#39;: 2019 obs. of 10 variables: ## $ Cell : int 207827637 207932307 207932463 207932470 207932455 207827656 207827659 207827661 207932479 207932480 ... ## $ Case : Factor w/ 2 levels &quot;Test&quot;,&quot;Train&quot;: 1 2 2 2 1 1 1 1 1 1 ... ## $ Class : Factor w/ 2 levels &quot;PS&quot;,&quot;WS&quot;: 1 1 2 1 1 2 2 1 2 2 ... ## $ AngleCh1 : num 143.25 133.75 106.65 69.15 2.89 ... ## $ AreaCh1 : int 185 819 431 298 285 172 177 251 495 384 ... ## $ AvgIntenCh1 : num 15.7 31.9 28 19.5 24.3 ... ## $ AvgIntenCh2 : num 4.95 206.88 116.32 102.29 112.42 ... ## $ AvgIntenCh3 : num 9.55 69.92 63.94 28.22 20.47 ... ## $ AvgIntenCh4 : num 2.21 164.15 106.7 31.03 40.58 ... ## $ ConvexHullAreaRatioCh1: num 1.12 1.26 1.05 1.2 1.11 ... 22.9 Sacramento House Price Data This data frame contains house and sale price data for 932 homes in Sacramento CA. The original data were obtained from the website for the SpatialKey software. From their website: “The Sacramento real estate transactions file is a list of 985 real estate transactions in the Sacramento area reported over a five-day period, as reported by the Sacramento Bee.” Google was used to fill in missing/incorrect data. data(Sacramento) str(Sacramento) ## &#39;data.frame&#39;: 932 obs. of 9 variables: ## $ city : Factor w/ 37 levels &quot;ANTELOPE&quot;,&quot;AUBURN&quot;,..: 34 34 34 34 34 34 34 34 29 31 ... ## $ zip : Factor w/ 68 levels &quot;z95603&quot;,&quot;z95608&quot;,..: 64 52 44 44 53 65 66 49 24 25 ... ## $ beds : int 2 3 2 2 2 3 3 3 2 3 ... ## $ baths : num 1 1 1 1 1 1 2 1 2 2 ... ## $ sqft : int 836 1167 796 852 797 1122 1104 1177 941 1146 ... ## $ type : Factor w/ 3 levels &quot;Condo&quot;,&quot;Multi_Family&quot;,..: 3 3 3 3 3 1 3 3 1 3 ... ## $ price : int 59222 68212 68880 69307 81900 89921 90895 91002 94905 98937 ... ## $ latitude : num 38.6 38.5 38.6 38.6 38.5 ... ## $ longitude: num -121 -121 -121 -121 -121 ... 22.10 Animal Scat Data Reid (2105) collected data on animal feses in coastal California. The data consist of DNA verified species designations as well as fields related to the time and place of the collection and the scat itself. The data frame scat_orig contains while scat contains data on the three main species. data(scat) str(scat) ## &#39;data.frame&#39;: 110 obs. of 19 variables: ## $ Species : Factor w/ 3 levels &quot;bobcat&quot;,&quot;coyote&quot;,..: 2 2 1 2 2 2 1 1 1 1 ... ## $ Month : Factor w/ 9 levels &quot;April&quot;,&quot;August&quot;,..: 4 4 4 4 4 4 4 4 4 4 ... ## $ Year : int 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 ... ## $ Site : Factor w/ 2 levels &quot;ANNU&quot;,&quot;YOLA&quot;: 2 2 2 2 2 2 1 1 1 1 ... ## $ Location : Factor w/ 3 levels &quot;edge&quot;,&quot;middle&quot;,..: 1 1 2 2 1 1 3 3 3 2 ... ## $ Age : int 5 3 3 5 5 5 1 3 5 5 ... ## $ Number : int 2 2 2 2 4 3 5 7 2 1 ... ## $ Length : num 9.5 14 9 8.5 8 9 6 5.5 11 20.5 ... ## $ Diameter : num 25.7 25.4 18.8 18.1 20.7 21.2 15.7 21.9 17.5 18 ... ## $ Taper : num 41.9 37.1 16.5 24.7 20.1 28.5 8.2 19.3 29.1 21.4 ... ## $ TI : num 1.63 1.46 0.88 1.36 0.97 1.34 0.52 0.88 1.66 1.19 ... ## $ Mass : num 15.9 17.6 8.4 7.4 25.4 ... ## $ d13C : num -26.9 -29.6 -28.7 -20.1 -23.2 ... ## $ d15N : num 6.94 9.87 8.52 5.79 7.01 8.28 4.2 3.89 7.34 6.06 ... ## $ CN : num 8.5 11.3 8.1 11.5 10.6 9 5.4 5.6 5.8 7.7 ... ## $ ropey : int 0 0 1 1 0 1 1 0 0 1 ... ## $ segmented: int 0 0 1 0 1 0 1 1 1 1 ... ## $ flat : int 0 0 0 0 0 0 0 0 0 0 ... ## $ scrape : int 0 0 1 0 0 0 1 0 0 0 ... "],
["session-information.html", "23 Session Information", " 23 Session Information This documentation was created on Wed Nov 30 2016 with the following R packages ## R version 3.3.2 (2016-10-31) ## Platform: x86_64-apple-darwin13.4.0 (64-bit) ## Running under: macOS Sierra 10.12.1 ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats4 grid parallel splines stats graphics grDevices ## [8] utils datasets methods base ## ## other attached packages: ## [1] desirability_2.1 Hmisc_4.0-0 ## [3] Formula_1.2-1 C50_0.1.0-24 ## [5] mda_0.4-9 class_7.3-14 ## [7] party_1.1-2 strucchange_1.5-1 ## [9] sandwich_2.3-4 zoo_1.7-13 ## [11] modeltools_0.2-21 mvtnorm_1.0-5 ## [13] QSARdata_1.3 nlme_3.1-128 ## [15] reshape2_1.4.2 pls_2.5-0 ## [17] mboost_2.7-0 stabs_0.5-1 ## [19] caTools_1.17.1 doMC_1.3.4 ## [21] iterators_1.0.8 foreach_1.4.3 ## [23] e1071_1.6-7 ipred_0.9-5 ## [25] randomForest_4.6-12 ROSE_0.0-3 ## [27] DMwR_0.4.1 d3heatmap_0.6.1.1 ## [29] networkD3_0.2.13 proxy_0.4-16 ## [31] DT_0.2 plyr_1.8.4 ## [33] pROC_1.8 kernlab_0.9-25 ## [35] klaR_0.6-12 MASS_7.3-45 ## [37] gbm_2.1.1 survival_2.40-1 ## [39] latticeExtra_0.6-28 RColorBrewer_1.1-2 ## [41] earth_4.4.7 plotmo_3.3.1 ## [43] TeachingDemos_2.10 plotrix_3.6-3 ## [45] mlbench_2.1-1 AppliedPredictiveModeling_1.1-6 ## [47] knitr_1.15.1 caret_6.0-73 ## [49] ggplot2_2.2.0 lattice_0.20-34 ## [51] bookdown_0.3 ## ## loaded via a namespace (and not attached): ## [1] TH.data_1.0-7 minqa_1.2.4 colorspace_1.3-1 ## [4] rprojroot_1.1 htmlTable_1.7 base64enc_0.1-3 ## [7] rstudioapi_0.6 MatrixModels_0.4-1 prodlim_1.5.7 ## [10] coin_1.1-3 codetools_0.2-15 jsonlite_1.1 ## [13] nloptr_1.0.4 pbkrtest_0.4-6 cluster_2.0.5 ## [16] png_0.1-7 compiler_3.3.2 backports_1.0.4 ## [19] assertthat_0.1 Matrix_1.2-7.1 lazyeval_0.2.0 ## [22] acepack_1.4.1 htmltools_0.3.5 quantreg_5.29 ## [25] tools_3.3.2 partykit_1.0-5 igraph_1.0.1 ## [28] gtable_0.2.0 Rcpp_0.12.8 gdata_2.17.0 ## [31] stringr_1.1.0 lme4_1.1-12 gtools_3.5.0 ## [34] MLmetrics_1.1.1 scales_0.4.1 SparseM_1.74 ## [37] yaml_2.1.14 quantmod_0.4-7 gridExtra_2.2.1 ## [40] rpart_4.1-10 gam_1.14 stringi_1.1.2 ## [43] TTR_0.23-1 lava_1.4.5 bitops_1.0-6 ## [46] evaluate_0.10 ROCR_1.0-7 htmlwidgets_0.8 ## [49] labeling_0.3 magrittr_1.5 CORElearn_1.48.0 ## [52] gplots_3.0.1 nnls_1.4 multcomp_1.4-6 ## [55] combinat_0.0-8 foreign_0.8-67 mgcv_1.8-15 ## [58] xts_0.9-7 abind_1.4-5 nnet_7.3-12 ## [61] tibble_1.2 car_2.1-3 KernSmooth_2.23-15 ## [64] ellipse_0.3-8 rmarkdown_1.2 jpeg_0.1-8 ## [67] data.table_1.9.8 ModelMetrics_1.1.0 digest_0.6.10 ## [70] munsell_0.4.3 quadprog_1.5-5 "]
]
