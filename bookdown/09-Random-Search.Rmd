```{r random_startup,echo=FALSE,message=FALSE,warning=FALSE}
library(knitr)
opts_chunk$set(digits = 3, tidy = FALSE, fig.path = 'random/', dev = 'svg', dev.args = list(bg = "transparent"))
library(gbm)
library(klaR)
library(mlbench)
library(kernlab)
library(pROC)
theme_set(theme_bw())
```

# Random Hyperparameter Search

```{r random_param, results='hide', echo=FALSE, message=FALSE}
mods <- getModelInfo()
isSeq <- unlist(lapply(mods, function(x) !is.null(x$loop)))
isSeq <- names(isSeq)[isSeq]

mod_list <- paste(sort(paste('<code>', isSeq, '</code>', sep = "")), collapse = ', ')

count_param <- function(x) {
  x <- x$parameter
  if(nrow(x) == 1 && all(x$parameter == "parameter")) return(0)
  nrow(x)
}
nparam <- unlist(lapply(mods, count_param))
```

The default method for optimizing tuning parameters in `train` is to use a [grid search](model-training-and-tuning.html#grids). This approach is usually effective but, in cases when there are many tuning parameters, it can be inefficient. An alternative is to use a combination of [grid search and racing](adaptive-resampling.html). Another is to use a [random selection of tuning parameter combinations](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf) to cover the parameter space to a lesser extent.

There are a number of models where this can be beneficial in finding reasonable values of the tuning parameters in a relatively short time. However, there are some models where the efficiency in a small search field can cancel out other optimizations. For example, a number of models in caret utilize the "sub-model trick" where *M* tuning parameter combinations are evaluated, potentially far fewer than M model fits are required. This approach is best leveraged when a simple grid search is used. For this reason, it may be inefficient to use random search for the following model codes: `r I(mod_list)`.

Finally, many of the models wrapped by `train` have a small number of parameters. The average number of parameters is `r round(mean(nparam), 1)`.

To use random search, another option is available in `trainControl` called `search`. Possible values of this argument are `"grid"` and `"random"`. The built-in models contained in caret contain code to generate random tuning parameter combinations. The total number of unique combinations is specified by the `tuneLength` option to `train`.

Again, we will use the sonar data from the previous training page to demonstrate the method with a regularized discriminant analysis by looking at a total of 30 tuning parameter combinations:

```{r random_rda, tidy=FALSE,cache=TRUE, message=FALSE,warning=FALSE}
library(mlbench)
data(Sonar)

library(caret)
set.seed(998)
inTraining <- createDataPartition(Sonar$Class, p = .75, list = FALSE)
training <- Sonar[ inTraining,]
testing  <- Sonar[-inTraining,]

fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 10,
                           classProbs = TRUE,
                           summaryFunction = twoClassSummary,
                           search = "random")

set.seed(825)
rda_fit <- train(Class ~ ., data = training, 
                  method = "rda",
                  metric = "ROC",
                  tuneLength = 30,
                  trControl = fitControl)
rda_fit
``` 

There is currently only a `ggplot` method (instead of a basic `plot` method). The results of this function with random searching depends on the number and type of tuning parameters. In this case, it produces a scatter plot of the  continuous parameters. 

```{r random_plot,fig.width=8,fig.height=6}
ggplot(rda_fit) + theme(legend.position = "top")
``` 


