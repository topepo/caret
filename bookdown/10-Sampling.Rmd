```{r samp_load, echo = FALSE, message=FALSE, warning=FALSE}
library(caret)
library(pROC)
library(DMwR)
library(ROSE)
library(randomForest)
library(ipred)
library(e1071)
library(parallel)
library(doMC)
registerDoMC(cores=detectCores()-1)

theme_set(theme_bw())

library(knitr)
opts_chunk$set(digits = 3, tidy = FALSE, fig.path = 'sampling/')
library(pROC)
```

# Subsampling For Class Imbalances

Contents

 - [Subsampling Techniques](#methods)
 - [Subsampling During Resampling](#resampling)
 - [Complications](#complications)
 - [Using Custom Subsampling Techniques](#custom-subsamp)

In classification problems, a disparity in the frequencies of the observed classes can have a significant negative impact on model fitting. One technique for resolving such a class imbalance is to subsample the training data in a manner that mitigates the issues. Examples of sampling methods for this purpose are:

 - *down-sampling*: randomly subset all the classes in the training set so that their class frequencies match the least prevalent class. For example, suppose that 80% of the training set samples are the first class and the remaining 20% are in the second class. Down-sampling would randomly sample the first class to be the same size as the second class (so that only 40% of the total training set is used to fit the model). **caret** contains a function (`downSample`) to do this.
 - *up-sampling*: randomly sample (with replacement) the minority class to be the same size as the majority class. **caret** contains a function (`upSample`) to do this.
 - *hybrid methods*: techniques such as     [SMOTE](https://scholar.google.com/scholar?hl=en&q=SMOTE&btnG=&as_sdt=1%2C33&as_sdtp=) and [ROSE](https://scholar.google.com/scholar?q=%22Training+and+assessing+classification+rules+with+imbalanced+data%22&btnG=&hl=en&as_sdt=0%2C33) down-sample the majority class and synthesize new data points in the minority class. There are two packages (**DMwR** and **ROSE**) that implement these procedures.

Note that this type of sampling is different from splitting the data into a training and test set. You would never want to artificially balance the test set; its class frequencies should be in-line with what one would see "in the wild". Also, the above procedures are independent of resampling methods such as cross-validation and the bootstrap.

In practice, one could take the training set and, before model fitting, sample the data. There are two issues with this approach

 - Firstly, during model tuning the holdout samples generated during resampling are also glanced and may not reflect the class imbalance that future predictions would encounter. This is likely to lead to overly optimistic estimates of performance.
 - Secondly, the subsampling process will probably induce more model uncertainty. Would the model results differ under a different subsample? As above, the resampling statistics are more likely to make the model appear more effective than it actually is.

The alternative is to include the subsampling inside of the usual resampling procedure. This is also advocated for pre-process and featur selection steps too. The two disadvantages are that it might increase computational times and that it might also complicate the analysis in other ways (see the [section below](#complications) about the pitfalls).

<div id="methods"></div>

## Subsampling Techniques

To illustrate these methods, let's simulate some data with a class imbalance using this method. We will simulate a training and test set where each contains 10000 samples and a minority class rate of about 5.9%:

``` {r samp_sim_data}
library(caret)

set.seed(2969)
imbal_train <- twoClassSim(10000, intercept = -20, linearVars = 20)
imbal_test  <- twoClassSim(10000, intercept = -20, linearVars = 20)
table(imbal_train$Class)
```

Let's create different versions of the training set prior to model tuning:

``` {r, samp_models, cache = TRUE}
set.seed(9560)
down_train <- downSample(x = imbal_train[, -ncol(imbal_train)],
                         y = imbal_train$Class)
table(down_train$Class)   

set.seed(9560)
up_train <- upSample(x = imbal_train[, -ncol(imbal_train)],
                     y = imbal_train$Class)                         
table(up_train$Class) 

library(DMwR)

set.seed(9560)
smote_train <- SMOTE(Class ~ ., data  = imbal_train)                         
table(smote_train$Class) 

library(ROSE)

set.seed(9560)
rose_train <- ROSE(Class ~ ., data  = imbal_train)$data                         
table(rose_train$Class) 
```

For these data, we'll use a bagged classification and estimate the area under the ROC curve using five repeats of 10-fold CV.


``` {r samp_outside, cache = TRUE}
ctrl <- trainControl(method = "repeatedcv", repeats = 5,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary)

set.seed(5627)
orig_fit <- train(Class ~ ., data = imbal_train, 
                  method = "treebag",
                  nbagg = 50,
                  metric = "ROC",
                  trControl = ctrl)

set.seed(5627)
down_outside <- train(Class ~ ., data = down_train, 
                      method = "treebag",
                      nbagg = 50,
                      metric = "ROC",
                      trControl = ctrl)

set.seed(5627)
up_outside <- train(Class ~ ., data = up_train, 
                    method = "treebag",
                    nbagg = 50,
                    metric = "ROC",
                    trControl = ctrl)

set.seed(5627)
rose_outside <- train(Class ~ ., data = rose_train, 
                      method = "treebag",
                      nbagg = 50,
                      metric = "ROC",
                      trControl = ctrl)


set.seed(5627)
smote_outside <- train(Class ~ ., data = smote_train, 
                       method = "treebag",
                       nbagg = 50,
                       metric = "ROC",
                       trControl = ctrl)
```

We will collate the resampling results and create a wrapper to estimate the test set performance:

```{r samp_outside_res, cache = TRUE}
outside_models <- list(original = orig_fit,
                       down = down_outside,
                       up = up_outside,
                       SMOTE = smote_outside,
                       ROSE = rose_outside)

outside_resampling <- resamples(outside_models)

test_roc <- function(model, data) {
  library(pROC)
  roc_obj <- roc(data$Class, 
                 predict(model, data, type = "prob")[, "Class1"],
                 levels = c("Class2", "Class1"))
  ci(roc_obj)
  }

outside_test <- lapply(outside_models, test_roc, data = imbal_test)
outside_test <- lapply(outside_test, as.vector)
outside_test <- do.call("rbind", outside_test)
colnames(outside_test) <- c("lower", "ROC", "upper")
outside_test <- as.data.frame(outside_test)

summary(outside_resampling, metric = "ROC")
outside_test
```


The training and test set estimates for the area under the ROC curve do not appear to correlate. Based on the resampling results, one would infer that up-sampling is nearly perfect and that ROSE does relatively poorly. The reason that up-sampling appears to perform so well is that the samples in the majority class are replicated and have a large potential to be in both the model building and hold-out sets. In essence, the hold-outs here are not truly independent samples.

In reality, all of the sampling methods do about the same (based on the test set). The statistics for the basic model fit with no sampling are fairly in-line with one another (`r I(round(getTrainPerf(orig_fit)[, "TrainROC"], 3))` via resampling and `r I(round(outside_test["original", "ROC"], 3))` for the test set).

<div id="resampling"> </div>

## Subsampling During Resampling


Recent versions of **caret** allow the user to specify subsampling when using `train` so that it is conducted inside of resampling. All four methods shown above can be accessed with the basic package using simple syntax. If you want to use your own technique, or want to change some of the parameters for  `SMOTE` or `ROSE`, the last section below shows how to use custom subsampling.

The way to enable subsampling is to use yet another option in  `trainControl` called  `sampling`. The most basic syntax is to use a character string with the name of the sampling method, either `"down"`, `"up"`, `"smote"`, or `"rose"`. Note that you will need to have the **DMwR** and **ROSE** packages installed to use SMOTE and ROSE, respectively.

One complication is related to pre-processing. Should the subsampling occur before or after the pre-processing? For example, if you down-sample the data and using PCA for signal extraction, should the loadings be estimated from the entire training set? The estimate is potentially better since the entire training set is being used but the subsample may happen to capture a small potion of the PCA space. There isn't any obvious answer.

The default behavior is to subsample the data prior to pre-processing. This can be easily changed and an example is given below.

Now let's re-run our bagged tree models while sampling inside of cross-validation:


``` {r samp_inside_mods}
ctrl <- trainControl(method = "repeatedcv", repeats = 5,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary,
                     ## new option here:
                     sampling = "down")

set.seed(5627)
down_inside <- train(Class ~ ., data = imbal_train,
                     method = "treebag",
                     nbagg = 50,
                     metric = "ROC",
                     trControl = ctrl)

## now just change that option
ctrl$sampling <- "up"

set.seed(5627)
up_inside <- train(Class ~ ., data = imbal_train,
                   method = "treebag",
                   nbagg = 50,
                   metric = "ROC",
                   trControl = ctrl)

ctrl$sampling <- "rose"

set.seed(5627)
rose_inside <- train(Class ~ ., data = imbal_train,
                     method = "treebag",
                     nbagg = 50,
                     metric = "ROC",
                     trControl = ctrl)

ctrl$sampling <- "smote"

set.seed(5627)
smote_inside <- train(Class ~ ., data = imbal_train,
                      method = "treebag",
                      nbagg = 50,
                      metric = "ROC",
                      trControl = ctrl)
```

Here are the resampling and test set results:

``` {r samp_inside_res}
inside_models <- list(original = orig_fit,
                      down = down_inside,
                      up = up_inside,
                      SMOTE = smote_inside,
                      ROSE = rose_inside)

inside_resampling <- resamples(inside_models)

inside_test <- lapply(inside_models, test_roc, data = imbal_test)
inside_test <- lapply(inside_test, as.vector)
inside_test <- do.call("rbind", inside_test)
colnames(inside_test) <- c("lower", "ROC", "upper")
inside_test <- as.data.frame(inside_test)

summary(inside_resampling, metric = "ROC")
inside_test
```


``` {r samp_insode_plot_data, echo=FALSE, message=FALSE, cache = TRUE}
inside_roc <- inside_resampling$values[, grepl("ROC", names(inside_resampling$values))]
inside_means <- apply(inside_roc, 2, mean)
names(inside_means) <- gsub("~ROC", "", names(inside_means))
inside_means <- data.frame(model = names(inside_means),
                           CV = as.vector(inside_means))
inside_means$When <- "Inside"
inside_test$model <- rownames(inside_test)

inside_results <- merge(inside_test[, c("model", "ROC")], inside_means)

outside_roc <- outside_resampling$values[, grepl("ROC", names(outside_resampling$values))]
outside_means <- apply(outside_roc, 2, mean)
names(outside_means) <- gsub("~ROC", "", names(outside_means))
outside_means <- data.frame(model = names(outside_means),
                            CV = as.vector(outside_means))
outside_means$When <- "Outside"
outside_test$model <- rownames(outside_test)

outside_results <- merge(outside_test[, c("model", "ROC")], outside_means)
outside_results$When[outside_results$model == "original"] <- "No Sampling"
plot_data <- rbind(inside_results, outside_results)
plot_data$Diff <- abs(plot_data$ROC - plot_data$CV)

rng <- extendrange(c(plot_data$CV, plot_data$ROC))
```

The figure below shows the difference in the area under the ROC curve and the test set results for the approaches shown here. Repeating the subsampling procedures for every resample produces results that are more consistent with the test set. 

``` {r samp_insode_plot, echo=FALSE, message=FALSE, cache = TRUE}
ggplot(plot_data, aes(x = model, y = Diff, color = When)) +
  geom_point() + 
  theme(legend.position = "top") + 
  ylab("Absolute Difference Between CV and Test Results") + xlab("")
```

<div id="complications"> </div>

## Complications


The user should be aware that there are a few things that can happening when subsampling that can cause issues in their code. As previously mentioned, when sampling occurs in relation to pre-processing is one such issue. Others are:

 - Sparsely represented categories in factor variables may turn into zero-variance predictors or may be completely sampled out of the model.
 - The underlying functions that do the sampling (e.g.  `SMOTE`, `downSample`, etc) operate in very different ways and this can affect your results. For example, `SMOTE` and `ROSE` will convert your predictor input argument into a data frame (even if you start with a matrix).
 - Currently, sample weights are not supported with sub-sampling.
 - If you use `tuneLength` to specify the search grid, understand that the data that is used to determine the grid has not been sampled. In most cases, this will not matter but if the grid creation process is affected by the sample size, you may end up using a sub-optimal tuning grid.
 - For some models that require more samples than parameters, a reduction in the sample size may prevent you from being able to fit the model.

<div id="custom-subsamp"> </div>

## Using Custom Subsampling Techniques


Users have the ability to create their own type of subsampling procedure. To do this, alternative syntax is used with the  `sampling` argument of the  `trainControl`. Previously, we used a simple string as the value of this argument. Another way to specify the argument is to use a list with three (named) elements:

 - The `name` value is a character string used when the `train` object is printed. It can be any string.
 - The `func` element is a function that does the subsampling. It should have arguments called `x` and `y` that will contain the predictors and outcome data, respectively. The function should return a list with elements of the same name.
 - The `first` element is a single logical value that indicates whether the subsampling should occur first relative to pre-process. A value of `FALSE` means that the subsampling function will receive the sampled versions of  `x` and `y`.

For example, here is what the list version of the  `sampling` argument looks like when simple down-sampling is used:

```{r samp_down_ex}
down_inside$control$sampling
```

As another example, suppose we want to use SMOTE but use 10 nearest neighbors instead of the default of 5. To do this, we can create a simple wrapper around the `SMOTE` function and call this instead:

```{r samp_smote_ex}
smotest <- list(name = "SMOTE with more neighbors!",
                func = function (x, y) {
                  library(DMwR)
                  dat <- if (is.data.frame(x)) x else as.data.frame(x)
                  dat$.y <- y
                  dat <- SMOTE(.y ~ ., data = dat, k = 10)
                  list(x = dat[, !grepl(".y", colnames(dat), fixed = TRUE)], 
                       y = dat$.y)
                  },
                first = TRUE)
```

The control object would then be:

```{r samp_smote_ex_ctrl}
ctrl <- trainControl(method = "repeatedcv", repeats = 5,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary,
                     sampling = smotest)
```
